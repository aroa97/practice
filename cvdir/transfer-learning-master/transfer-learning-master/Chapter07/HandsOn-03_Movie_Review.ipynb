{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. 텍스트 문서의 범주화 - (3) 리뷰 감성 분류기 구현\n",
    "\n",
    "\n",
    "- 이제 앞에서 구현한 CNN 문서 모델을 훈련해서 감성 분류기를 구축해 보자\n",
    "- 캐글에서 아마존 감성 분석 리뷰 데이터 세트를 다운로드 받아 압축해제하여 저장한다. (train.ft.txt와 test.ft.txt 두 파일 모두 다운)\n",
    "    - 다운로드 url\n",
    "        - https://www.kaggle.com/bittlingmayer/amazonreviews\n",
    "    - 저장경로\n",
    "        - train.ft.txt -> data/amazonreviews/train.ft/train.ft.txt\n",
    "        - test.ft.txt -> data/amazonreviews/test.ft/test.ft.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import config\n",
    "from dataloader.loader import Loader\n",
    "from preprocessing.utils import Preprocess, remove_empty_docs\n",
    "from dataloader.embeddings import GloVe\n",
    "from model.cnn_document_model import DocumentModel, TrainingParameters\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아마존 리뷰 데이터 로드\n",
    "\n",
    "- 아마존 리뷰 데이터를 로드한다. (data/amazonreviews 경로)\n",
    "    - 360만개의 훈련 샘플과 40만개의 테스트 샘플이 있다. train 데이터셋은 랜덤으로 20만개만 추출하여 사용한다\n",
    "    - <b>\\__label__1</b>은 별점 1-2점을 매긴 리뷰에 해당, <b>\\__label__2</b>는 별점 4-5점을 매긴 리뷰에 해당한다\n",
    "    - 별점 3점의 리뷰, 즉, 중립적인 감성을 가진 리뷰는 이 데이터 세트에 포함되지 않았다\n",
    "    - 원본 데이터 예시\n",
    "````\n",
    "__label__<X> <summary/title>: <Review Text>\n",
    "Example:\n",
    "__label__2 Good Movie: Awesome.... simply awesome. I couldn't put this down\n",
    "and laughed, smiled, and even got tears! A brand new favorite author.\n",
    "```\n",
    "\n",
    "- 아마존 리뷰 데이터를 데이터프레임으로 변환한다\n",
    "    - sentiment 칼럼에 0(부정) 또는 1(긍정) 값을 입력\n",
    "    - 데이터프레임 예시\n",
    "```\n",
    "index   review                                              sentiment\n",
    "0       Stuning even for the non-gamer . This sound t...    1\n",
    "1       The best soundtrack ever to anything. . I'm r...    1\n",
    "2       Amazing! . This soundtrack is my favorite mus...    1\n",
    "3       Excellent Soundtrack: I truly like this soundt...   1\n",
    "4       Remember, Pull Your Jaw Off The Floor After He...   1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape : (3600000, 2)\n",
      "test_df.shape : (400000, 2)\n"
     ]
    }
   ],
   "source": [
    "# dataloader/loader.py 의 Loader.load_amazon_reviews 참고\n",
    "\n",
    "# 아마존 리뷰 데이터를 로드하여 데이터프레임으로 변환한다\n",
    "train_df = Loader.load_amazon_reviews('train')\n",
    "print(f'train_df.shape : {train_df.shape}')\n",
    "\n",
    "test_df = Loader.load_amazon_reviews('test')\n",
    "print(f'test_df.shape : {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    100020\n",
       "0     99980\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습셋에서 랜덤으로 20만개만 추출하여 feature 추출에 사용한다\n",
    "dataset = train_df.sample(n=200000, random_state=42)\n",
    "dataset.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer .  This sound t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything. .  I'm r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing! .  This soundtrack is my favorite mus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Stuning even for the non-gamer .  This sound t...          1\n",
       "1  The best soundtrack ever to anything. .  I'm r...          1\n",
       "2  Amazing! .  This soundtrack is my favorite mus...          1\n",
       "3  Excellent Soundtrack: I truly like this soundt...          1\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인덱스 시퀀스 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.shape : (200000,)\n",
      "target.shape : (200000,)\n",
      "=== after remove_empty_docs ===\n",
      "corpus size : 200000\n",
      "target size : 200000\n"
     ]
    }
   ],
   "source": [
    "# 추출한 20만개 데이터 샘플에서 review, sentiment 칼럼 값들 추출\n",
    "corpus = dataset['review'].values\n",
    "target = dataset['sentiment'].values\n",
    "print(f'corpus.shape : {corpus.shape}')\n",
    "print(f'target.shape : {target.shape}')\n",
    "\n",
    "# 유효하지 않은 값 제거 (비어있거나 길이가 30 이하인 경우 제거)\n",
    "corpus, target = remove_empty_docs(corpus, target)\n",
    "print('=== after remove_empty_docs ===')\n",
    "print(f'corpus size : {len(corpus)}')\n",
    "print(f'target size : {len(target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43195 unique tokens.\n",
      "All documents processed.ocessed."
     ]
    }
   ],
   "source": [
    "# 20만개 데이터 샘플에 대해 인덱스 사전 구축 및 인덱스 시퀀스 변환\n",
    "preprocessor = Preprocess(corpus=corpus)\n",
    "corpus_to_seq = preprocessor.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_to_seq size : 200000\n",
      "corpus_to_seq[0] size : 300\n",
      "corpus_to_seq[0] :\n",
      "[ 2  3  4  5  6  7  8  9  7 10 11 12 13 14 15 16 17 18 19 20 21  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 22 23 24 25 26 27 28 29  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16 30 31 32 33 34\n",
      " 17 30 35 36 37 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      " 38 39 40 41 42 37 16 43 44 45 46 17 37 47 48 37 49  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 18 19 20 30 50 51 52 17 53 54 46 55 56 36 57  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 37 58 59  8 60 39 61 62 63 64 65 59\n",
      " 66 41 67 68 28 69 17 70 71 72  0  0  0  0  0  0  0  0 39 61 73 74 75 76\n",
      "  4  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(f'corpus_to_seq size : {len(corpus_to_seq)}')\n",
    "print(f'corpus_to_seq[0] size : {len(corpus_to_seq[0])}')\n",
    "print(f'corpus_to_seq[0] :')\n",
    "print(corpus_to_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Expensive Junk: This product consists of a piece of thin flexible insulating material, adhesive backed velcro and white electrical tape.Problems . 1. Instructions are three pictures with little more information.2. Velcro was all crumpled as received and was stronger than the adhesive. When i tried to disengage the velcro both pieces came off and the paint from the ceiling.3. White electrical tape was horrible... cheap, narrow and it fell off in less than 1 hour.4. The price is a ripoff.I am building my own which is easier to use, cheaper, more attractive, and higher r-value. I am surprised Amazon even lists this junk.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱싱되기 전 원본 문서\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holdout_corpus.shape : (400000,)\n",
      "holdout_target.shape : (400000,)\n",
      "=== after remove_empty_docs ===\n",
      "holdout_corpus size : 400000\n",
      "holdout_target size : 400000\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋(test_df) 40만건 리뷰에서 review, sentiment 칼럼 값 추출\n",
    "holdout_corpus = test_df['review'].values\n",
    "holdout_target = test_df['sentiment'].values\n",
    "print(f'holdout_corpus.shape : {holdout_corpus.shape}')\n",
    "print(f'holdout_target.shape : {holdout_target.shape}')\n",
    "\n",
    "# 유효하지 않은 값 제거 (비어있거나 길이가 30 이하인 경우 제거)\n",
    "holdout_corpus, holdout_target = remove_empty_docs(holdout_corpus, holdout_target)\n",
    "print('=== after remove_empty_docs ===')\n",
    "print(f'holdout_corpus size : {len(holdout_corpus)}')\n",
    "print(f'holdout_target size : {len(holdout_target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents processed.ocessed."
     ]
    }
   ],
   "source": [
    "# 테스트셋을 인덱스 시퀀스로 변환 (위에서 생성한 인덱스 사전 그대로 사용)\n",
    "holdout_corpus_to_seq = preprocessor.transform(holdout_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holdout_corpus_to_seq size : 400000\n",
      "holdout_corpus_to_seq[0] size : 300\n",
      "holdout_corpus_to_seq[0] :\n",
      "[  335   336     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0    63  4565  6750   132   120     7\n",
      "    37   335  2779     7   244  3736     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    39    91   569    41     4   336    83   765    17    39   670  1043\n",
      "    53     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0    38    39   449    55     8   238\n",
      "  9021    53   262   214  1112   131     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     8   178  9021   184 19876   118  5560    55    37  1317     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     4   336   184  8860   172     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "   583    23    17   584   184  4719     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0   120     7   172   241  1812  2542\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     4    59     8  6845  7235   336    55    63   167     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0   957   507   820   416    53  1120\n",
      "    59   184  2967   214     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(f'holdout_corpus_to_seq size : {len(holdout_corpus_to_seq)}')\n",
    "print(f'holdout_corpus_to_seq[0] size : {len(holdout_corpus_to_seq[0])}')\n",
    "print(f'holdout_corpus_to_seq[0] :')\n",
    "print(holdout_corpus_to_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 50 dim GloVe vectors\n",
      "Found 400000 word vectors.\n",
      "words not found in embeddings: 2582\n"
     ]
    }
   ],
   "source": [
    "# 인덱싱된 텍스트 데이터를 GloVe로 임베딩 초기화.\n",
    "# glove.6B.50d.txt에 없는 단어는 OOV..txt에 write한다\n",
    "# word_index는 {'expensive': 2, 'junk': 3, 'this': 4, ...} 형태의 인덱싱 사전\n",
    "glove = GloVe(50)\n",
    "initial_embeddings = glove.get_embedding(preprocessor.word_index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43195"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스 사전의 단어 수\n",
    "len(preprocessor.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43197, 50)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GloVe로 임베딩 초기화된 행렬. 벡터 개수는 word_index 인덱스 사전의 단어 + 2, 차원 수는 50이다\n",
    "initial_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove6B.50d의 단어 수는 40만개이며, 이 중 아마존 리뷰 데이터 속 4만 3천여개 단어에 대한 임베딩 행렬을 생성하였다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 감성분석 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 43197  and the index of vocabulary words passed has 43195 words\n",
      "WARNING:tensorflow:From /Users/dhkdn9192/venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/dhkdn9192/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# model/cnn_document_model.py의 DocumentModel 클래스 참조\n",
    "\n",
    "# CNN 기반 문서 분류 모델 인스턴스 생성. 위에서 GloVe로 만든 임베딩 행렬을 임베딩 초깃값으로 사용한다\n",
    "amazon_review_model = DocumentModel(vocab_size=preprocessor.get_vocab_size(),\n",
    "                                    word_index=preprocessor.word_index,\n",
    "                                    num_sentences=Preprocess.NUM_SENTENCES,\n",
    "                                    embedding_weights=initial_embeddings,\n",
    "                                    conv_activation='tanh',\n",
    "                                    hidden_dims=64,\n",
    "                                    input_dropout=0.40,\n",
    "                                    hidden_gaussian_noise_sd=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape : (200000, 300)\n",
      "y_train.shape : (200000,)\n",
      "x_test.shape : (400000, 300)\n",
      "y_test.shape : (400000,)\n",
      "WARNING:tensorflow:From /Users/dhkdn9192/venv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 190000 samples, validate on 10000 samples\n",
      "Epoch 1/35\n",
      " - 230s - loss: 0.3844 - acc: 0.8207 - val_loss: 0.2410 - val_acc: 0.9014\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.24105, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "Epoch 2/35\n",
      " - 228s - loss: 0.2607 - acc: 0.8940 - val_loss: 0.2173 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.24105 to 0.21735, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "Epoch 3/35\n",
      " - 231s - loss: 0.2352 - acc: 0.9054 - val_loss: 0.2164 - val_acc: 0.9124\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.21735 to 0.21641, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "Epoch 4/35\n",
      " - 233s - loss: 0.2206 - acc: 0.9126 - val_loss: 0.2001 - val_acc: 0.9218\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.21641 to 0.20013, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "Epoch 5/35\n",
      " - 238s - loss: 0.2121 - acc: 0.9164 - val_loss: 0.2152 - val_acc: 0.9169\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.20013\n",
      "Epoch 6/35\n",
      " - 226s - loss: 0.2055 - acc: 0.9197 - val_loss: 0.1999 - val_acc: 0.9211\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.20013 to 0.19994, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "Epoch 7/35\n",
      " - 221s - loss: 0.2002 - acc: 0.9220 - val_loss: 0.1950 - val_acc: 0.9219\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.19994 to 0.19496, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "Epoch 8/35\n",
      " - 223s - loss: 0.1961 - acc: 0.9242 - val_loss: 0.1872 - val_acc: 0.9265\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.19496 to 0.18725, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "Epoch 9/35\n",
      " - 228s - loss: 0.1918 - acc: 0.9259 - val_loss: 0.1979 - val_acc: 0.9268\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.18725\n",
      "Epoch 10/35\n",
      " - 230s - loss: 0.1887 - acc: 0.9279 - val_loss: 0.1868 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.18725 to 0.18682, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "Epoch 11/35\n",
      " - 229s - loss: 0.1863 - acc: 0.9284 - val_loss: 0.1861 - val_acc: 0.9275\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.18682 to 0.18614, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "Epoch 12/35\n",
      " - 230s - loss: 0.1841 - acc: 0.9294 - val_loss: 0.1865 - val_acc: 0.9281\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.18614\n",
      "Epoch 13/35\n",
      " - 226s - loss: 0.1817 - acc: 0.9306 - val_loss: 0.1975 - val_acc: 0.9256\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.18614\n",
      "Epoch 14/35\n",
      " - 229s - loss: 0.1797 - acc: 0.9312 - val_loss: 0.1837 - val_acc: 0.9288\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.18614 to 0.18374, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "Epoch 15/35\n",
      " - 229s - loss: 0.1779 - acc: 0.9324 - val_loss: 0.1826 - val_acc: 0.9318\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.18374 to 0.18264, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "Epoch 16/35\n",
      " - 230s - loss: 0.1762 - acc: 0.9331 - val_loss: 0.1850 - val_acc: 0.9317\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.18264\n",
      "Epoch 17/35\n",
      " - 230s - loss: 0.1736 - acc: 0.9340 - val_loss: 0.1815 - val_acc: 0.9288\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.18264 to 0.18152, saving model to ./checkpoint/amazonreviews/model_06.hdf5\n",
      "Epoch 18/35\n",
      " - 230s - loss: 0.1735 - acc: 0.9344 - val_loss: 0.1889 - val_acc: 0.9288\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.18152\n",
      "Epoch 19/35\n",
      " - 230s - loss: 0.1717 - acc: 0.9353 - val_loss: 0.1874 - val_acc: 0.9275\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.18152\n",
      "Epoch 20/35\n",
      " - 232s - loss: 0.1709 - acc: 0.9350 - val_loss: 0.1826 - val_acc: 0.9282\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.18152\n",
      "Epoch 21/35\n",
      " - 232s - loss: 0.1710 - acc: 0.9356 - val_loss: 0.1857 - val_acc: 0.9280\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.18152\n",
      "Epoch 22/35\n",
      " - 231s - loss: 0.1675 - acc: 0.9371 - val_loss: 0.1855 - val_acc: 0.9305\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.18152\n",
      "Epoch 23/35\n",
      " - 235s - loss: 0.1675 - acc: 0.9369 - val_loss: 0.1872 - val_acc: 0.9303\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.18152\n",
      "Epoch 24/35\n",
      " - 233s - loss: 0.1658 - acc: 0.9377 - val_loss: 0.1818 - val_acc: 0.9301\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.18152\n",
      "Epoch 25/35\n",
      " - 233s - loss: 0.1651 - acc: 0.9383 - val_loss: 0.1859 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.18152\n",
      "Epoch 26/35\n",
      " - 233s - loss: 0.1644 - acc: 0.9382 - val_loss: 0.1832 - val_acc: 0.9308\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.18152\n",
      "Epoch 27/35\n",
      " - 233s - loss: 0.1641 - acc: 0.9387 - val_loss: 0.1908 - val_acc: 0.9260\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.18152\n",
      "Epoch 28/35\n",
      " - 230s - loss: 0.1645 - acc: 0.9388 - val_loss: 0.1832 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.18152\n",
      "Epoch 29/35\n",
      " - 231s - loss: 0.1623 - acc: 0.9397 - val_loss: 0.1870 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.18152\n",
      "Epoch 30/35\n",
      " - 231s - loss: 0.1613 - acc: 0.9398 - val_loss: 0.1843 - val_acc: 0.9322\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.18152\n",
      "Epoch 31/35\n",
      " - 233s - loss: 0.1618 - acc: 0.9396 - val_loss: 0.1891 - val_acc: 0.9288\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.18152\n",
      "Epoch 32/35\n",
      " - 233s - loss: 0.1605 - acc: 0.9399 - val_loss: 0.1915 - val_acc: 0.9292\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.18152\n",
      "Epoch 33/35\n",
      " - 232s - loss: 0.1593 - acc: 0.9405 - val_loss: 0.1830 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.18152\n",
      "Epoch 34/35\n",
      " - 232s - loss: 0.1591 - acc: 0.9403 - val_loss: 0.1839 - val_acc: 0.9326\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.18152\n",
      "Epoch 35/35\n",
      " - 231s - loss: 0.1580 - acc: 0.9410 - val_loss: 0.1815 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.18152\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델을 저장할 디렉토리 생성\n",
    "if not os.path.exists(os.path.join(config.MODEL_DIR, 'amazonreviews')):\n",
    "    os.makedirs(os.path.join(config.MODEL_DIR, 'amazonreviews'))\n",
    "\n",
    "# 학습 파라미터 저장 클래스\n",
    "train_params = TrainingParameters('model_with_tanh_activation', \n",
    "                                  model_file_path = config.MODEL_DIR + '/amazonreviews/model_06.hdf5',\n",
    "                                  model_hyper_parameters = config.MODEL_DIR + '/amazonreviews/model_06.json',\n",
    "                                  model_train_parameters = config.MODEL_DIR + '/amazonreviews/model_06_meta.json',\n",
    "                                  num_epochs=35)\n",
    "\n",
    "# 모델 컴파일\n",
    "amazon_review_model.get_classification_model().compile(loss=\"binary_crossentropy\", \n",
    "                                                       optimizer=train_params.optimizer,\n",
    "                                                       metrics=[\"accuracy\"])\n",
    "\n",
    "# callback (1) - 자동저장 체크포인트\n",
    "checkpointer = ModelCheckpoint(filepath=train_params.model_file_path,\n",
    "                               verbose=1,\n",
    "                               save_best_only=True,\n",
    "                               save_weights_only=True)\n",
    "\n",
    "# callback (2) - 조기종료\n",
    "early_stop = EarlyStopping(patience=2)\n",
    "\n",
    "# 모델에 입력할 학습데이터, 테스트데이터 (인덱스 값들의 시퀀스로 변환된 값)\n",
    "x_train = np.array(corpus_to_seq)\n",
    "y_train = np.array(target)\n",
    "x_test = np.array(holdout_corpus_to_seq)\n",
    "y_test = np.array(holdout_target)\n",
    "print(f'x_train.shape : {x_train.shape}')\n",
    "print(f'y_train.shape : {y_train.shape}')\n",
    "print(f'x_test.shape : {x_test.shape}')\n",
    "print(f'y_test.shape : {y_test.shape}')\n",
    "\n",
    "# 모델 훈련 시작\n",
    "amazon_review_model.get_classification_model().fit(x_train,\n",
    "                                                   y_train, \n",
    "                                                   batch_size=train_params.batch_size, \n",
    "                                                   epochs=train_params.num_epochs,  # 35 epochs\n",
    "                                                   verbose=2,\n",
    "                                                   validation_split=train_params.validation_split, # 5%\n",
    "                                                   callbacks=[checkpointer])\n",
    "\n",
    "# 모델 저장\n",
    "amazon_review_model._save_model(train_params.model_hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1873703473329544, 0.9297700004577637]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 평가 - 테스트 데이터셋으로 수행\n",
    "amazon_review_model.get_classification_model().evaluate(x_test,\n",
    "                                                        y_test, \n",
    "                                                        train_params.batch_size*10,\n",
    "                                                        verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가장 많이 변경된 임베딩은 무엇일까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', 20.422384305958015),\n",
       " ('refund', 19.359769787994853),\n",
       " ('waste', 19.03402415763812),\n",
       " ('disappointment', 16.008671764349923),\n",
       " ('junk', 15.945898010350172),\n",
       " ('poorly', 15.658867285795957),\n",
       " ('garbage', 15.54411400583432),\n",
       " ('warranty', 15.385526910548734),\n",
       " ('awful', 15.262878414344836),\n",
       " ('returned', 15.018080871537782),\n",
       " ('worthless', 14.880819286333272),\n",
       " ('useless', 14.848631768992588),\n",
       " ('terrible', 14.759487084156051),\n",
       " ('disappointing', 14.651085664386049),\n",
       " ('defective', 14.642242740096961),\n",
       " ('worse', 14.44785083374463),\n",
       " ('wasted', 14.365058565732028),\n",
       " ('horrible', 14.074117954679732),\n",
       " ('boring', 13.958321077396967),\n",
       " ('returning', 13.793071303484254)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_embeddings = amazon_review_model.get_classification_model().get_layer('imdb_embedding').get_weights()[0]\n",
    "\n",
    "embd_change = {}\n",
    "for word, i in preprocessor.word_index.items():\n",
    "    # Frobenium norm (Euclidean norm) 계\n",
    "    embd_change[word] = np.linalg.norm(initial_embeddings[i]-learned_embeddings[i])\n",
    "embd_change = sorted(embd_change.items(), key=lambda x: x[1], reverse=True)\n",
    "embd_change[0:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
