{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. 텍스트 문서의 범주화 - (4) IMDB 전체 데이터로 전이학습\n",
    "\n",
    "- 앞선 전이학습 실습과는 달리, IMDB 영화리뷰 데이터셋 전체를 사용하며 문장 수는 10개 -> 20개로 조정한다\n",
    "- IMDB 영화 리뷰 데이터를 다운로드 받아 data 디렉토리에 압축 해제한다\n",
    "    - 다운로드 : http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "    - 저장경로 : data/aclImdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import config\n",
    "from dataloader.loader import Loader\n",
    "from preprocessing.utils import Preprocess, remove_empty_docs\n",
    "from dataloader.embeddings import GloVe\n",
    "from model.cnn_document_model import DocumentModel, TrainingParameters\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델을 저장할 디렉토리 생성\n",
    "if not os.path.exists(os.path.join(config.MODEL_DIR, 'imdb')):\n",
    "    os.makedirs(os.path.join(config.MODEL_DIR, 'imdb'))\n",
    "\n",
    "# 학습 파라미터 설정\n",
    "train_params = TrainingParameters('imdb_transfer_tanh_activation', \n",
    "                                  model_file_path = config.MODEL_DIR+ '/imdb/full_model_10.hdf5',\n",
    "                                  model_hyper_parameters = config.MODEL_DIR+ '/imdb/full_model_10.json',\n",
    "                                  model_train_parameters = config.MODEL_DIR+ '/imdb/full_model_10_meta.json',\n",
    "                                  num_epochs=30,\n",
    "                                  batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape : (25000, 2)\n",
      "test_df.shape : (25000, 2)\n",
      "corpus size : 25000\n",
      "target size : 25000\n"
     ]
    }
   ],
   "source": [
    "# 다운받은 IMDB 데이터 로드: 학습셋 전체 사용\n",
    "train_df = Loader.load_imdb_data(directory = 'train')\n",
    "# train_df = train_df.sample(frac=0.05, random_state = train_params.seed)\n",
    "print(f'train_df.shape : {train_df.shape}')\n",
    "\n",
    "test_df = Loader.load_imdb_data(directory = 'test')\n",
    "print(f'test_df.shape : {test_df.shape}')\n",
    "\n",
    "# 텍스트 데이터, 레이블 추출\n",
    "corpus = train_df['review'].tolist()\n",
    "target = train_df['sentiment'].tolist()\n",
    "corpus, target = remove_empty_docs(corpus, target)\n",
    "print(f'corpus size : {len(corpus)}')\n",
    "print(f'target size : {len(target)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인덱스 시퀀스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28654 unique tokens.\n",
      "All documents processed.ocessed."
     ]
    }
   ],
   "source": [
    "# 앞선 전이학습 실습과 달리, 문장 개수를 10개 -> 20개로 상향\n",
    "Preprocess.NUM_SENTENCES = 20\n",
    "\n",
    "# 학습셋을 인덱스 시퀀스로 변환\n",
    "preprocessor = Preprocess(corpus=corpus)\n",
    "corpus_to_seq = preprocessor.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_to_seq size : 25000\n",
      "corpus_to_seq[0] size : 600\n"
     ]
    }
   ],
   "source": [
    "print(f'corpus_to_seq size : {len(corpus_to_seq)}')\n",
    "print(f'corpus_to_seq[0] size : {len(corpus_to_seq[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents processed.ocessed."
     ]
    }
   ],
   "source": [
    "# 테스트셋을 인덱스 시퀀스로 변환\n",
    "test_corpus = test_df['review'].tolist()\n",
    "test_target = test_df['sentiment'].tolist()\n",
    "test_corpus, test_target = remove_empty_docs(test_corpus, test_target)\n",
    "test_corpus_to_seq = preprocessor.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_corpus_to_seq size : 25000\n",
      "test_corpus_to_seq[0] size : 600\n"
     ]
    }
   ],
   "source": [
    "print(f'test_corpus_to_seq size : {len(test_corpus_to_seq)}')\n",
    "print(f'test_corpus_to_seq[0] size : {len(test_corpus_to_seq[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape : (25000, 600)\n",
      "y_train.shape : (25000,)\n",
      "x_test.shape : (25000, 600)\n",
      "y_test.shape : (25000,)\n"
     ]
    }
   ],
   "source": [
    "# 학습셋, 테스트셋 준비\n",
    "x_train = np.array(corpus_to_seq)\n",
    "x_test = np.array(test_corpus_to_seq)\n",
    "y_train = np.array(target)\n",
    "y_test = np.array(test_target)\n",
    "\n",
    "print(f'x_train.shape : {x_train.shape}')\n",
    "print(f'y_train.shape : {y_train.shape}')\n",
    "print(f'x_test.shape : {x_test.shape}')\n",
    "print(f'y_test.shape : {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe 임베딩 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 50 dim GloVe vectors\n",
      "Found 400000 word vectors.\n",
      "words not found in embeddings: 499\n",
      "initial_embeddings.shape : (28656, 50)\n"
     ]
    }
   ],
   "source": [
    "# GloVe 임베딩 초기화 - glove.6B.50d.txt pretrained 벡터 사용\n",
    "glove = GloVe(50)\n",
    "initial_embeddings = glove.get_embedding(preprocessor.word_index)\n",
    "print(f'initial_embeddings.shape : {initial_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  훈련된 모델 로드\n",
    "\n",
    "- HandsOn03에서 아마존 리뷰 데이터로 학습한 CNN 모델을 로드한다.\n",
    "- DocumentModel 클래스의 load_model로 모델을 로드하고, load_model_weights로 학습된 가중치를 가져온다. \n",
    "- 그 후, GloVe.update_embeddings 함수로 GloVe 초기화 임베딩을 업데이트한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 43197  and the index of vocabulary words passed has 43195 words\n",
      "WARNING:tensorflow:From /Users/dhkdn9192/venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/dhkdn9192/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# 모델 하이퍼파라미터 로드\n",
    "model_json_path = os.path.join(config.MODEL_DIR, 'amazonreviews/model_06.json')\n",
    "amazon_review_model = DocumentModel.load_model(model_json_path)\n",
    "\n",
    "# 모델 가중치 로드\n",
    "model_hdf5_path = os.path.join(config.MODEL_DIR, 'amazonreviews/model_06.hdf5')\n",
    "amazon_review_model.load_model_weights(model_hdf5_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_embeddings size : 43197\n",
      "23629 words are updated out of 28654\n"
     ]
    }
   ],
   "source": [
    "# 모델 임베딩 레이어 추출\n",
    "learned_embeddings = amazon_review_model.get_classification_model().get_layer('imdb_embedding').get_weights()[0]\n",
    "print(f'learned_embeddings size : {len(learned_embeddings)}')\n",
    "\n",
    "# 기존 GloVe 모델을 학습된 임베딩 행렬로 업데이트한다\n",
    "glove.update_embeddings(preprocessor.word_index, \n",
    "                        np.array(learned_embeddings), \n",
    "                        amazon_review_model.word_index)\n",
    "\n",
    "# 업데이트된 임베딩을 얻는다\n",
    "initial_embeddings = glove.get_embedding(preprocessor.word_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  IMDB 전이학습 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 28656  and the index of vocabulary words passed has 28654 words\n"
     ]
    }
   ],
   "source": [
    "# 분류 모델 생성 : IMDB 리뷰 데이터를 입력받아 이진분류를 수행하는 모델 생성\n",
    "imdb_model = DocumentModel(vocab_size=preprocessor.get_vocab_size(),\n",
    "                           word_index = preprocessor.word_index,\n",
    "                           num_sentences=Preprocess.NUM_SENTENCES,     \n",
    "                           embedding_weights=initial_embeddings,\n",
    "                           embedding_regularizer_l2 = 0.0,\n",
    "                           conv_activation = 'tanh',\n",
    "                           train_embedding = True,   # 임베딩 레이어의 가중치 학습함\n",
    "                           learn_word_conv = False,  # 단어 수준 conv 레이어의 가중치 학습 안 함\n",
    "                           learn_sent_conv = False,  # 문장 수준 conv 레이어의 가중치 학습 안 함\n",
    "                           hidden_dims=64,                                        \n",
    "                           input_dropout=0.1, \n",
    "                           hidden_layer_kernel_regularizer=0.01,\n",
    "                           final_layer_kernel_regularizer=0.01)\n",
    "\n",
    "# 가중치 업데이트 : 생성한 imdb_model 모델에서 다음의 각 레이어들의 가중치를 위에서 로드한 가중치로 갱신한다\n",
    "for l_name in ['word_conv','sentence_conv','hidden_0', 'final']:\n",
    "    new_weights = amazon_review_model.get_classification_model().get_layer(l_name).get_weights()\n",
    "    imdb_model.get_classification_model().get_layer(l_name).set_weights(weights=new_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dhkdn9192/venv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 24750 samples, validate on 250 samples\n",
      "Epoch 1/30\n",
      " - 40s - loss: 1.0038 - acc: 0.8686 - val_loss: 0.6622 - val_acc: 0.8560\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66215, saving model to ./checkpoint/imdb/full_model_10.hdf5\n",
      "Epoch 2/30\n",
      " - 39s - loss: 0.4992 - acc: 0.8782 - val_loss: 0.4534 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66215 to 0.45337, saving model to ./checkpoint/imdb/full_model_10.hdf5\n",
      "Epoch 3/30\n",
      " - 39s - loss: 0.3740 - acc: 0.8825 - val_loss: 0.3998 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.45337 to 0.39985, saving model to ./checkpoint/imdb/full_model_10.hdf5\n",
      "Epoch 4/30\n",
      " - 39s - loss: 0.3271 - acc: 0.8903 - val_loss: 0.3741 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.39985 to 0.37414, saving model to ./checkpoint/imdb/full_model_10.hdf5\n",
      "Epoch 5/30\n",
      " - 39s - loss: 0.3025 - acc: 0.8960 - val_loss: 0.3723 - val_acc: 0.8680\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.37414 to 0.37228, saving model to ./checkpoint/imdb/full_model_10.hdf5\n",
      "Epoch 6/30\n",
      " - 39s - loss: 0.2845 - acc: 0.9001 - val_loss: 0.3585 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.37228 to 0.35849, saving model to ./checkpoint/imdb/full_model_10.hdf5\n",
      "Epoch 7/30\n",
      " - 39s - loss: 0.2719 - acc: 0.9044 - val_loss: 0.3591 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35849\n",
      "Epoch 8/30\n",
      " - 39s - loss: 0.2621 - acc: 0.9082 - val_loss: 0.3634 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35849\n",
      "Epoch 9/30\n",
      " - 39s - loss: 0.2536 - acc: 0.9112 - val_loss: 0.3543 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35849 to 0.35432, saving model to ./checkpoint/imdb/full_model_10.hdf5\n",
      "Epoch 10/30\n",
      " - 39s - loss: 0.2437 - acc: 0.9183 - val_loss: 0.3536 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35432 to 0.35362, saving model to ./checkpoint/imdb/full_model_10.hdf5\n",
      "Epoch 11/30\n",
      " - 39s - loss: 0.2362 - acc: 0.9202 - val_loss: 0.3510 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.35362 to 0.35096, saving model to ./checkpoint/imdb/full_model_10.hdf5\n",
      "Epoch 12/30\n",
      " - 39s - loss: 0.2260 - acc: 0.9244 - val_loss: 0.3560 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35096\n",
      "Epoch 13/30\n",
      " - 39s - loss: 0.2216 - acc: 0.9270 - val_loss: 0.3514 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.35096\n",
      "Epoch 14/30\n",
      " - 39s - loss: 0.2147 - acc: 0.9301 - val_loss: 0.3557 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.35096\n",
      "Epoch 15/30\n",
      " - 39s - loss: 0.2066 - acc: 0.9328 - val_loss: 0.3516 - val_acc: 0.8680\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.35096\n",
      "Epoch 16/30\n",
      " - 40s - loss: 0.2000 - acc: 0.9375 - val_loss: 0.3524 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.35096\n",
      "Epoch 17/30\n",
      " - 42s - loss: 0.1963 - acc: 0.9386 - val_loss: 0.3600 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.35096\n",
      "Epoch 18/30\n",
      " - 41s - loss: 0.1866 - acc: 0.9434 - val_loss: 0.3635 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.35096\n",
      "Epoch 19/30\n",
      " - 40s - loss: 0.1821 - acc: 0.9449 - val_loss: 0.3638 - val_acc: 0.8680\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.35096\n",
      "Epoch 20/30\n",
      " - 40s - loss: 0.1771 - acc: 0.9476 - val_loss: 0.3630 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.35096\n",
      "Epoch 21/30\n",
      " - 40s - loss: 0.1724 - acc: 0.9499 - val_loss: 0.3765 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.35096\n",
      "Epoch 22/30\n",
      " - 40s - loss: 0.1663 - acc: 0.9528 - val_loss: 0.3639 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.35096\n",
      "Epoch 23/30\n",
      " - 40s - loss: 0.1613 - acc: 0.9539 - val_loss: 0.3627 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.35096\n",
      "Epoch 24/30\n",
      " - 40s - loss: 0.1571 - acc: 0.9562 - val_loss: 0.3682 - val_acc: 0.8920\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.35096\n",
      "Epoch 25/30\n",
      " - 41s - loss: 0.1535 - acc: 0.9581 - val_loss: 0.3757 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.35096\n",
      "Epoch 26/30\n",
      " - 40s - loss: 0.1493 - acc: 0.9590 - val_loss: 0.3844 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.35096\n",
      "Epoch 27/30\n",
      " - 40s - loss: 0.1465 - acc: 0.9598 - val_loss: 0.3686 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.35096\n",
      "Epoch 28/30\n",
      " - 40s - loss: 0.1398 - acc: 0.9632 - val_loss: 0.3864 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.35096\n",
      "Epoch 29/30\n",
      " - 40s - loss: 0.1376 - acc: 0.9644 - val_loss: 0.3744 - val_acc: 0.8880\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.35096\n",
      "Epoch 30/30\n",
      " - 40s - loss: 0.1318 - acc: 0.9659 - val_loss: 0.3763 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.35096\n"
     ]
    }
   ],
   "source": [
    "# 모델 컴파일              \n",
    "imdb_model.get_classification_model().compile(loss=\"binary_crossentropy\", \n",
    "                                              optimizer='rmsprop',\n",
    "                                              metrics=[\"accuracy\"])\n",
    "\n",
    "# callback (1) - 체크포인트\n",
    "checkpointer = ModelCheckpoint(filepath=train_params.model_file_path,\n",
    "                                verbose=1,\n",
    "                                save_best_only=True,\n",
    "                                save_weights_only=True)\n",
    "\n",
    "# callback (2) - 조기종료\n",
    "early_stop = EarlyStopping(patience=2)\n",
    "\n",
    "# 학습 시작\n",
    "imdb_model.get_classification_model().fit(x_train, \n",
    "                                          y_train, \n",
    "                                          batch_size=train_params.batch_size,\n",
    "                                          epochs=train_params.num_epochs,\n",
    "                                          verbose=2,\n",
    "                                          validation_split=0.01,\n",
    "                                          callbacks=[checkpointer])\n",
    "\n",
    "# 모델 저장\n",
    "imdb_model._save_model(train_params.model_hyper_parameters)\n",
    "train_params.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.30160063052177427, 0.8941999991416931]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 평가\n",
    "imdb_model.get_classification_model().evaluate(x_test, \n",
    "                                               y_test, \n",
    "                                               batch_size=train_params.batch_size*10,\n",
    "                                               verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
