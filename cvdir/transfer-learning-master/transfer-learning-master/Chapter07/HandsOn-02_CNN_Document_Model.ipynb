{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. 텍스트 문서의 범주화 - (2) CNN Document Model\n",
    "\n",
    "model/cnn_document_model.py에 구현된 DocumentModel CNN 문서 분류 모델의 주요 기능을 관련 코드와 함께 알아보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cnn](assets/handson02_cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Document Model\n",
    "\n",
    "- 문서의 계층적 분산 표현을 구현한 합성곱망(ConvNet)의 문서 모델이다\n",
    "- 문장 수준과 문서 수준의 두 단계로 나뉘며, 둘 다 합성곱망(convNet)으로 구현된다\n",
    "- 문장들을 단어 수중으로 임베딩 > 합성곱 > 풀링 하는 과정을 포함한다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 인덱싱 처리\n",
    "- empty값은 0, OOV(Out-of-Vocabulary) 단어는 1, 그 외 단어들은 고유한 인덱스값을 부여하는 방식으로 단어사전을 구축한다\n",
    "- 단어 인덱싱 예시\n",
    "```\n",
    "dict_items([('the', 2), ('cat', 3), ('sat', 4), ('on', 5), ('mat', 6),\n",
    "  ('.', 7), ('it', 8), ('was', 9), ('a', 10), ('nice', 11), ('!', 12),\n",
    "  ('rat', 13), ('damaged', 14), ('found', 15), ('at', 16), ('places', 17)])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing/utils.py - Preprocess 클래스\n",
    "class Preprocess:\n",
    "    # ...(생략)...\n",
    "    \n",
    "    def _build_vocab(self):\n",
    "        # 문서를 토큰화하여 각 단어 빈도 세기\n",
    "        word_index ={}\n",
    "        for doc in self.corpus:\n",
    "            for sentence in sent_tokenize(doc):\n",
    "                tokens = wordpunct_tokenize(sentence)\n",
    "                tokens = [token.lower().strip()  for token in tokens]\n",
    "                tokens = [token for token in tokens  if re.match('^[a-z]+$',token) is not None ]\n",
    "                for token in tokens:\n",
    "                    word_index[token] = word_index.get(token, 0)+1\n",
    "                    \n",
    "        filtered_word_index={}\n",
    "        \n",
    "        # 사용시 빈값은 0으로 (i = 0 for empty), 사전에 없는 단어는 1로 표기한다 (i = 1 for OOV)\n",
    "        # 즉, vocabulary에 있는 단어는 모두 2 이상의 값을 인덱스로 할당받느다\n",
    "        # 또한, 단어 빈도수가 MIN_WD_COUNT(5) 미만인 경우는 제외한다\n",
    "        i = 2\n",
    "        for word, count in word_index.items():\n",
    "            if count >= Preprocess.MIN_WD_COUNT :\n",
    "                filtered_word_index[word] = i\n",
    "                i +=1\n",
    "        print('Found %s unique tokens.' % len(filtered_word_index))\n",
    "        return filtered_word_index\n",
    "\n",
    "    # ...(생략)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제로 패딩\n",
    "- 각 인덱스값들의 시퀀스들에 대해 최대길이까지 남는 부분을 0으로 제로 패딩한다\n",
    "- keras.preprocessing.sequence의 pad_sequences 모듈을 사용한다\n",
    "- 제로 패딩 예시\n",
    "```\n",
    "[ 2 3 4 5 2 6 7 0 0 0 8 9 10 11 6 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "[ 2 13 4 5 2 6 7 0 0 0 2 6 9 14 15 16 1 17 7 0 0 0 0 0 0 0 0 0 0 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing/utils.py - Preprocess 클래스\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "class Preprocess:\n",
    "    # ...(생략)...\n",
    "    \n",
    "    def _text2wordindex_seq(self, word_index, corpus):\n",
    "        data = []\n",
    "        doc_count = 0 \n",
    "        for doc in corpus:\n",
    "            doc2wordseq = []\n",
    "            sent_num =0\n",
    "            doc_count+=1\n",
    "            if doc_count%1000 == 0 :\n",
    "                percent_processed = doc_count*100/len(corpus)\n",
    "                sys.stdout.write(\"\\r%f%% documents processed.\" % percent_processed)\n",
    "                sys.stdout.flush()\n",
    "            for sentence in sent_tokenize(doc):\n",
    "                \n",
    "                words = wordpunct_tokenize(sentence)\n",
    "                words = [token.lower().strip()  for token in words]\n",
    "                word_id_seq = [word_index[word] for word in words  if word_index.get(word) is not None]\n",
    "                #word_id_seq = tokenizer.texts_to_sequences([sentence])\n",
    "                \n",
    "                # padding : maxlen까지 남는 부분 제로패딩\n",
    "                # truncating : maxlen 넘는 부분 제거\n",
    "                # Preprocess.SENTENCE_LEN : 문장 최대 길이(단어수), 30\n",
    "                padded_word_id_seq = pad_sequences([word_id_seq], maxlen=Preprocess.SENTENCE_LEN,\n",
    "                                                   padding='post',\n",
    "                                                   truncating='post')\n",
    "                # ...(생략)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 임베딩 (및 OOV 단어 처리)\n",
    "\n",
    "- 인덱싱 처리된 각 단어들은 GloVe 모델을 통해 임베딩 초기화된다.\n",
    "- 이 과정에서 OOV 단어들은 나머지 모든 벡터의 평균(np.mean)에 약간의 노이즈를 추가한 값으로 초기화한다. \n",
    "- pre-train된 GloVe 임베딩 벡터를 다운로드 받아 data 디렉토리에 압축 해제한다\n",
    "    - 다운로드 : http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    - 저장경로 : data/glove.6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader/embeddings.py - GloVe 클래스\n",
    "class GloVe:\n",
    "    # ...(생략)...\n",
    "    \n",
    "    # 사전 학습된 GloVe 임베딩 모델 로드하기 (data/glove.6B/glove.6B.50d.txt)\n",
    "    def _load(self):\n",
    "        print('Reading {} dim GloVe vectors'.format(self.EMBEDDING_DIM))\n",
    "        self.embeddings_index = {}\n",
    "        with open(os.path.join(config.GLOVE_DIR, 'glove.6B.'+str(self.EMBEDDING_DIM)+'d.txt'),encoding=\"utf8\") as fin:\n",
    "            for line in fin:\n",
    "                try:\n",
    "                    values = line.split()\n",
    "                    coefs = np.asarray(values[1:], dtype='float32')\n",
    "                    word = values[0]\n",
    "                    self.embeddings_index[word] = coefs  # GloVe 임베딩 데이터를 파이썬 딕셔너리 형태로 저장한다\n",
    "                except:\n",
    "                    print(line)\n",
    "\n",
    "        print('Found %s word vectors.' % len(self.embeddings_index))\n",
    "\n",
    "        \n",
    "    # 위에서 로드한 임베딩 모델로 입력 문서를 임베딩 매트릭스로 만들기\n",
    "    def _init_embedding_matrix(self, word_index_dict, oov_words_file='OOV-Words.txt'):\n",
    "        self.embedding_matrix = np.zeros((len(word_index_dict)+2 , self.EMBEDDING_DIM))\n",
    "        not_found_words=0\n",
    "        missing_word_index = []\n",
    "        \n",
    "        # 각 단어들을 임베딩 벡터로 치환\n",
    "        with open(oov_words_file, 'w') as f: \n",
    "            for word, i in word_index_dict.items():\n",
    "                embedding_vector = self.embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    # words not found in embedding index will be all-zeros.\n",
    "                    self.embedding_matrix[i] = embedding_vector\n",
    "                else:\n",
    "                    not_found_words+=1\n",
    "                    f.write(word + ','+str(i)+'\\n')\n",
    "                    missing_word_index.append(i)\n",
    "\n",
    "            # oov by average vector:\n",
    "            self.embedding_matrix[1] = np.mean(self.embedding_matrix, axis=0)\n",
    "            for indx in missing_word_index:\n",
    "                self.embedding_matrix[indx] = np.random.rand(self.EMBEDDING_DIM)+ self.embedding_matrix[1]\n",
    "        print(\"words not found in embeddings: {}\".format(not_found_words))\n",
    "    \n",
    "    # ...(생략)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임베딩 레이어\n",
    "\n",
    "- 임베딩 행렬을 초기화 했으면 이제 다음과 같이 첫 번째 레이어인 임베딩 레이어를 만들 준비가 된 것이다.\n",
    "- tf.keras.layers 의 Embedding 모듈로 임베딩 레이어를 생성하고 초기화된 임베딩 가중치를 입력한다\n",
    "```\n",
    "embedding_layer = Embedding(weights=[self.embedding_weights])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/cnn_document_model.py - DocumentModel 클래스\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "class DocumentModel:\n",
    "    # ...(생략)...\n",
    "\n",
    "    def _build_model(self):\n",
    "        max_seq_length = self.sentence_len*self.num_sentences\n",
    "\n",
    "        # ...(생략)...\n",
    "\n",
    "        # 초기화된 가중치를 입력한다면 그걸 사용하여 임베딩하는 레이어 생성\n",
    "        if self.embedding_weights is not None:\n",
    "            embedding_layer = Embedding(self.vocab_size,\n",
    "                                        self.embedding_dim,                # 임베딩 깊이 50\n",
    "                                        weights=[self.embedding_weights],  # 초기화된 가중치 입력\n",
    "                                        input_length=max_seq_length,\n",
    "                                        trainable=self.train_embedding,\n",
    "                                        embeddings_regularizer = regularizers.l2(self.embedding_regularizer_l2),\n",
    "                                        name='imdb_embedding')\n",
    "        # ...(생략)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컨볼루션 레이어\n",
    "\n",
    "- 모든 문장에 동일한 1차원 합성곱 필터를 적용한다\n",
    "- 람다(lambda) 레이어를 사용해서 입력을 문장 별로 나눈다\n",
    "- 컨볼루션을 거치면 텐서의 모양은 다음과 같이 변한다\n",
    "    - 컨볼루션 전 : SENTENCE_LEN x EMBEDDING_DIM\n",
    "    - 컨볼루션 후 : (SENTENCE_LEN - kernel_size + 1) x filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/cnn_document_model.py - DocumentModel 클래스\n",
    "\n",
    "from keras.layers import Conv1D\n",
    "\n",
    "\n",
    "class DocumentModel:\n",
    "    # ...(생략)...\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # ...(생략)...\n",
    "        \n",
    "        # 모든 문장에 대해 같은 컨볼루션 필터가 적용된다\n",
    "        word_conv_model = Conv1D(filters=self.word_filters,           # 30\n",
    "                                 kernel_size=self.word_kernel_size,   # 5\n",
    "                                 padding=\"valid\",\n",
    "                                 activation=self.conv_activation, \n",
    "                                 trainable = self.learn_word_conv,\n",
    "                                 name = \"word_conv\",\n",
    "                                 strides=1)\n",
    "    \n",
    "        for sent in range(self.num_sentences):\n",
    "            # 입력 문서에서 하나의 문장을 받는다.\n",
    "            sentence =  Lambda(lambda x : x[:,sent*self.sentence_len: (sent+1)*self.sentence_len, :])(z) \n",
    "            \n",
    "            # 컨볼루션 입력 shape : (None, 30, 50)\n",
    "            conv = word_conv_model(sentence)\n",
    "            # 컨볼루션 출력 shape : (None, 26, 30)\n",
    "            \n",
    "            conv = KMaxPooling(k=self.sent_k_maxpool)(conv)\n",
    "            #transpose pooled values per sentence\n",
    "            conv = Reshape([self.word_filters*self.sent_k_maxpool,1])(conv)\n",
    "            conv_blocks.append(conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Max Pooling\n",
    "\n",
    "- k-max pooling은 가장 큰 k개의 값들만 추출하는 방식이다. \n",
    "    - 예를 들어 \\[3, 1, 5, 2\\] 에 2-max 풀링을 적용하면 \\[3, 5\\] 가 된다.    \n",
    "- k-max 풀링 레이어는 keras에서 제공되지 않으므로 커스텀 레이어로 구현한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/custom_layer.py - KMaxPooling 클래스\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Layer, InputSpec\n",
    "\n",
    "\n",
    "class KMaxPooling(Layer):\n",
    "    #...(생략)...\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # swap last two dimensions since top_k will be applied along the last dimension\n",
    "        shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
    "        \n",
    "        # extract top_k, returns two tensors [values, indices]\n",
    "        top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=True, name=None)[0]\n",
    "        \n",
    "        # return flattened output\n",
    "        return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/cnn_document_model.py - DocumentModel 클래스\n",
    "\n",
    "from keras.layers import Conv1D\n",
    "\n",
    "\n",
    "class DocumentModel:\n",
    "    # ...(생략)...\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # ...(생략)...\n",
    "    \n",
    "        for sent in range(self.num_sentences):\n",
    "            sentence =  Lambda(lambda x : x[:,sent*self.sentence_len: (sent+1)*self.sentence_len, :])(z) \n",
    "            conv = word_conv_model(sentence)\n",
    "\n",
    "            # KMax 풀링 입력 shape : (None, 26, 30)\n",
    "            conv = KMaxPooling(k=self.sent_k_maxpool)(conv)  # K 값은 디폴트 3\n",
    "            # KMax 풀링 출력 shape : (None, 30, 3)\n",
    "            \n",
    "            conv = Reshape([self.word_filters*self.sent_k_maxpool,1])(conv)\n",
    "            # Reshape 출력 shape : (None, 90, 1)\n",
    "            conv_blocks.append(conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 50  and the index of vocabulary words passed has 0 words\n",
      "WARNING:tensorflow:From /Users/dhkdn9192/venv/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "imdb_embedding (Embedding)      (None, 300, 50)      2500        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 30, 50)       0           imdb_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 30, 50)       0           imdb_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 30, 50)       0           imdb_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 30, 50)       0           imdb_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 30, 50)       0           imdb_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 30, 50)       0           imdb_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 30, 50)       0           imdb_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 30, 50)       0           imdb_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 30, 50)       0           imdb_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 30, 50)       0           imdb_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "word_conv (Conv1D)              (None, 26, 30)       7530        lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling (KMaxPooling)     (None, 30, 3)        0           word_conv[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_1 (KMaxPooling)   (None, 30, 3)        0           word_conv[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_2 (KMaxPooling)   (None, 30, 3)        0           word_conv[2][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_3 (KMaxPooling)   (None, 30, 3)        0           word_conv[3][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_4 (KMaxPooling)   (None, 30, 3)        0           word_conv[4][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_5 (KMaxPooling)   (None, 30, 3)        0           word_conv[5][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_6 (KMaxPooling)   (None, 30, 3)        0           word_conv[6][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_7 (KMaxPooling)   (None, 30, 3)        0           word_conv[7][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_8 (KMaxPooling)   (None, 30, 3)        0           word_conv[8][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_9 (KMaxPooling)   (None, 30, 3)        0           word_conv[9][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 90, 1)        0           k_max_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 90, 1)        0           k_max_pooling_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 90, 1)        0           k_max_pooling_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 90, 1)        0           k_max_pooling_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 90, 1)        0           k_max_pooling_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 90, 1)        0           k_max_pooling_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 90, 1)        0           k_max_pooling_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 90, 1)        0           k_max_pooling_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 90, 1)        0           k_max_pooling_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 90, 1)        0           k_max_pooling_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 90, 10)       0           reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "                                                                 reshape_8[0][0]                  \n",
      "                                                                 reshape_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sentence_embeddings (Permute)   (None, 10, 90)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sentence_conv (Conv1D)          (None, 6, 16)        7216        sentence_embeddings[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_10 (KMaxPooling)  (None, 16, 4)        0           sentence_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "document_embedding (Flatten)    (None, 64)           0           k_max_pooling_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 64)           0           document_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hidden_0 (Dense)                (None, 64)           4160        gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "final (Dense)                   (None, 1)            65          hidden_0[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 21,471\n",
      "Trainable params: 21,471\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from model.cnn_document_model import DocumentModel\n",
    "\n",
    "dmodel = DocumentModel(50, {})\n",
    "dmodel._model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) keras의 plot_model로 시각화\n",
    "- keras의 plot_model로 시각화를 하려면 아래 패키지를 설치\n",
    "    - Mac OS\n",
    "    ```\n",
    "    pip install pydot\n",
    "    pip install graphviz\n",
    "    brew install graphviz\n",
    "    ```\n",
    "\n",
    "    - Windows\n",
    "        - https://bobswift.atlassian.net/wiki/spaces/GVIZ/pages/20971549/How+to+install+Graphviz+software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "plot_model(dmodel._model, to_file='assets/handson02_model_detail.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model_detail](assets/handson02_model_detail.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
