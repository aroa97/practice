{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스타일 전이(Style Transfer)\n",
    "\n",
    "* 그림은 내용과 스타일의 복잡한 상호작용을 표현한다. 한편 사진은 원근과 빛의 조합이다. 이 둘을 합쳤을 때의 결과는 매우 놀랍다. 이 과정을 예술적 스타일 전이라고 한다. \n",
    "\n",
    "* 논문 “A Neural Algorithm for Artistic Style(예술적 스타일의 신경 알고리즘)”(https://arxiv.org/abs/1508.06576)에서 제시한 전이학습 알고리즘의 구현에 대해서 논의할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경스타일전이\n",
    "* 신경 스타일 전이는 특정 타깃 이미지에 참조 이미지의 스타일을 적용하면서 타깃 이미지의 원래 내용은 변경되지 않도록 하는 과정이다. \n",
    "* 여기서 주된 목적은 원본 타깃 이미지의 내용을 유지하면서 타깃 이미지에 참조 이미지의 스타일을 겹치거나 적용하는 것이다.\n",
    "* 이 개념을 수학적으로 표현하려면 다음과 같은 3가지 이미지를 고려해야 한다. 원본 내용(c로 표시), 참조 스타일(s로 표시), 생성된 이미지(g로 표시)가 그것이다. c와 g가 내용면에서 얼마나 다른 이미지인지 측정할 방법이 필요하다. 또한 출력 스타일의 특성이라는 측면에서 출력 이미지는 스타일 이미지와 차이가 크지 않아야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/dyanos/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dyanos/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dyanos/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dyanos/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dyanos/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dyanos/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/dyanos/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dyanos/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dyanos/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dyanos/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dyanos/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dyanos/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.applications import vgg16\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def preprocess_image(image_path, height=None, width=None):\n",
    "    height = 400 if not height else height\n",
    "    width = width if width else int(width * height / height)\n",
    "    img = load_img(image_path, target_size=(height, width))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg16.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):\n",
    "    #  평균 픽셀로 zero-center 제거\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dyanos/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# 변환하려는 이미지의 경로\n",
    "TARGET_IMG = 'lotr.jpg'\n",
    "# 스타일 이미지의 경로\n",
    "REFERENCE_STYLE_IMG = 'pattern1.jpg'\n",
    "\n",
    "width, height = load_img(TARGET_IMG).size\n",
    "img_height = 480\n",
    "img_width = int(width * img_height / height)\n",
    "\n",
    "\n",
    "target_image = K.constant(preprocess_image(TARGET_IMG, height=img_height, width=img_width))\n",
    "style_image = K.constant(preprocess_image(REFERENCE_STYLE_IMG, height=img_height, width=img_width))\n",
    "\n",
    "# 생성될 이미지의 플레이스홀더\n",
    "generated_image = K.placeholder((1, img_height, img_width, 3))\n",
    "\n",
    "# 3개의 이미지를 단일 배치로 결합\n",
    "input_tensor = K.concatenate([target_image,\n",
    "                              style_image,\n",
    "                              generated_image], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(1, 480, 2012, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dyanos/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dyanos/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dyanos/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dyanos/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dyanos/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = vgg16.VGG16(input_tensor=input_tensor,\n",
    "                    weights='imagenet',\n",
    "                    include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_1': <tf.Tensor 'concat:0' shape=(3, 480, 2012, 3) dtype=float32>,\n",
       " 'block1_conv1': <tf.Tensor 'block1_conv1/Relu:0' shape=(3, 480, 2012, 64) dtype=float32>,\n",
       " 'block1_conv2': <tf.Tensor 'block1_conv2/Relu:0' shape=(3, 480, 2012, 64) dtype=float32>,\n",
       " 'block1_pool': <tf.Tensor 'block1_pool/MaxPool:0' shape=(3, 240, 1006, 64) dtype=float32>,\n",
       " 'block2_conv1': <tf.Tensor 'block2_conv1/Relu:0' shape=(3, 240, 1006, 128) dtype=float32>,\n",
       " 'block2_conv2': <tf.Tensor 'block2_conv2/Relu:0' shape=(3, 240, 1006, 128) dtype=float32>,\n",
       " 'block2_pool': <tf.Tensor 'block2_pool/MaxPool:0' shape=(3, 120, 503, 128) dtype=float32>,\n",
       " 'block3_conv1': <tf.Tensor 'block3_conv1/Relu:0' shape=(3, 120, 503, 256) dtype=float32>,\n",
       " 'block3_conv2': <tf.Tensor 'block3_conv2/Relu:0' shape=(3, 120, 503, 256) dtype=float32>,\n",
       " 'block3_conv3': <tf.Tensor 'block3_conv3/Relu:0' shape=(3, 120, 503, 256) dtype=float32>,\n",
       " 'block3_pool': <tf.Tensor 'block3_pool/MaxPool:0' shape=(3, 60, 251, 256) dtype=float32>,\n",
       " 'block4_conv1': <tf.Tensor 'block4_conv1/Relu:0' shape=(3, 60, 251, 512) dtype=float32>,\n",
       " 'block4_conv2': <tf.Tensor 'block4_conv2/Relu:0' shape=(3, 60, 251, 512) dtype=float32>,\n",
       " 'block4_conv3': <tf.Tensor 'block4_conv3/Relu:0' shape=(3, 60, 251, 512) dtype=float32>,\n",
       " 'block4_pool': <tf.Tensor 'block4_pool/MaxPool:0' shape=(3, 30, 125, 512) dtype=float32>,\n",
       " 'block5_conv1': <tf.Tensor 'block5_conv1/Relu:0' shape=(3, 30, 125, 512) dtype=float32>,\n",
       " 'block5_conv2': <tf.Tensor 'block5_conv2/Relu:0' shape=(3, 30, 125, 512) dtype=float32>,\n",
       " 'block5_conv3': <tf.Tensor 'block5_conv3/Relu:0' shape=(3, 30, 125, 512) dtype=float32>,\n",
       " 'block5_pool': <tf.Tensor 'block5_pool/MaxPool:0' shape=(3, 15, 62, 512) dtype=float32>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = {l.name: l.output for l in model.layers}\n",
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 손실함수 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 내용 손실 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "CNN 기반 모델에서 최상위 계층의 활성화는 더 전역적이고 추상적인 정보(예: 얼굴과 같은 고수준 구조)를 포함하며, 하단 층은 이미지에 대한 국소 정보(예: 눈, 코, 에지, 코너와 같은 저수준 구조)를 포함한다. 여기서는 이미지 내용에 대한 올바른 표현을 포착하기 위해 CNN의 상층을 활용한다. 따라서 사전 훈련된 VGG-16 모델을 사용할 때의 내용 손실은 타깃 이미지를 통해 계산된 상위 층(주어진 특성 표현)의 활성화 계산과 생성된 이미지를 통해 계산된 동일한 층의 활성화 사이의 L2 노름(스케일과제곱의 유클리드 거리)으로 정의할 수 있다. 보통 CNN의 상위 층으로부터 이미지의 내용과 관련된 특성적 표현을 받는다고 가정하면 생성된 이미지는 기본 타깃 이미지와 유사해 보일 것이다. 다음 코드는 내용 손실을 계산하는 함수다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스타일 손실"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "논문 “A Neural Algorithm of Artistic Style” (https://arxiv.org/abs/1508.06576)에 따라 여기서는 그램 행렬을 사용할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(style, combination, height, width):\n",
    "    \n",
    "    def build_gram_matrix(x):\n",
    "        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "        gram_matrix = K.dot(features, K.transpose(features))\n",
    "        return gram_matrix\n",
    "\n",
    "    S = build_gram_matrix(style)\n",
    "    C = build_gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = height * width\n",
    "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 총 변동 손실"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스타일과 내용 손실을 줄이기 위한 최적화는 때때로 너무 픽셀화되고 노이즈가 있는 출력으로 이어지기도 한다. 이를 극복하기 위해 총 변동 손실이 도입됐다. 총 변동 손실(total variation loss)은 ‘정규화 손실’과 비슷하다. 발생할 이미지를 공간 연속적이고 매끄럽게 보이기 위해 도입됐기 때문에 화소에 노이즈가 과다하게 나타나는 것을 방지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    a = K.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] - x[:, 1:, :img_width - 1, :])\n",
    "    b = K.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] - x[:, :img_height - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 총 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경 전이를 위한 총 손실 함수의 구성 요소를 정의했으므로 다음 단계는 구성 요소를 결합하는 것이다. 내용 및 스타일 정보는 CNN 네트워크의 다른 깊이에서 포착되므로 적절한 층에 각각의 손실 유형을 적용하고 계산해야 한다. 스타일 손실을 위해서 합성곱층 1부터 5까지를 취하고 각 층에 적절한 가중치를 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중 평균 손실 함수에 대한 가중치\n",
    "content_weight = 0.05\n",
    "total_variation_weight = 1e-4\n",
    "\n",
    "\n",
    "content_layer = 'block4_conv2'\n",
    "style_layers = ['block{}_conv2'.format(o) for o in range(1,6)]\n",
    "style_weights = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "\n",
    "# 전체 손실 값 초기화\n",
    "loss = K.variable(0.)\n",
    "\n",
    "# 내용 손실 추가\n",
    "layer_features = layers[content_layer]\n",
    "target_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[2, :, :, :]\n",
    "loss = loss + (content_weight * content_loss(target_image_features,\n",
    "                                      combination_features))\n",
    "\n",
    "# 스타일 손실 추가\n",
    "for layer_name, sw in zip(style_layers, style_weights):\n",
    "    layer_features = layers[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    sl = style_loss(style_reference_features, combination_features, \n",
    "                    height=img_height, width=img_width)\n",
    "    loss = loss + sl*sw\n",
    "\n",
    "# 전체 편차의 손실 값 추가\n",
    "loss = loss + (total_variation_weight * total_variation_loss(generated_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dyanos/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# 손실로 생성된 이미지의 기울기를 구한다.\n",
    "grads = K.gradients(loss, generated_image)[0]\n",
    "\n",
    "# 현재 손실과 현재 기울기를 구하는 함수\n",
    "fetch_loss_and_grads = K.function([generated_image], [loss, grads])\n",
    "\n",
    "\n",
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self, height=None, width=None):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        x = x.reshape((1, self.height, self.width, 3))\n",
    "        outs = fetch_loss_and_grads([x])\n",
    "        loss_value = outs[0]\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "evaluator = Evaluator(height=img_height, width=img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of iteration 1\n",
      "Current loss value: 51848086000.0\n",
      "Image saved as st_res_lotr_iter1.png\n",
      "Iteration 1 completed in 20s\n",
      "Start of iteration 2\n",
      "Current loss value: 50630926000.0\n",
      "Iteration 2 completed in 14s\n",
      "Start of iteration 3\n",
      "Current loss value: 49781457000.0\n",
      "Iteration 3 completed in 14s\n",
      "Start of iteration 4\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "#from scipy.misc import imsave # scipy 1.3.0에서는 deprecated된 것이라 아래 것으로 대체\n",
    "from imageio import imwrite\n",
    "import time\n",
    "\n",
    "result_prefix = 'st_res_'+TARGET_IMG.split('.')[0]\n",
    "iterations = 20\n",
    "\n",
    "# Run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "# so as to minimize the neural style loss.\n",
    "# This is our initial state: the target image.\n",
    "# Note that `scipy.optimize.fmin_l_bfgs_b` can only process flat vectors.\n",
    "x = preprocess_image(TARGET_IMG, height=img_height, width=img_width)\n",
    "x = x.flatten()\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('Start of iteration', (i+1))\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x,\n",
    "                                     fprime=evaluator.grads, maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    if (i+1) % 5 == 0 or i == 0:\n",
    "        # Save current generated image only every 5 iterations\n",
    "        img = x.copy().reshape((img_height, img_width, 3))\n",
    "        img = deprocess_image(img)\n",
    "        fname = result_prefix + '_iter%d.png' %(i+1)\n",
    "        imwrite(fname, img)\n",
    "        print('Image saved as', fname)\n",
    "    end_time = time.time()\n",
    "    print('Iteration %d completed in %ds' % (i+1, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = io.imread('lotr.jpg')\n",
    "style_image = io.imread('pattern1.jpg')\n",
    "\n",
    "iter1 = io.imread('st_res_lotr_iter1.png')\n",
    "iter5 = io.imread('st_res_lotr_iter5.png')\n",
    "iter10 = io.imread('st_res_lotr_iter10.png')\n",
    "iter15 = io.imread('st_res_lotr_iter15.png')\n",
    "iter20 = io.imread('st_res_lotr_iter20.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15, 6))\n",
    "ax1 = fig.add_subplot(1,2, 1)\n",
    "ax1.imshow(content_image)\n",
    "t1 = ax1.set_title('Content Image')\n",
    "ax2 = fig.add_subplot(1,2, 2)\n",
    "ax2.imshow(style_image)\n",
    "t2 = ax2.set_title('Style Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15, 15))\n",
    "\n",
    "ax1 = fig.add_subplot(6,3, 1)\n",
    "ax1.imshow(content_image)\n",
    "t1 = ax1.set_title('Original')\n",
    "\n",
    "ax1 = fig.add_subplot(6,3, 2)\n",
    "ax1.imshow(iter1)\n",
    "t1 = ax1.set_title('Iteration 1')\n",
    "\n",
    "ax1 = fig.add_subplot(6,3, 3)\n",
    "ax1.imshow(iter5)\n",
    "t1 = ax1.set_title('Iteration 5')\n",
    "\n",
    "ax1 = fig.add_subplot(6,3, 4)\n",
    "ax1.imshow(iter10)\n",
    "t1 = ax1.set_title('Iteration 10')\n",
    "\n",
    "ax1 = fig.add_subplot(6,3, 5)\n",
    "ax1.imshow(iter15)\n",
    "t1 = ax1.set_title('Iteration 15')\n",
    "\n",
    "ax1 = fig.add_subplot(6,3, 6)\n",
    "ax1.imshow(iter20)\n",
    "t1 = ax1.set_title('Iteration 20')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "t = fig.suptitle('LOTR Scene after Style Transfer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15, 15))\n",
    "ax1 = fig.add_subplot(6,3, 1)\n",
    "ax1.imshow(content_image)\n",
    "t1 = ax1.set_title('Original')\n",
    "\n",
    "gen_images = [iter1,iter5, iter10, iter15, iter20]\n",
    "\n",
    "for i, img in enumerate(gen_images):\n",
    "    ax1 = fig.add_subplot(6,3,i+1)\n",
    "    ax1.imshow(content_image)\n",
    "    t1 = ax1.set_title('Iteration {}'.format(i+5))\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "t = fig.suptitle('LOTR Scene after Style Transfer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 20))\n",
    "\n",
    "ax1 = fig.add_subplot(2,1, 1)\n",
    "ax1.imshow(content_image)\n",
    "t1 = ax1.set_title('Original Image')\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(2,1, 2)\n",
    "ax1.imshow(iter20)\n",
    "t1 = ax1.set_title('Stylized Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
