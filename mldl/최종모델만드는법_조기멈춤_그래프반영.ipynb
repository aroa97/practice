{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/ex/mldl/data/wine_copy.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6497 entries, 0 to 6496\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       6497 non-null   float64\n",
      " 1   1       6497 non-null   float64\n",
      " 2   2       6497 non-null   float64\n",
      " 3   3       6497 non-null   float64\n",
      " 4   4       6497 non-null   float64\n",
      " 5   5       6497 non-null   float64\n",
      " 6   6       6497 non-null   float64\n",
      " 7   7       6497 non-null   float64\n",
      " 8   8       6497 non-null   float64\n",
      " 9   9       6497 non-null   float64\n",
      " 10  10      6497 non-null   float64\n",
      " 11  11      6497 non-null   int64  \n",
      " 12  12      6497 non-null   int64  \n",
      "dtypes: float64(11), int64(2)\n",
      "memory usage: 660.0 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5197.6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6497 * 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1299.4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6497 * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5197, 12)\n",
      "(1300, 12)\n",
      "(5197,)\n",
      "(1300,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelpath=\"./model/all/{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "modelpath=\"./model/all4/best_model.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patience=20 검증셋의 오차가 20번 이상 낮아지지 않을 경우 학습종료하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\mldl\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 775ms/step - accuracy: 0.8780 - loss: 0.4033\n",
      "Epoch 1: val_loss improved from inf to 0.32848, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8748 - loss: 0.3526 - val_accuracy: 0.8754 - val_loss: 0.3285\n",
      "Epoch 2/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8980 - loss: 0.2584\n",
      "Epoch 2: val_loss improved from 0.32848 to 0.25809, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9014 - loss: 0.2583 - val_accuracy: 0.9108 - val_loss: 0.2581\n",
      "Epoch 3/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9160 - loss: 0.2377\n",
      "Epoch 3: val_loss improved from 0.25809 to 0.22175, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9248 - loss: 0.2164 - val_accuracy: 0.9223 - val_loss: 0.2218\n",
      "Epoch 4/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9260 - loss: 0.2000\n",
      "Epoch 4: val_loss improved from 0.22175 to 0.21158, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9348 - loss: 0.1901 - val_accuracy: 0.9285 - val_loss: 0.2116\n",
      "Epoch 5/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9480 - loss: 0.1640\n",
      "Epoch 5: val_loss improved from 0.21158 to 0.20876, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9391 - loss: 0.1845 - val_accuracy: 0.9285 - val_loss: 0.2088\n",
      "Epoch 6/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9360 - loss: 0.1852\n",
      "Epoch 6: val_loss improved from 0.20876 to 0.20354, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9359 - loss: 0.1876 - val_accuracy: 0.9300 - val_loss: 0.2035\n",
      "Epoch 7/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9300 - loss: 0.1965\n",
      "Epoch 7: val_loss improved from 0.20354 to 0.20162, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9362 - loss: 0.1877 - val_accuracy: 0.9331 - val_loss: 0.2016\n",
      "Epoch 8/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9240 - loss: 0.2010\n",
      "Epoch 8: val_loss improved from 0.20162 to 0.20025, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9368 - loss: 0.1809 - val_accuracy: 0.9315 - val_loss: 0.2003\n",
      "Epoch 9/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9460 - loss: 0.1469\n",
      "Epoch 9: val_loss improved from 0.20025 to 0.19738, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9384 - loss: 0.1747 - val_accuracy: 0.9338 - val_loss: 0.1974\n",
      "Epoch 10/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9360 - loss: 0.2032\n",
      "Epoch 10: val_loss improved from 0.19738 to 0.19643, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9375 - loss: 0.1843 - val_accuracy: 0.9331 - val_loss: 0.1964\n",
      "Epoch 11/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9440 - loss: 0.1966\n",
      "Epoch 11: val_loss improved from 0.19643 to 0.19470, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9396 - loss: 0.1820 - val_accuracy: 0.9331 - val_loss: 0.1947\n",
      "Epoch 12/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9280 - loss: 0.1834\n",
      "Epoch 12: val_loss improved from 0.19470 to 0.19239, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9377 - loss: 0.1772 - val_accuracy: 0.9346 - val_loss: 0.1924\n",
      "Epoch 13/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9420 - loss: 0.1546\n",
      "Epoch 13: val_loss improved from 0.19239 to 0.19140, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9413 - loss: 0.1675 - val_accuracy: 0.9346 - val_loss: 0.1914\n",
      "Epoch 14/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9380 - loss: 0.1763\n",
      "Epoch 14: val_loss improved from 0.19140 to 0.18898, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9442 - loss: 0.1651 - val_accuracy: 0.9362 - val_loss: 0.1890\n",
      "Epoch 15/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9120 - loss: 0.2029\n",
      "Epoch 15: val_loss improved from 0.18898 to 0.18794, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9367 - loss: 0.1727 - val_accuracy: 0.9362 - val_loss: 0.1879\n",
      "Epoch 16/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9360 - loss: 0.1775\n",
      "Epoch 16: val_loss improved from 0.18794 to 0.18647, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9406 - loss: 0.1697 - val_accuracy: 0.9369 - val_loss: 0.1865\n",
      "Epoch 17/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9540 - loss: 0.1477\n",
      "Epoch 17: val_loss improved from 0.18647 to 0.18477, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9437 - loss: 0.1628 - val_accuracy: 0.9369 - val_loss: 0.1848\n",
      "Epoch 18/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9520 - loss: 0.1527\n",
      "Epoch 18: val_loss improved from 0.18477 to 0.18423, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9434 - loss: 0.1645 - val_accuracy: 0.9385 - val_loss: 0.1842\n",
      "Epoch 19/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9620 - loss: 0.1268\n",
      "Epoch 19: val_loss did not improve from 0.18423\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9475 - loss: 0.1517 - val_accuracy: 0.9346 - val_loss: 0.1856\n",
      "Epoch 20/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9440 - loss: 0.1809\n",
      "Epoch 20: val_loss did not improve from 0.18423\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9390 - loss: 0.1773 - val_accuracy: 0.9354 - val_loss: 0.1891\n",
      "Epoch 21/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9200 - loss: 0.1938\n",
      "Epoch 21: val_loss improved from 0.18423 to 0.18015, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9346 - loss: 0.1753 - val_accuracy: 0.9362 - val_loss: 0.1802\n",
      "Epoch 22/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9300 - loss: 0.1921\n",
      "Epoch 22: val_loss improved from 0.18015 to 0.17749, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9398 - loss: 0.1684 - val_accuracy: 0.9385 - val_loss: 0.1775\n",
      "Epoch 23/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9440 - loss: 0.1624\n",
      "Epoch 23: val_loss improved from 0.17749 to 0.17599, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9442 - loss: 0.1590 - val_accuracy: 0.9385 - val_loss: 0.1760\n",
      "Epoch 24/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9660 - loss: 0.1232\n",
      "Epoch 24: val_loss improved from 0.17599 to 0.17449, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9456 - loss: 0.1555 - val_accuracy: 0.9385 - val_loss: 0.1745\n",
      "Epoch 25/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9460 - loss: 0.1324\n",
      "Epoch 25: val_loss improved from 0.17449 to 0.17253, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9406 - loss: 0.1566 - val_accuracy: 0.9392 - val_loss: 0.1725\n",
      "Epoch 26/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9540 - loss: 0.1425\n",
      "Epoch 26: val_loss improved from 0.17253 to 0.17172, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9463 - loss: 0.1539 - val_accuracy: 0.9408 - val_loss: 0.1717\n",
      "Epoch 27/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9500 - loss: 0.1368\n",
      "Epoch 27: val_loss improved from 0.17172 to 0.16966, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9493 - loss: 0.1438 - val_accuracy: 0.9408 - val_loss: 0.1697\n",
      "Epoch 28/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9520 - loss: 0.1320\n",
      "Epoch 28: val_loss improved from 0.16966 to 0.16777, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9470 - loss: 0.1470 - val_accuracy: 0.9408 - val_loss: 0.1678\n",
      "Epoch 29/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9540 - loss: 0.1261\n",
      "Epoch 29: val_loss did not improve from 0.16777\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9478 - loss: 0.1446 - val_accuracy: 0.9385 - val_loss: 0.1678\n",
      "Epoch 30/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9420 - loss: 0.1436\n",
      "Epoch 30: val_loss improved from 0.16777 to 0.16432, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9453 - loss: 0.1480 - val_accuracy: 0.9423 - val_loss: 0.1643\n",
      "Epoch 31/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9580 - loss: 0.1283\n",
      "Epoch 31: val_loss improved from 0.16432 to 0.16220, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9523 - loss: 0.1373 - val_accuracy: 0.9438 - val_loss: 0.1622\n",
      "Epoch 32/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9480 - loss: 0.1541\n",
      "Epoch 32: val_loss improved from 0.16220 to 0.16144, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9469 - loss: 0.1493 - val_accuracy: 0.9438 - val_loss: 0.1614\n",
      "Epoch 33/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9480 - loss: 0.1354\n",
      "Epoch 33: val_loss improved from 0.16144 to 0.15931, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9455 - loss: 0.1441 - val_accuracy: 0.9438 - val_loss: 0.1593\n",
      "Epoch 34/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9560 - loss: 0.1348\n",
      "Epoch 34: val_loss improved from 0.15931 to 0.15698, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9504 - loss: 0.1398 - val_accuracy: 0.9446 - val_loss: 0.1570\n",
      "Epoch 35/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9480 - loss: 0.1376\n",
      "Epoch 35: val_loss improved from 0.15698 to 0.15631, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9473 - loss: 0.1409 - val_accuracy: 0.9462 - val_loss: 0.1563\n",
      "Epoch 36/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9440 - loss: 0.1503\n",
      "Epoch 36: val_loss improved from 0.15631 to 0.15404, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9471 - loss: 0.1427 - val_accuracy: 0.9438 - val_loss: 0.1540\n",
      "Epoch 37/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9540 - loss: 0.1274\n",
      "Epoch 37: val_loss improved from 0.15404 to 0.15277, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9492 - loss: 0.1362 - val_accuracy: 0.9462 - val_loss: 0.1528\n",
      "Epoch 38/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9460 - loss: 0.1296\n",
      "Epoch 38: val_loss improved from 0.15277 to 0.14951, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9492 - loss: 0.1316 - val_accuracy: 0.9469 - val_loss: 0.1495\n",
      "Epoch 39/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9660 - loss: 0.1084\n",
      "Epoch 39: val_loss did not improve from 0.14951\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9495 - loss: 0.1374 - val_accuracy: 0.9462 - val_loss: 0.1551\n",
      "Epoch 40/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9440 - loss: 0.1564\n",
      "Epoch 40: val_loss did not improve from 0.14951\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9515 - loss: 0.1344 - val_accuracy: 0.9423 - val_loss: 0.1536\n",
      "Epoch 41/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9520 - loss: 0.1578\n",
      "Epoch 41: val_loss improved from 0.14951 to 0.14452, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9539 - loss: 0.1337 - val_accuracy: 0.9477 - val_loss: 0.1445\n",
      "Epoch 42/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9580 - loss: 0.1147\n",
      "Epoch 42: val_loss did not improve from 0.14452\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9536 - loss: 0.1245 - val_accuracy: 0.9462 - val_loss: 0.1482\n",
      "Epoch 43/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9480 - loss: 0.1332\n",
      "Epoch 43: val_loss improved from 0.14452 to 0.14111, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9501 - loss: 0.1302 - val_accuracy: 0.9477 - val_loss: 0.1411\n",
      "Epoch 44/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9540 - loss: 0.1422\n",
      "Epoch 44: val_loss improved from 0.14111 to 0.13976, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9526 - loss: 0.1294 - val_accuracy: 0.9477 - val_loss: 0.1398\n",
      "Epoch 45/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9640 - loss: 0.1020\n",
      "Epoch 45: val_loss improved from 0.13976 to 0.13769, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9567 - loss: 0.1206 - val_accuracy: 0.9485 - val_loss: 0.1377\n",
      "Epoch 46/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9580 - loss: 0.1154\n",
      "Epoch 46: val_loss improved from 0.13769 to 0.13696, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9546 - loss: 0.1165 - val_accuracy: 0.9500 - val_loss: 0.1370\n",
      "Epoch 47/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9660 - loss: 0.1053\n",
      "Epoch 47: val_loss improved from 0.13696 to 0.13467, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9577 - loss: 0.1156 - val_accuracy: 0.9500 - val_loss: 0.1347\n",
      "Epoch 48/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9660 - loss: 0.0969\n",
      "Epoch 48: val_loss improved from 0.13467 to 0.13376, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9588 - loss: 0.1138 - val_accuracy: 0.9500 - val_loss: 0.1338\n",
      "Epoch 49/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9680 - loss: 0.0864\n",
      "Epoch 49: val_loss improved from 0.13376 to 0.13105, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9595 - loss: 0.1087 - val_accuracy: 0.9523 - val_loss: 0.1310\n",
      "Epoch 50/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9620 - loss: 0.1144\n",
      "Epoch 50: val_loss improved from 0.13105 to 0.13101, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9581 - loss: 0.1151 - val_accuracy: 0.9515 - val_loss: 0.1310\n",
      "Epoch 51/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9520 - loss: 0.1163\n",
      "Epoch 51: val_loss improved from 0.13101 to 0.12882, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9576 - loss: 0.1144 - val_accuracy: 0.9515 - val_loss: 0.1288\n",
      "Epoch 52/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9600 - loss: 0.1148\n",
      "Epoch 52: val_loss improved from 0.12882 to 0.12578, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9555 - loss: 0.1121 - val_accuracy: 0.9523 - val_loss: 0.1258\n",
      "Epoch 53/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9620 - loss: 0.1031\n",
      "Epoch 53: val_loss improved from 0.12578 to 0.12411, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9593 - loss: 0.1059 - val_accuracy: 0.9600 - val_loss: 0.1241\n",
      "Epoch 54/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9600 - loss: 0.0942\n",
      "Epoch 54: val_loss improved from 0.12411 to 0.12196, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9619 - loss: 0.1034 - val_accuracy: 0.9608 - val_loss: 0.1220\n",
      "Epoch 55/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9660 - loss: 0.0872\n",
      "Epoch 55: val_loss improved from 0.12196 to 0.11993, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9624 - loss: 0.1039 - val_accuracy: 0.9577 - val_loss: 0.1199\n",
      "Epoch 56/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9620 - loss: 0.1023\n",
      "Epoch 56: val_loss did not improve from 0.11993\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9623 - loss: 0.1069 - val_accuracy: 0.9562 - val_loss: 0.1216\n",
      "Epoch 57/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9560 - loss: 0.1127\n",
      "Epoch 57: val_loss did not improve from 0.11993\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9639 - loss: 0.1045 - val_accuracy: 0.9569 - val_loss: 0.1207\n",
      "Epoch 58/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9660 - loss: 0.1072\n",
      "Epoch 58: val_loss did not improve from 0.11993\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9619 - loss: 0.1054 - val_accuracy: 0.9485 - val_loss: 0.1268\n",
      "Epoch 59/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9640 - loss: 0.0954\n",
      "Epoch 59: val_loss did not improve from 0.11993\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9582 - loss: 0.1103 - val_accuracy: 0.9508 - val_loss: 0.1228\n",
      "Epoch 60/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9620 - loss: 0.0895\n",
      "Epoch 60: val_loss improved from 0.11993 to 0.11247, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9609 - loss: 0.0984 - val_accuracy: 0.9592 - val_loss: 0.1125\n",
      "Epoch 61/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9620 - loss: 0.1085\n",
      "Epoch 61: val_loss improved from 0.11247 to 0.11091, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9641 - loss: 0.0954 - val_accuracy: 0.9700 - val_loss: 0.1109\n",
      "Epoch 62/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9700 - loss: 0.0965\n",
      "Epoch 62: val_loss improved from 0.11091 to 0.10791, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9655 - loss: 0.0987 - val_accuracy: 0.9685 - val_loss: 0.1079\n",
      "Epoch 63/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9700 - loss: 0.0708\n",
      "Epoch 63: val_loss improved from 0.10791 to 0.10623, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9654 - loss: 0.0916 - val_accuracy: 0.9715 - val_loss: 0.1062\n",
      "Epoch 64/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9760 - loss: 0.0917\n",
      "Epoch 64: val_loss did not improve from 0.10623\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9721 - loss: 0.0906 - val_accuracy: 0.9731 - val_loss: 0.1069\n",
      "Epoch 65/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9780 - loss: 0.0784\n",
      "Epoch 65: val_loss improved from 0.10623 to 0.10375, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9672 - loss: 0.0916 - val_accuracy: 0.9738 - val_loss: 0.1038\n",
      "Epoch 66/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9660 - loss: 0.0953\n",
      "Epoch 66: val_loss did not improve from 0.10375\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9706 - loss: 0.0870 - val_accuracy: 0.9708 - val_loss: 0.1064\n",
      "Epoch 67/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9720 - loss: 0.0800\n",
      "Epoch 67: val_loss improved from 0.10375 to 0.10087, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9731 - loss: 0.0854 - val_accuracy: 0.9738 - val_loss: 0.1009\n",
      "Epoch 68/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9740 - loss: 0.0947\n",
      "Epoch 68: val_loss did not improve from 0.10087\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9748 - loss: 0.0880 - val_accuracy: 0.9700 - val_loss: 0.1040\n",
      "Epoch 69/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9720 - loss: 0.0936\n",
      "Epoch 69: val_loss improved from 0.10087 to 0.09518, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9731 - loss: 0.0871 - val_accuracy: 0.9738 - val_loss: 0.0952\n",
      "Epoch 70/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9720 - loss: 0.0837\n",
      "Epoch 70: val_loss did not improve from 0.09518\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9745 - loss: 0.0765 - val_accuracy: 0.9708 - val_loss: 0.0995\n",
      "Epoch 71/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9780 - loss: 0.0793\n",
      "Epoch 71: val_loss improved from 0.09518 to 0.09263, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9760 - loss: 0.0771 - val_accuracy: 0.9738 - val_loss: 0.0926\n",
      "Epoch 72/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9800 - loss: 0.0718\n",
      "Epoch 72: val_loss improved from 0.09263 to 0.09214, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9767 - loss: 0.0735 - val_accuracy: 0.9762 - val_loss: 0.0921\n",
      "Epoch 73/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9680 - loss: 0.0944\n",
      "Epoch 73: val_loss improved from 0.09214 to 0.09133, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9738 - loss: 0.0820 - val_accuracy: 0.9762 - val_loss: 0.0913\n",
      "Epoch 74/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9720 - loss: 0.0734\n",
      "Epoch 74: val_loss did not improve from 0.09133\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9768 - loss: 0.0762 - val_accuracy: 0.9723 - val_loss: 0.0947\n",
      "Epoch 75/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9600 - loss: 0.1007\n",
      "Epoch 75: val_loss improved from 0.09133 to 0.08942, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9724 - loss: 0.0790 - val_accuracy: 0.9762 - val_loss: 0.0894\n",
      "Epoch 76/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9760 - loss: 0.0734\n",
      "Epoch 76: val_loss improved from 0.08942 to 0.08938, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9786 - loss: 0.0744 - val_accuracy: 0.9769 - val_loss: 0.0894\n",
      "Epoch 77/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9700 - loss: 0.0947\n",
      "Epoch 77: val_loss did not improve from 0.08938\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9752 - loss: 0.0777 - val_accuracy: 0.9754 - val_loss: 0.0904\n",
      "Epoch 78/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9680 - loss: 0.0904\n",
      "Epoch 78: val_loss improved from 0.08938 to 0.08799, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9774 - loss: 0.0743 - val_accuracy: 0.9777 - val_loss: 0.0880\n",
      "Epoch 79/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9760 - loss: 0.0784\n",
      "Epoch 79: val_loss did not improve from 0.08799\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9764 - loss: 0.0729 - val_accuracy: 0.9723 - val_loss: 0.0910\n",
      "Epoch 80/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9720 - loss: 0.0764\n",
      "Epoch 80: val_loss improved from 0.08799 to 0.08600, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9779 - loss: 0.0658 - val_accuracy: 0.9785 - val_loss: 0.0860\n",
      "Epoch 81/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9780 - loss: 0.0746\n",
      "Epoch 81: val_loss improved from 0.08600 to 0.08521, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9787 - loss: 0.0698 - val_accuracy: 0.9800 - val_loss: 0.0852\n",
      "Epoch 82/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9760 - loss: 0.0888\n",
      "Epoch 82: val_loss did not improve from 0.08521\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9783 - loss: 0.0694 - val_accuracy: 0.9762 - val_loss: 0.0864\n",
      "Epoch 83/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9780 - loss: 0.0715\n",
      "Epoch 83: val_loss improved from 0.08521 to 0.08467, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9786 - loss: 0.0694 - val_accuracy: 0.9777 - val_loss: 0.0847\n",
      "Epoch 84/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9760 - loss: 0.0550\n",
      "Epoch 84: val_loss did not improve from 0.08467\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9771 - loss: 0.0629 - val_accuracy: 0.9777 - val_loss: 0.0900\n",
      "Epoch 85/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9840 - loss: 0.0521\n",
      "Epoch 85: val_loss did not improve from 0.08467\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9758 - loss: 0.0680 - val_accuracy: 0.9731 - val_loss: 0.0929\n",
      "Epoch 86/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9740 - loss: 0.0738\n",
      "Epoch 86: val_loss did not improve from 0.08467\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9757 - loss: 0.0728 - val_accuracy: 0.9785 - val_loss: 0.0860\n",
      "Epoch 87/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9860 - loss: 0.0489\n",
      "Epoch 87: val_loss improved from 0.08467 to 0.08258, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9817 - loss: 0.0620 - val_accuracy: 0.9785 - val_loss: 0.0826\n",
      "Epoch 88/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9880 - loss: 0.0523\n",
      "Epoch 88: val_loss improved from 0.08258 to 0.08235, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9807 - loss: 0.0604 - val_accuracy: 0.9792 - val_loss: 0.0824\n",
      "Epoch 89/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9760 - loss: 0.0731\n",
      "Epoch 89: val_loss did not improve from 0.08235\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9812 - loss: 0.0607 - val_accuracy: 0.9762 - val_loss: 0.0854\n",
      "Epoch 90/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9860 - loss: 0.0597\n",
      "Epoch 90: val_loss did not improve from 0.08235\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9816 - loss: 0.0633 - val_accuracy: 0.9738 - val_loss: 0.0854\n",
      "Epoch 91/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9760 - loss: 0.0671\n",
      "Epoch 91: val_loss did not improve from 0.08235\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9777 - loss: 0.0650 - val_accuracy: 0.9808 - val_loss: 0.0828\n",
      "Epoch 92/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9820 - loss: 0.0776\n",
      "Epoch 92: val_loss did not improve from 0.08235\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9800 - loss: 0.0655 - val_accuracy: 0.9800 - val_loss: 0.0826\n",
      "Epoch 93/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9860 - loss: 0.0460\n",
      "Epoch 93: val_loss improved from 0.08235 to 0.08201, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9830 - loss: 0.0575 - val_accuracy: 0.9792 - val_loss: 0.0820\n",
      "Epoch 94/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.0562\n",
      "Epoch 94: val_loss improved from 0.08201 to 0.08068, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9817 - loss: 0.0615 - val_accuracy: 0.9808 - val_loss: 0.0807\n",
      "Epoch 95/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9840 - loss: 0.0487\n",
      "Epoch 95: val_loss did not improve from 0.08068\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9823 - loss: 0.0570 - val_accuracy: 0.9800 - val_loss: 0.0808\n",
      "Epoch 96/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9920 - loss: 0.0476\n",
      "Epoch 96: val_loss did not improve from 0.08068\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9829 - loss: 0.0557 - val_accuracy: 0.9800 - val_loss: 0.0814\n",
      "Epoch 97/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9880 - loss: 0.0377\n",
      "Epoch 97: val_loss did not improve from 0.08068\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9814 - loss: 0.0586 - val_accuracy: 0.9792 - val_loss: 0.0818\n",
      "Epoch 98/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9760 - loss: 0.0681\n",
      "Epoch 98: val_loss did not improve from 0.08068\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9800 - loss: 0.0658 - val_accuracy: 0.9785 - val_loss: 0.0819\n",
      "Epoch 99/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9720 - loss: 0.0526\n",
      "Epoch 99: val_loss improved from 0.08068 to 0.08008, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9795 - loss: 0.0577 - val_accuracy: 0.9792 - val_loss: 0.0801\n",
      "Epoch 100/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9800 - loss: 0.0517\n",
      "Epoch 100: val_loss improved from 0.08008 to 0.07954, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9829 - loss: 0.0555 - val_accuracy: 0.9792 - val_loss: 0.0795\n",
      "Epoch 101/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9860 - loss: 0.0637\n",
      "Epoch 101: val_loss did not improve from 0.07954\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9835 - loss: 0.0606 - val_accuracy: 0.9723 - val_loss: 0.0909\n",
      "Epoch 102/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9840 - loss: 0.0732\n",
      "Epoch 102: val_loss did not improve from 0.07954\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9803 - loss: 0.0672 - val_accuracy: 0.9754 - val_loss: 0.0856\n",
      "Epoch 103/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9840 - loss: 0.0437\n",
      "Epoch 103: val_loss did not improve from 0.07954\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9821 - loss: 0.0589 - val_accuracy: 0.9777 - val_loss: 0.0836\n",
      "Epoch 104/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9800 - loss: 0.0692\n",
      "Epoch 104: val_loss did not improve from 0.07954\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9837 - loss: 0.0593 - val_accuracy: 0.9785 - val_loss: 0.0811\n",
      "Epoch 105/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9920 - loss: 0.0429\n",
      "Epoch 105: val_loss did not improve from 0.07954\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9864 - loss: 0.0515 - val_accuracy: 0.9800 - val_loss: 0.0797\n",
      "Epoch 106/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9780 - loss: 0.0561\n",
      "Epoch 106: val_loss improved from 0.07954 to 0.07899, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9813 - loss: 0.0538 - val_accuracy: 0.9792 - val_loss: 0.0790\n",
      "Epoch 107/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9780 - loss: 0.0478\n",
      "Epoch 107: val_loss did not improve from 0.07899\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9796 - loss: 0.0594 - val_accuracy: 0.9785 - val_loss: 0.0810\n",
      "Epoch 108/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9820 - loss: 0.0427\n",
      "Epoch 108: val_loss did not improve from 0.07899\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9812 - loss: 0.0525 - val_accuracy: 0.9777 - val_loss: 0.0824\n",
      "Epoch 109/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9720 - loss: 0.0896\n",
      "Epoch 109: val_loss improved from 0.07899 to 0.07793, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9802 - loss: 0.0637 - val_accuracy: 0.9815 - val_loss: 0.0779\n",
      "Epoch 110/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9860 - loss: 0.0452\n",
      "Epoch 110: val_loss did not improve from 0.07793\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9834 - loss: 0.0533 - val_accuracy: 0.9808 - val_loss: 0.0786\n",
      "Epoch 111/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9940 - loss: 0.0321\n",
      "Epoch 111: val_loss did not improve from 0.07793\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9889 - loss: 0.0439 - val_accuracy: 0.9800 - val_loss: 0.0800\n",
      "Epoch 112/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9940 - loss: 0.0450\n",
      "Epoch 112: val_loss did not improve from 0.07793\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9860 - loss: 0.0503 - val_accuracy: 0.9815 - val_loss: 0.0781\n",
      "Epoch 113/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9880 - loss: 0.0486\n",
      "Epoch 113: val_loss did not improve from 0.07793\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9861 - loss: 0.0520 - val_accuracy: 0.9785 - val_loss: 0.0832\n",
      "Epoch 114/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9900 - loss: 0.0454\n",
      "Epoch 114: val_loss did not improve from 0.07793\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9856 - loss: 0.0506 - val_accuracy: 0.9792 - val_loss: 0.0785\n",
      "Epoch 115/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9820 - loss: 0.0529\n",
      "Epoch 115: val_loss did not improve from 0.07793\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9840 - loss: 0.0503 - val_accuracy: 0.9800 - val_loss: 0.0790\n",
      "Epoch 116/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9840 - loss: 0.0533\n",
      "Epoch 116: val_loss did not improve from 0.07793\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9855 - loss: 0.0511 - val_accuracy: 0.9815 - val_loss: 0.0781\n",
      "Epoch 117/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9780 - loss: 0.0615\n",
      "Epoch 117: val_loss improved from 0.07793 to 0.07787, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9826 - loss: 0.0543 - val_accuracy: 0.9815 - val_loss: 0.0779\n",
      "Epoch 118/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9840 - loss: 0.0536\n",
      "Epoch 118: val_loss improved from 0.07787 to 0.07748, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9847 - loss: 0.0541 - val_accuracy: 0.9808 - val_loss: 0.0775\n",
      "Epoch 119/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.0383\n",
      "Epoch 119: val_loss improved from 0.07748 to 0.07707, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9867 - loss: 0.0442 - val_accuracy: 0.9831 - val_loss: 0.0771\n",
      "Epoch 120/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9840 - loss: 0.0424\n",
      "Epoch 120: val_loss did not improve from 0.07707\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9858 - loss: 0.0490 - val_accuracy: 0.9792 - val_loss: 0.0773\n",
      "Epoch 121/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9780 - loss: 0.0637\n",
      "Epoch 121: val_loss did not improve from 0.07707\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9820 - loss: 0.0563 - val_accuracy: 0.9785 - val_loss: 0.0798\n",
      "Epoch 122/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9840 - loss: 0.0453\n",
      "Epoch 122: val_loss did not improve from 0.07707\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9832 - loss: 0.0532 - val_accuracy: 0.9792 - val_loss: 0.0810\n",
      "Epoch 123/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9880 - loss: 0.0371\n",
      "Epoch 123: val_loss did not improve from 0.07707\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9852 - loss: 0.0530 - val_accuracy: 0.9762 - val_loss: 0.0856\n",
      "Epoch 124/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9820 - loss: 0.0710\n",
      "Epoch 124: val_loss did not improve from 0.07707\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9834 - loss: 0.0602 - val_accuracy: 0.9792 - val_loss: 0.0772\n",
      "Epoch 125/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9860 - loss: 0.0448\n",
      "Epoch 125: val_loss did not improve from 0.07707\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0499 - val_accuracy: 0.9777 - val_loss: 0.0773\n",
      "Epoch 126/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9880 - loss: 0.0414\n",
      "Epoch 126: val_loss improved from 0.07707 to 0.07643, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9873 - loss: 0.0448 - val_accuracy: 0.9800 - val_loss: 0.0764\n",
      "Epoch 127/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9860 - loss: 0.0470\n",
      "Epoch 127: val_loss did not improve from 0.07643\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9872 - loss: 0.0439 - val_accuracy: 0.9800 - val_loss: 0.0782\n",
      "Epoch 128/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9900 - loss: 0.0477\n",
      "Epoch 128: val_loss did not improve from 0.07643\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9876 - loss: 0.0463 - val_accuracy: 0.9785 - val_loss: 0.0826\n",
      "Epoch 129/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9900 - loss: 0.0399\n",
      "Epoch 129: val_loss improved from 0.07643 to 0.07628, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9858 - loss: 0.0501 - val_accuracy: 0.9815 - val_loss: 0.0763\n",
      "Epoch 130/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9820 - loss: 0.0487\n",
      "Epoch 130: val_loss improved from 0.07628 to 0.07623, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9852 - loss: 0.0491 - val_accuracy: 0.9815 - val_loss: 0.0762\n",
      "Epoch 131/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9800 - loss: 0.0513\n",
      "Epoch 131: val_loss did not improve from 0.07623\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9849 - loss: 0.0513 - val_accuracy: 0.9777 - val_loss: 0.0794\n",
      "Epoch 132/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9780 - loss: 0.0762\n",
      "Epoch 132: val_loss did not improve from 0.07623\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9856 - loss: 0.0525 - val_accuracy: 0.9808 - val_loss: 0.0763\n",
      "Epoch 133/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9860 - loss: 0.0451\n",
      "Epoch 133: val_loss did not improve from 0.07623\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9858 - loss: 0.0496 - val_accuracy: 0.9815 - val_loss: 0.0767\n",
      "Epoch 134/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9840 - loss: 0.0525\n",
      "Epoch 134: val_loss did not improve from 0.07623\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9858 - loss: 0.0497 - val_accuracy: 0.9808 - val_loss: 0.0770\n",
      "Epoch 135/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9880 - loss: 0.0349\n",
      "Epoch 135: val_loss did not improve from 0.07623\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9884 - loss: 0.0420 - val_accuracy: 0.9815 - val_loss: 0.0767\n",
      "Epoch 136/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9800 - loss: 0.0609\n",
      "Epoch 136: val_loss did not improve from 0.07623\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9865 - loss: 0.0458 - val_accuracy: 0.9800 - val_loss: 0.0775\n",
      "Epoch 137/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9900 - loss: 0.0353\n",
      "Epoch 137: val_loss did not improve from 0.07623\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9891 - loss: 0.0391 - val_accuracy: 0.9808 - val_loss: 0.0774\n",
      "Epoch 138/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9920 - loss: 0.0387\n",
      "Epoch 138: val_loss did not improve from 0.07623\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0447 - val_accuracy: 0.9808 - val_loss: 0.0764\n",
      "Epoch 139/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9900 - loss: 0.0334\n",
      "Epoch 139: val_loss did not improve from 0.07623\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9823 - loss: 0.0526 - val_accuracy: 0.9785 - val_loss: 0.0791\n",
      "Epoch 140/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9860 - loss: 0.0432\n",
      "Epoch 140: val_loss did not improve from 0.07623\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9866 - loss: 0.0461 - val_accuracy: 0.9785 - val_loss: 0.0785\n",
      "Epoch 141/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9840 - loss: 0.0626\n",
      "Epoch 141: val_loss did not improve from 0.07623\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9840 - loss: 0.0568 - val_accuracy: 0.9754 - val_loss: 0.0877\n",
      "Epoch 142/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9700 - loss: 0.0860\n",
      "Epoch 142: val_loss did not improve from 0.07623\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9808 - loss: 0.0602 - val_accuracy: 0.9777 - val_loss: 0.0811\n",
      "Epoch 143/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9860 - loss: 0.0473\n",
      "Epoch 143: val_loss improved from 0.07623 to 0.07556, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9866 - loss: 0.0490 - val_accuracy: 0.9815 - val_loss: 0.0756\n",
      "Epoch 144/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9900 - loss: 0.0644\n",
      "Epoch 144: val_loss did not improve from 0.07556\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9868 - loss: 0.0519 - val_accuracy: 0.9785 - val_loss: 0.0767\n",
      "Epoch 145/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9820 - loss: 0.0350\n",
      "Epoch 145: val_loss did not improve from 0.07556\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9842 - loss: 0.0509 - val_accuracy: 0.9762 - val_loss: 0.0859\n",
      "Epoch 146/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.0340\n",
      "Epoch 146: val_loss did not improve from 0.07556\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9818 - loss: 0.0498 - val_accuracy: 0.9785 - val_loss: 0.0800\n",
      "Epoch 147/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9760 - loss: 0.0711\n",
      "Epoch 147: val_loss did not improve from 0.07556\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9830 - loss: 0.0565 - val_accuracy: 0.9777 - val_loss: 0.0790\n",
      "Epoch 148/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9760 - loss: 0.0779\n",
      "Epoch 148: val_loss improved from 0.07556 to 0.07502, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9838 - loss: 0.0520 - val_accuracy: 0.9808 - val_loss: 0.0750\n",
      "Epoch 149/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.0409\n",
      "Epoch 149: val_loss improved from 0.07502 to 0.07494, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9872 - loss: 0.0459 - val_accuracy: 0.9838 - val_loss: 0.0749\n",
      "Epoch 150/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9920 - loss: 0.0288\n",
      "Epoch 150: val_loss did not improve from 0.07494\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9888 - loss: 0.0379 - val_accuracy: 0.9777 - val_loss: 0.0800\n",
      "Epoch 151/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9760 - loss: 0.0886\n",
      "Epoch 151: val_loss did not improve from 0.07494\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9844 - loss: 0.0555 - val_accuracy: 0.9777 - val_loss: 0.0788\n",
      "Epoch 152/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9880 - loss: 0.0508\n",
      "Epoch 152: val_loss did not improve from 0.07494\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9874 - loss: 0.0468 - val_accuracy: 0.9785 - val_loss: 0.0788\n",
      "Epoch 153/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9800 - loss: 0.0719\n",
      "Epoch 153: val_loss did not improve from 0.07494\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9843 - loss: 0.0570 - val_accuracy: 0.9785 - val_loss: 0.0756\n",
      "Epoch 154/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9860 - loss: 0.0435\n",
      "Epoch 154: val_loss did not improve from 0.07494\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9867 - loss: 0.0474 - val_accuracy: 0.9785 - val_loss: 0.0758\n",
      "Epoch 155/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9800 - loss: 0.0780\n",
      "Epoch 155: val_loss improved from 0.07494 to 0.07440, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9862 - loss: 0.0529 - val_accuracy: 0.9808 - val_loss: 0.0744\n",
      "Epoch 156/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9940 - loss: 0.0392\n",
      "Epoch 156: val_loss did not improve from 0.07440\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9887 - loss: 0.0410 - val_accuracy: 0.9831 - val_loss: 0.0749\n",
      "Epoch 157/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9920 - loss: 0.0311\n",
      "Epoch 157: val_loss did not improve from 0.07440\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9875 - loss: 0.0438 - val_accuracy: 0.9815 - val_loss: 0.0746\n",
      "Epoch 158/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9940 - loss: 0.0251\n",
      "Epoch 158: val_loss did not improve from 0.07440\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9872 - loss: 0.0425 - val_accuracy: 0.9769 - val_loss: 0.0819\n",
      "Epoch 159/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9920 - loss: 0.0306\n",
      "Epoch 159: val_loss did not improve from 0.07440\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9837 - loss: 0.0532 - val_accuracy: 0.9769 - val_loss: 0.0780\n",
      "Epoch 160/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9860 - loss: 0.0358\n",
      "Epoch 160: val_loss improved from 0.07440 to 0.07366, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9859 - loss: 0.0443 - val_accuracy: 0.9815 - val_loss: 0.0737\n",
      "Epoch 161/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9900 - loss: 0.0373\n",
      "Epoch 161: val_loss did not improve from 0.07366\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9863 - loss: 0.0477 - val_accuracy: 0.9792 - val_loss: 0.0746\n",
      "Epoch 162/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9860 - loss: 0.0403\n",
      "Epoch 162: val_loss did not improve from 0.07366\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9876 - loss: 0.0439 - val_accuracy: 0.9808 - val_loss: 0.0750\n",
      "Epoch 163/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9800 - loss: 0.0680\n",
      "Epoch 163: val_loss did not improve from 0.07366\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9854 - loss: 0.0495 - val_accuracy: 0.9831 - val_loss: 0.0744\n",
      "Epoch 164/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9920 - loss: 0.0330\n",
      "Epoch 164: val_loss did not improve from 0.07366\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9865 - loss: 0.0455 - val_accuracy: 0.9846 - val_loss: 0.0751\n",
      "Epoch 165/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9920 - loss: 0.0617\n",
      "Epoch 165: val_loss did not improve from 0.07366\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9877 - loss: 0.0517 - val_accuracy: 0.9831 - val_loss: 0.0748\n",
      "Epoch 166/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9900 - loss: 0.0442\n",
      "Epoch 166: val_loss did not improve from 0.07366\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9883 - loss: 0.0462 - val_accuracy: 0.9823 - val_loss: 0.0744\n",
      "Epoch 167/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9900 - loss: 0.0357\n",
      "Epoch 167: val_loss improved from 0.07366 to 0.07307, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9900 - loss: 0.0384 - val_accuracy: 0.9823 - val_loss: 0.0731\n",
      "Epoch 168/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9820 - loss: 0.0464\n",
      "Epoch 168: val_loss improved from 0.07307 to 0.07288, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9830 - loss: 0.0523 - val_accuracy: 0.9831 - val_loss: 0.0729\n",
      "Epoch 169/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9900 - loss: 0.0327\n",
      "Epoch 169: val_loss did not improve from 0.07288\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9882 - loss: 0.0439 - val_accuracy: 0.9808 - val_loss: 0.0739\n",
      "Epoch 170/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9880 - loss: 0.0319\n",
      "Epoch 170: val_loss did not improve from 0.07288\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0390 - val_accuracy: 0.9823 - val_loss: 0.0752\n",
      "Epoch 171/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9860 - loss: 0.0440\n",
      "Epoch 171: val_loss did not improve from 0.07288\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9886 - loss: 0.0428 - val_accuracy: 0.9838 - val_loss: 0.0745\n",
      "Epoch 172/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9860 - loss: 0.0499\n",
      "Epoch 172: val_loss improved from 0.07288 to 0.07231, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9881 - loss: 0.0441 - val_accuracy: 0.9831 - val_loss: 0.0723\n",
      "Epoch 173/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9860 - loss: 0.0331\n",
      "Epoch 173: val_loss improved from 0.07231 to 0.07228, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9869 - loss: 0.0431 - val_accuracy: 0.9846 - val_loss: 0.0723\n",
      "Epoch 174/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9920 - loss: 0.0323\n",
      "Epoch 174: val_loss did not improve from 0.07228\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9890 - loss: 0.0406 - val_accuracy: 0.9831 - val_loss: 0.0730\n",
      "Epoch 175/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9900 - loss: 0.0376\n",
      "Epoch 175: val_loss did not improve from 0.07228\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9886 - loss: 0.0394 - val_accuracy: 0.9792 - val_loss: 0.0741\n",
      "Epoch 176/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9760 - loss: 0.0645\n",
      "Epoch 176: val_loss did not improve from 0.07228\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9841 - loss: 0.0491 - val_accuracy: 0.9823 - val_loss: 0.0725\n",
      "Epoch 177/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9900 - loss: 0.0274\n",
      "Epoch 177: val_loss did not improve from 0.07228\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9873 - loss: 0.0383 - val_accuracy: 0.9808 - val_loss: 0.0724\n",
      "Epoch 178/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9860 - loss: 0.0432\n",
      "Epoch 178: val_loss improved from 0.07228 to 0.07199, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9873 - loss: 0.0441 - val_accuracy: 0.9846 - val_loss: 0.0720\n",
      "Epoch 179/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9920 - loss: 0.0396\n",
      "Epoch 179: val_loss did not improve from 0.07199\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9904 - loss: 0.0410 - val_accuracy: 0.9846 - val_loss: 0.0724\n",
      "Epoch 180/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9860 - loss: 0.0435\n",
      "Epoch 180: val_loss did not improve from 0.07199\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9890 - loss: 0.0417 - val_accuracy: 0.9846 - val_loss: 0.0725\n",
      "Epoch 181/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9860 - loss: 0.0376\n",
      "Epoch 181: val_loss did not improve from 0.07199\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9887 - loss: 0.0390 - val_accuracy: 0.9846 - val_loss: 0.0745\n",
      "Epoch 182/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9880 - loss: 0.0528\n",
      "Epoch 182: val_loss did not improve from 0.07199\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9875 - loss: 0.0495 - val_accuracy: 0.9838 - val_loss: 0.0724\n",
      "Epoch 183/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9900 - loss: 0.0299\n",
      "Epoch 183: val_loss did not improve from 0.07199\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9883 - loss: 0.0403 - val_accuracy: 0.9792 - val_loss: 0.0744\n",
      "Epoch 184/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9860 - loss: 0.0393\n",
      "Epoch 184: val_loss did not improve from 0.07199\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0417 - val_accuracy: 0.9792 - val_loss: 0.0739\n",
      "Epoch 185/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9920 - loss: 0.0296\n",
      "Epoch 185: val_loss did not improve from 0.07199\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9896 - loss: 0.0365 - val_accuracy: 0.9831 - val_loss: 0.0720\n",
      "Epoch 186/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9940 - loss: 0.0325\n",
      "Epoch 186: val_loss did not improve from 0.07199\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9898 - loss: 0.0392 - val_accuracy: 0.9854 - val_loss: 0.0724\n",
      "Epoch 187/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9840 - loss: 0.0688\n",
      "Epoch 187: val_loss did not improve from 0.07199\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9858 - loss: 0.0493 - val_accuracy: 0.9854 - val_loss: 0.0721\n",
      "Epoch 188/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9940 - loss: 0.0325\n",
      "Epoch 188: val_loss improved from 0.07199 to 0.07180, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9884 - loss: 0.0403 - val_accuracy: 0.9831 - val_loss: 0.0718\n",
      "Epoch 189/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9800 - loss: 0.0456\n",
      "Epoch 189: val_loss did not improve from 0.07180\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9876 - loss: 0.0401 - val_accuracy: 0.9854 - val_loss: 0.0729\n",
      "Epoch 190/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9840 - loss: 0.0525\n",
      "Epoch 190: val_loss did not improve from 0.07180\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9854 - loss: 0.0492 - val_accuracy: 0.9838 - val_loss: 0.0747\n",
      "Epoch 191/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9840 - loss: 0.0435\n",
      "Epoch 191: val_loss did not improve from 0.07180\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9888 - loss: 0.0375 - val_accuracy: 0.9815 - val_loss: 0.0798\n",
      "Epoch 192/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9960 - loss: 0.0293\n",
      "Epoch 192: val_loss did not improve from 0.07180\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9893 - loss: 0.0428 - val_accuracy: 0.9854 - val_loss: 0.0728\n",
      "Epoch 193/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9880 - loss: 0.0299\n",
      "Epoch 193: val_loss did not improve from 0.07180\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9903 - loss: 0.0378 - val_accuracy: 0.9846 - val_loss: 0.0724\n",
      "Epoch 194/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9860 - loss: 0.0386\n",
      "Epoch 194: val_loss did not improve from 0.07180\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0406 - val_accuracy: 0.9862 - val_loss: 0.0744\n",
      "Epoch 195/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0192\n",
      "Epoch 195: val_loss did not improve from 0.07180\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9920 - loss: 0.0337 - val_accuracy: 0.9862 - val_loss: 0.0734\n",
      "Epoch 196/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9900 - loss: 0.0488\n",
      "Epoch 196: val_loss did not improve from 0.07180\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9899 - loss: 0.0424 - val_accuracy: 0.9846 - val_loss: 0.0749\n",
      "Epoch 197/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9900 - loss: 0.0440\n",
      "Epoch 197: val_loss improved from 0.07180 to 0.07109, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9903 - loss: 0.0391 - val_accuracy: 0.9823 - val_loss: 0.0711\n",
      "Epoch 198/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9920 - loss: 0.0289\n",
      "Epoch 198: val_loss improved from 0.07109 to 0.07102, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9902 - loss: 0.0384 - val_accuracy: 0.9854 - val_loss: 0.0710\n",
      "Epoch 199/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9880 - loss: 0.0283\n",
      "Epoch 199: val_loss did not improve from 0.07102\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9891 - loss: 0.0395 - val_accuracy: 0.9815 - val_loss: 0.0733\n",
      "Epoch 200/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9860 - loss: 0.0298\n",
      "Epoch 200: val_loss did not improve from 0.07102\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9882 - loss: 0.0361 - val_accuracy: 0.9838 - val_loss: 0.0717\n",
      "Epoch 201/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9940 - loss: 0.0280\n",
      "Epoch 201: val_loss did not improve from 0.07102\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9897 - loss: 0.0380 - val_accuracy: 0.9800 - val_loss: 0.0721\n",
      "Epoch 202/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9840 - loss: 0.0472\n",
      "Epoch 202: val_loss improved from 0.07102 to 0.07093, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9877 - loss: 0.0426 - val_accuracy: 0.9846 - val_loss: 0.0709\n",
      "Epoch 203/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9900 - loss: 0.0327\n",
      "Epoch 203: val_loss did not improve from 0.07093\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9908 - loss: 0.0351 - val_accuracy: 0.9846 - val_loss: 0.0724\n",
      "Epoch 204/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9940 - loss: 0.0236\n",
      "Epoch 204: val_loss did not improve from 0.07093\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9897 - loss: 0.0388 - val_accuracy: 0.9815 - val_loss: 0.0713\n",
      "Epoch 205/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9940 - loss: 0.0345\n",
      "Epoch 205: val_loss improved from 0.07093 to 0.07083, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9901 - loss: 0.0383 - val_accuracy: 0.9823 - val_loss: 0.0708\n",
      "Epoch 206/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9920 - loss: 0.0334\n",
      "Epoch 206: val_loss improved from 0.07083 to 0.07070, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9901 - loss: 0.0361 - val_accuracy: 0.9838 - val_loss: 0.0707\n",
      "Epoch 207/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9940 - loss: 0.0292\n",
      "Epoch 207: val_loss did not improve from 0.07070\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9905 - loss: 0.0338 - val_accuracy: 0.9838 - val_loss: 0.0711\n",
      "Epoch 208/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9840 - loss: 0.0552\n",
      "Epoch 208: val_loss did not improve from 0.07070\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9869 - loss: 0.0425 - val_accuracy: 0.9800 - val_loss: 0.0714\n",
      "Epoch 209/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9960 - loss: 0.0134\n",
      "Epoch 209: val_loss did not improve from 0.07070\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9897 - loss: 0.0338 - val_accuracy: 0.9808 - val_loss: 0.0725\n",
      "Epoch 210/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9860 - loss: 0.0611\n",
      "Epoch 210: val_loss improved from 0.07070 to 0.07005, saving model to ./model/all4/best_model.keras\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9870 - loss: 0.0478 - val_accuracy: 0.9846 - val_loss: 0.0700\n",
      "Epoch 211/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9900 - loss: 0.0375\n",
      "Epoch 211: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9888 - loss: 0.0407 - val_accuracy: 0.9823 - val_loss: 0.0711\n",
      "Epoch 212/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9900 - loss: 0.0379\n",
      "Epoch 212: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0382 - val_accuracy: 0.9831 - val_loss: 0.0715\n",
      "Epoch 213/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9920 - loss: 0.0428\n",
      "Epoch 213: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9899 - loss: 0.0408 - val_accuracy: 0.9831 - val_loss: 0.0708\n",
      "Epoch 214/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9840 - loss: 0.0274\n",
      "Epoch 214: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9860 - loss: 0.0385 - val_accuracy: 0.9854 - val_loss: 0.0712\n",
      "Epoch 215/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9880 - loss: 0.0420\n",
      "Epoch 215: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9876 - loss: 0.0401 - val_accuracy: 0.9862 - val_loss: 0.0734\n",
      "Epoch 216/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9920 - loss: 0.0384\n",
      "Epoch 216: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9894 - loss: 0.0391 - val_accuracy: 0.9854 - val_loss: 0.0718\n",
      "Epoch 217/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9820 - loss: 0.0658\n",
      "Epoch 217: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9856 - loss: 0.0462 - val_accuracy: 0.9854 - val_loss: 0.0749\n",
      "Epoch 218/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9860 - loss: 0.0479\n",
      "Epoch 218: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9862 - loss: 0.0394 - val_accuracy: 0.9808 - val_loss: 0.0808\n",
      "Epoch 219/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9960 - loss: 0.0261\n",
      "Epoch 219: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9881 - loss: 0.0402 - val_accuracy: 0.9815 - val_loss: 0.0780\n",
      "Epoch 220/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9820 - loss: 0.0568\n",
      "Epoch 220: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9876 - loss: 0.0399 - val_accuracy: 0.9846 - val_loss: 0.0746\n",
      "Epoch 221/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9900 - loss: 0.0544\n",
      "Epoch 221: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9907 - loss: 0.0382 - val_accuracy: 0.9823 - val_loss: 0.0764\n",
      "Epoch 222/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9880 - loss: 0.0489\n",
      "Epoch 222: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9881 - loss: 0.0394 - val_accuracy: 0.9838 - val_loss: 0.0746\n",
      "Epoch 223/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9940 - loss: 0.0304\n",
      "Epoch 223: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9909 - loss: 0.0392 - val_accuracy: 0.9862 - val_loss: 0.0716\n",
      "Epoch 224/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9880 - loss: 0.0565\n",
      "Epoch 224: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9889 - loss: 0.0415 - val_accuracy: 0.9854 - val_loss: 0.0717\n",
      "Epoch 225/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9900 - loss: 0.0339\n",
      "Epoch 225: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9884 - loss: 0.0381 - val_accuracy: 0.9854 - val_loss: 0.0716\n",
      "Epoch 226/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9880 - loss: 0.0401\n",
      "Epoch 226: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9901 - loss: 0.0380 - val_accuracy: 0.9838 - val_loss: 0.0704\n",
      "Epoch 227/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9880 - loss: 0.0398\n",
      "Epoch 227: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9883 - loss: 0.0412 - val_accuracy: 0.9846 - val_loss: 0.0706\n",
      "Epoch 228/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9920 - loss: 0.0211\n",
      "Epoch 228: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9892 - loss: 0.0357 - val_accuracy: 0.9854 - val_loss: 0.0710\n",
      "Epoch 229/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9960 - loss: 0.0165\n",
      "Epoch 229: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9923 - loss: 0.0291 - val_accuracy: 0.9846 - val_loss: 0.0712\n",
      "Epoch 230/2000\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9940 - loss: 0.0205\n",
      "Epoch 230: val_loss did not improve from 0.07005\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9910 - loss: 0.0334 - val_accuracy: 0.9831 - val_loss: 0.0702\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(36, input_dim=12, activation='relu'))\n",
    "model3.add(Dense(12, activation='relu'))\n",
    "model3.add(Dense(8, activation='relu'))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history3 = model3.fit(X_train, y_train, epochs=2000, batch_size=500, validation_split=0.25, callbacks=[checkpointer, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation_split<br>\n",
    "검증 데이터로 사용될 훈련 데이터의 비율입니다. 모델은 훈련 데이터의 이 부분을 구분하고, 각 에포크가 끝날 때 이 데이터에 대한 손실과 모델 측정항목을 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3897.75 1299.25\n"
     ]
    }
   ],
   "source": [
    "print(5197 * 0.75, 5197 * 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23116f1c770>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.8737490177154541,\n",
       "  0.9042853713035583,\n",
       "  0.924044132232666,\n",
       "  0.9330254197120667,\n",
       "  0.9353348612785339,\n",
       "  0.936874508857727,\n",
       "  0.937644362449646,\n",
       "  0.9381575584411621,\n",
       "  0.9373877048492432,\n",
       "  0.9399538040161133,\n",
       "  0.9407236576080322,\n",
       "  0.9396972060203552,\n",
       "  0.9396972060203552,\n",
       "  0.9409802556037903,\n",
       "  0.9399538040161133,\n",
       "  0.9420066475868225,\n",
       "  0.9417500495910645,\n",
       "  0.9425199031829834,\n",
       "  0.9417500495910645,\n",
       "  0.9414934515953064,\n",
       "  0.9402104020118713,\n",
       "  0.9414934515953064,\n",
       "  0.9438029527664185,\n",
       "  0.9440595507621765,\n",
       "  0.9438029527664185,\n",
       "  0.9453426003456116,\n",
       "  0.9473954439163208,\n",
       "  0.9471388459205627,\n",
       "  0.9463689923286438,\n",
       "  0.9468821883201599,\n",
       "  0.948165237903595,\n",
       "  0.9473954439163208,\n",
       "  0.9468821883201599,\n",
       "  0.9489350914955139,\n",
       "  0.9499614834785461,\n",
       "  0.94944828748703,\n",
       "  0.950218141078949,\n",
       "  0.949191689491272,\n",
       "  0.9507313370704651,\n",
       "  0.9512445330619812,\n",
       "  0.9522709846496582,\n",
       "  0.9527841806411743,\n",
       "  0.9532974362373352,\n",
       "  0.9535540342330933,\n",
       "  0.9550936818122864,\n",
       "  0.9540672302246094,\n",
       "  0.9545804262161255,\n",
       "  0.9566333293914795,\n",
       "  0.9563766717910767,\n",
       "  0.9576597213745117,\n",
       "  0.9594559669494629,\n",
       "  0.9566333293914795,\n",
       "  0.9589427709579468,\n",
       "  0.9615088701248169,\n",
       "  0.9597126245498657,\n",
       "  0.962022066116333,\n",
       "  0.9635617136955261,\n",
       "  0.962791919708252,\n",
       "  0.96304851770401,\n",
       "  0.962791919708252,\n",
       "  0.9622786641120911,\n",
       "  0.9643315076828003,\n",
       "  0.9668976068496704,\n",
       "  0.9697203040122986,\n",
       "  0.9648447632789612,\n",
       "  0.9694637060165405,\n",
       "  0.9727995991706848,\n",
       "  0.9740826487541199,\n",
       "  0.973569393157959,\n",
       "  0.973825991153717,\n",
       "  0.9761354923248291,\n",
       "  0.9761354923248291,\n",
       "  0.9761354923248291,\n",
       "  0.9761354923248291,\n",
       "  0.9763920903205872,\n",
       "  0.9776751399040222,\n",
       "  0.9771619439125061,\n",
       "  0.9781883358955383,\n",
       "  0.9779317378997803,\n",
       "  0.9776751399040222,\n",
       "  0.9789581894874573,\n",
       "  0.9787015914916992,\n",
       "  0.9792147874832153,\n",
       "  0.9763920903205872,\n",
       "  0.9753656387329102,\n",
       "  0.9776751399040222,\n",
       "  0.9804978370666504,\n",
       "  0.9797279834747314,\n",
       "  0.9799845814704895,\n",
       "  0.9810110330581665,\n",
       "  0.9776751399040222,\n",
       "  0.9787015914916992,\n",
       "  0.9817808866500854,\n",
       "  0.9802412390708923,\n",
       "  0.9815242290496826,\n",
       "  0.9807544350624084,\n",
       "  0.9812676310539246,\n",
       "  0.9822940826416016,\n",
       "  0.9817808866500854,\n",
       "  0.9822940826416016,\n",
       "  0.9812676310539246,\n",
       "  0.9789581894874573,\n",
       "  0.9825506806373596,\n",
       "  0.9833204746246338,\n",
       "  0.9843469262123108,\n",
       "  0.9820374846458435,\n",
       "  0.9825506806373596,\n",
       "  0.9825506806373596,\n",
       "  0.9835771322250366,\n",
       "  0.9830638766288757,\n",
       "  0.9853733777999878,\n",
       "  0.9840903282165527,\n",
       "  0.9846035242080688,\n",
       "  0.9846035242080688,\n",
       "  0.9851167798042297,\n",
       "  0.9856299757957458,\n",
       "  0.9858865737915039,\n",
       "  0.9848601222038269,\n",
       "  0.9846035242080688,\n",
       "  0.9856299757957458,\n",
       "  0.9846035242080688,\n",
       "  0.9846035242080688,\n",
       "  0.9848601222038269,\n",
       "  0.9812676310539246,\n",
       "  0.9858865737915039,\n",
       "  0.9851167798042297,\n",
       "  0.98639976978302,\n",
       "  0.986143171787262,\n",
       "  0.9856299757957458,\n",
       "  0.9858865737915039,\n",
       "  0.98639976978302,\n",
       "  0.9866564273834229,\n",
       "  0.987169623374939,\n",
       "  0.9856299757957458,\n",
       "  0.9869130253791809,\n",
       "  0.9866564273834229,\n",
       "  0.987169623374939,\n",
       "  0.987169623374939,\n",
       "  0.9817808866500854,\n",
       "  0.986143171787262,\n",
       "  0.9853733777999878,\n",
       "  0.9835771322250366,\n",
       "  0.9879394173622131,\n",
       "  0.9869130253791809,\n",
       "  0.9848601222038269,\n",
       "  0.9830638766288757,\n",
       "  0.9866564273834229,\n",
       "  0.9856299757957458,\n",
       "  0.9866564273834229,\n",
       "  0.9879394173622131,\n",
       "  0.986143171787262,\n",
       "  0.9856299757957458,\n",
       "  0.986143171787262,\n",
       "  0.986143171787262,\n",
       "  0.988196074962616,\n",
       "  0.9869130253791809,\n",
       "  0.9866564273834229,\n",
       "  0.9856299757957458,\n",
       "  0.9830638766288757,\n",
       "  0.987426221370697,\n",
       "  0.9866564273834229,\n",
       "  0.9876828193664551,\n",
       "  0.98639976978302,\n",
       "  0.9848601222038269,\n",
       "  0.98639976978302,\n",
       "  0.9876828193664551,\n",
       "  0.9876828193664551,\n",
       "  0.9833204746246338,\n",
       "  0.9876828193664551,\n",
       "  0.9879394173622131,\n",
       "  0.987426221370697,\n",
       "  0.9879394173622131,\n",
       "  0.988196074962616,\n",
       "  0.988452672958374,\n",
       "  0.9869130253791809,\n",
       "  0.98639976978302,\n",
       "  0.987426221370697,\n",
       "  0.9869130253791809,\n",
       "  0.9889658689498901,\n",
       "  0.9887092709541321,\n",
       "  0.9879394173622131,\n",
       "  0.9879394173622131,\n",
       "  0.988196074962616,\n",
       "  0.9879394173622131,\n",
       "  0.9887092709541321,\n",
       "  0.988452672958374,\n",
       "  0.988196074962616,\n",
       "  0.9879394173622131,\n",
       "  0.988452672958374,\n",
       "  0.9879394173622131,\n",
       "  0.987426221370697,\n",
       "  0.988452672958374,\n",
       "  0.9897357225418091,\n",
       "  0.9892224669456482,\n",
       "  0.9879394173622131,\n",
       "  0.9899923205375671,\n",
       "  0.9899923205375671,\n",
       "  0.9887092709541321,\n",
       "  0.9887092709541321,\n",
       "  0.9889658689498901,\n",
       "  0.9897357225418091,\n",
       "  0.9887092709541321,\n",
       "  0.9899923205375671,\n",
       "  0.988452672958374,\n",
       "  0.9887092709541321,\n",
       "  0.9887092709541321,\n",
       "  0.988452672958374,\n",
       "  0.9879394173622131,\n",
       "  0.988452672958374,\n",
       "  0.9879394173622131,\n",
       "  0.9892224669456482,\n",
       "  0.9869130253791809,\n",
       "  0.9905055165290833,\n",
       "  0.9887092709541321,\n",
       "  0.987426221370697,\n",
       "  0.988196074962616,\n",
       "  0.9876828193664551,\n",
       "  0.987426221370697,\n",
       "  0.987426221370697,\n",
       "  0.9879394173622131,\n",
       "  0.9894790649414062,\n",
       "  0.9879394173622131,\n",
       "  0.9905055165290833,\n",
       "  0.9892224669456482,\n",
       "  0.988452672958374,\n",
       "  0.9907621145248413,\n",
       "  0.9894790649414062,\n",
       "  0.9892224669456482,\n",
       "  0.9894790649414062,\n",
       "  0.9889658689498901],\n",
       " 'loss': [0.3338351547718048,\n",
       "  0.2602005898952484,\n",
       "  0.21351848542690277,\n",
       "  0.1966986507177353,\n",
       "  0.1900070309638977,\n",
       "  0.18535245954990387,\n",
       "  0.1829753816127777,\n",
       "  0.1811964213848114,\n",
       "  0.17898941040039062,\n",
       "  0.17694613337516785,\n",
       "  0.17591264843940735,\n",
       "  0.1746295839548111,\n",
       "  0.17248843610286713,\n",
       "  0.17095227539539337,\n",
       "  0.17058880627155304,\n",
       "  0.1678556501865387,\n",
       "  0.1661926656961441,\n",
       "  0.16461992263793945,\n",
       "  0.16305696964263916,\n",
       "  0.16609372198581696,\n",
       "  0.1663030982017517,\n",
       "  0.16283588111400604,\n",
       "  0.1591905951499939,\n",
       "  0.15744327008724213,\n",
       "  0.1549733728170395,\n",
       "  0.15332388877868652,\n",
       "  0.1513422429561615,\n",
       "  0.15061259269714355,\n",
       "  0.1485115885734558,\n",
       "  0.14691880345344543,\n",
       "  0.14474661648273468,\n",
       "  0.14424845576286316,\n",
       "  0.1419755220413208,\n",
       "  0.1400299221277237,\n",
       "  0.1379602998495102,\n",
       "  0.13690342009067535,\n",
       "  0.13495199382305145,\n",
       "  0.13400936126708984,\n",
       "  0.13160845637321472,\n",
       "  0.13181650638580322,\n",
       "  0.13201077282428741,\n",
       "  0.12794387340545654,\n",
       "  0.12646488845348358,\n",
       "  0.12329468131065369,\n",
       "  0.12154628336429596,\n",
       "  0.11942300945520401,\n",
       "  0.1182595044374466,\n",
       "  0.1164228767156601,\n",
       "  0.11471131443977356,\n",
       "  0.11307396739721298,\n",
       "  0.11147365719079971,\n",
       "  0.1114668920636177,\n",
       "  0.10812359303236008,\n",
       "  0.10712707787752151,\n",
       "  0.10582126677036285,\n",
       "  0.10545603185892105,\n",
       "  0.10324624925851822,\n",
       "  0.10215920209884644,\n",
       "  0.10453946888446808,\n",
       "  0.1009509414434433,\n",
       "  0.09993863105773926,\n",
       "  0.09643079340457916,\n",
       "  0.09310887008905411,\n",
       "  0.09489375352859497,\n",
       "  0.09734074771404266,\n",
       "  0.08832311630249023,\n",
       "  0.08668066561222076,\n",
       "  0.08613025397062302,\n",
       "  0.08205113559961319,\n",
       "  0.08000613749027252,\n",
       "  0.07837004959583282,\n",
       "  0.07648269087076187,\n",
       "  0.07525189965963364,\n",
       "  0.07365015894174576,\n",
       "  0.07451070845127106,\n",
       "  0.07298223674297333,\n",
       "  0.07223338633775711,\n",
       "  0.07117679715156555,\n",
       "  0.06956934928894043,\n",
       "  0.07035935670137405,\n",
       "  0.06967290490865707,\n",
       "  0.06730614602565765,\n",
       "  0.06953386217355728,\n",
       "  0.07075691968202591,\n",
       "  0.0697992742061615,\n",
       "  0.06802710890769958,\n",
       "  0.06522103399038315,\n",
       "  0.06328029930591583,\n",
       "  0.06280991435050964,\n",
       "  0.06402353197336197,\n",
       "  0.06641107052564621,\n",
       "  0.06489183753728867,\n",
       "  0.061293475329875946,\n",
       "  0.061330635100603104,\n",
       "  0.05907461419701576,\n",
       "  0.05926799401640892,\n",
       "  0.060121625661849976,\n",
       "  0.0590568371117115,\n",
       "  0.06025566905736923,\n",
       "  0.059030573815107346,\n",
       "  0.06161762401461601,\n",
       "  0.06344246864318848,\n",
       "  0.0591394416987896,\n",
       "  0.05818276107311249,\n",
       "  0.05694286897778511,\n",
       "  0.058990947902202606,\n",
       "  0.055768903344869614,\n",
       "  0.05500092729926109,\n",
       "  0.05521649122238159,\n",
       "  0.053750522434711456,\n",
       "  0.05241994187235832,\n",
       "  0.052874378859996796,\n",
       "  0.0536937452852726,\n",
       "  0.05227779597043991,\n",
       "  0.05191447585821152,\n",
       "  0.05136226862668991,\n",
       "  0.05098956823348999,\n",
       "  0.05069172382354736,\n",
       "  0.05128154158592224,\n",
       "  0.052027665078639984,\n",
       "  0.052197255194187164,\n",
       "  0.052806343883275986,\n",
       "  0.055772099643945694,\n",
       "  0.0595063790678978,\n",
       "  0.0531996414065361,\n",
       "  0.049844663590192795,\n",
       "  0.048150449991226196,\n",
       "  0.049426235258579254,\n",
       "  0.05040619149804115,\n",
       "  0.05035768449306488,\n",
       "  0.04833391681313515,\n",
       "  0.04960532486438751,\n",
       "  0.0471905842423439,\n",
       "  0.047116052359342575,\n",
       "  0.04684644564986229,\n",
       "  0.04666738584637642,\n",
       "  0.047226984053850174,\n",
       "  0.04817470163106918,\n",
       "  0.05203952640295029,\n",
       "  0.04927470535039902,\n",
       "  0.051533862948417664,\n",
       "  0.052779801189899445,\n",
       "  0.046917688101530075,\n",
       "  0.0467863455414772,\n",
       "  0.05050843954086304,\n",
       "  0.05179491639137268,\n",
       "  0.047862011939287186,\n",
       "  0.0475175678730011,\n",
       "  0.04566625878214836,\n",
       "  0.04499879479408264,\n",
       "  0.050783585757017136,\n",
       "  0.047782037407159805,\n",
       "  0.04974162578582764,\n",
       "  0.046594295650720596,\n",
       "  0.04501739516854286,\n",
       "  0.04424084722995758,\n",
       "  0.045454464852809906,\n",
       "  0.04980906844139099,\n",
       "  0.05482684075832367,\n",
       "  0.04559877887368202,\n",
       "  0.046136628836393356,\n",
       "  0.04416356235742569,\n",
       "  0.04556231573224068,\n",
       "  0.04792854189872742,\n",
       "  0.048335764557123184,\n",
       "  0.044661879539489746,\n",
       "  0.044565122574567795,\n",
       "  0.051120270043611526,\n",
       "  0.045311089605093,\n",
       "  0.04185295104980469,\n",
       "  0.043032534420490265,\n",
       "  0.0417708083987236,\n",
       "  0.041322652250528336,\n",
       "  0.04205426573753357,\n",
       "  0.043281491845846176,\n",
       "  0.04327625036239624,\n",
       "  0.041167426854372025,\n",
       "  0.04305538162589073,\n",
       "  0.041488487273454666,\n",
       "  0.04111267626285553,\n",
       "  0.042196255177259445,\n",
       "  0.04428466781973839,\n",
       "  0.04269460216164589,\n",
       "  0.042651187628507614,\n",
       "  0.04178685322403908,\n",
       "  0.04039523005485535,\n",
       "  0.04078248515725136,\n",
       "  0.04021201282739639,\n",
       "  0.04033356159925461,\n",
       "  0.04189085215330124,\n",
       "  0.04082139953970909,\n",
       "  0.04166146367788315,\n",
       "  0.03928364813327789,\n",
       "  0.03913548216223717,\n",
       "  0.03991486504673958,\n",
       "  0.03863353282213211,\n",
       "  0.03868820518255234,\n",
       "  0.04255454242229462,\n",
       "  0.0416804663836956,\n",
       "  0.03957788273692131,\n",
       "  0.04030807316303253,\n",
       "  0.04058904945850372,\n",
       "  0.038298100233078,\n",
       "  0.0397365465760231,\n",
       "  0.03957460820674896,\n",
       "  0.037964921444654465,\n",
       "  0.03791123628616333,\n",
       "  0.03850860148668289,\n",
       "  0.03959851711988449,\n",
       "  0.04065482318401337,\n",
       "  0.03765420988202095,\n",
       "  0.039305735379457474,\n",
       "  0.03775307163596153,\n",
       "  0.03943702206015587,\n",
       "  0.040555957704782486,\n",
       "  0.03934280946850777,\n",
       "  0.03983646631240845,\n",
       "  0.0381171889603138,\n",
       "  0.040607161819934845,\n",
       "  0.03992066904902458,\n",
       "  0.037131231278181076,\n",
       "  0.03852955996990204,\n",
       "  0.036814481019973755,\n",
       "  0.03645572066307068,\n",
       "  0.03763846307992935,\n",
       "  0.03669265657663345,\n",
       "  0.03761892020702362,\n",
       "  0.036111555993556976,\n",
       "  0.03746053948998451,\n",
       "  0.04052609205245972],\n",
       " 'val_accuracy': [0.8753846287727356,\n",
       "  0.9107692241668701,\n",
       "  0.9223076701164246,\n",
       "  0.9284615516662598,\n",
       "  0.9284615516662598,\n",
       "  0.9300000071525574,\n",
       "  0.9330769181251526,\n",
       "  0.931538462638855,\n",
       "  0.9338461756706238,\n",
       "  0.9330769181251526,\n",
       "  0.9330769181251526,\n",
       "  0.9346153736114502,\n",
       "  0.9346153736114502,\n",
       "  0.9361538290977478,\n",
       "  0.9361538290977478,\n",
       "  0.936923086643219,\n",
       "  0.936923086643219,\n",
       "  0.9384615421295166,\n",
       "  0.9346153736114502,\n",
       "  0.9353846311569214,\n",
       "  0.9361538290977478,\n",
       "  0.9384615421295166,\n",
       "  0.9384615421295166,\n",
       "  0.9384615421295166,\n",
       "  0.939230740070343,\n",
       "  0.9407692551612854,\n",
       "  0.9407692551612854,\n",
       "  0.9407692551612854,\n",
       "  0.9384615421295166,\n",
       "  0.942307710647583,\n",
       "  0.9438461661338806,\n",
       "  0.9438461661338806,\n",
       "  0.9438461661338806,\n",
       "  0.944615364074707,\n",
       "  0.9461538195610046,\n",
       "  0.9438461661338806,\n",
       "  0.9461538195610046,\n",
       "  0.9469230771064758,\n",
       "  0.9461538195610046,\n",
       "  0.942307710647583,\n",
       "  0.947692334651947,\n",
       "  0.9461538195610046,\n",
       "  0.947692334651947,\n",
       "  0.947692334651947,\n",
       "  0.9484615325927734,\n",
       "  0.949999988079071,\n",
       "  0.949999988079071,\n",
       "  0.949999988079071,\n",
       "  0.9523077011108398,\n",
       "  0.9515384435653687,\n",
       "  0.9515384435653687,\n",
       "  0.9523077011108398,\n",
       "  0.9599999785423279,\n",
       "  0.9607692360877991,\n",
       "  0.9576923251152039,\n",
       "  0.9561538696289062,\n",
       "  0.9569230675697327,\n",
       "  0.9484615325927734,\n",
       "  0.9507692456245422,\n",
       "  0.9592307806015015,\n",
       "  0.9700000286102295,\n",
       "  0.9684615135192871,\n",
       "  0.9715384840965271,\n",
       "  0.9730769395828247,\n",
       "  0.9738461375236511,\n",
       "  0.9707692265510559,\n",
       "  0.9738461375236511,\n",
       "  0.9700000286102295,\n",
       "  0.9738461375236511,\n",
       "  0.9707692265510559,\n",
       "  0.9738461375236511,\n",
       "  0.9761538505554199,\n",
       "  0.9761538505554199,\n",
       "  0.9723076820373535,\n",
       "  0.9761538505554199,\n",
       "  0.9769230484962463,\n",
       "  0.9753845930099487,\n",
       "  0.9776923060417175,\n",
       "  0.9723076820373535,\n",
       "  0.9784615635871887,\n",
       "  0.9800000190734863,\n",
       "  0.9761538505554199,\n",
       "  0.9776923060417175,\n",
       "  0.9776923060417175,\n",
       "  0.9730769395828247,\n",
       "  0.9784615635871887,\n",
       "  0.9784615635871887,\n",
       "  0.9792307615280151,\n",
       "  0.9761538505554199,\n",
       "  0.9738461375236511,\n",
       "  0.9807692170143127,\n",
       "  0.9800000190734863,\n",
       "  0.9792307615280151,\n",
       "  0.9807692170143127,\n",
       "  0.9800000190734863,\n",
       "  0.9800000190734863,\n",
       "  0.9792307615280151,\n",
       "  0.9784615635871887,\n",
       "  0.9792307615280151,\n",
       "  0.9792307615280151,\n",
       "  0.9723076820373535,\n",
       "  0.9753845930099487,\n",
       "  0.9776923060417175,\n",
       "  0.9784615635871887,\n",
       "  0.9800000190734863,\n",
       "  0.9792307615280151,\n",
       "  0.9784615635871887,\n",
       "  0.9776923060417175,\n",
       "  0.9815384745597839,\n",
       "  0.9807692170143127,\n",
       "  0.9800000190734863,\n",
       "  0.9815384745597839,\n",
       "  0.9784615635871887,\n",
       "  0.9792307615280151,\n",
       "  0.9800000190734863,\n",
       "  0.9815384745597839,\n",
       "  0.9815384745597839,\n",
       "  0.9807692170143127,\n",
       "  0.9830769300460815,\n",
       "  0.9792307615280151,\n",
       "  0.9784615635871887,\n",
       "  0.9792307615280151,\n",
       "  0.9761538505554199,\n",
       "  0.9792307615280151,\n",
       "  0.9776923060417175,\n",
       "  0.9800000190734863,\n",
       "  0.9800000190734863,\n",
       "  0.9784615635871887,\n",
       "  0.9815384745597839,\n",
       "  0.9815384745597839,\n",
       "  0.9776923060417175,\n",
       "  0.9807692170143127,\n",
       "  0.9815384745597839,\n",
       "  0.9807692170143127,\n",
       "  0.9815384745597839,\n",
       "  0.9800000190734863,\n",
       "  0.9807692170143127,\n",
       "  0.9807692170143127,\n",
       "  0.9784615635871887,\n",
       "  0.9784615635871887,\n",
       "  0.9753845930099487,\n",
       "  0.9776923060417175,\n",
       "  0.9815384745597839,\n",
       "  0.9784615635871887,\n",
       "  0.9761538505554199,\n",
       "  0.9784615635871887,\n",
       "  0.9776923060417175,\n",
       "  0.9807692170143127,\n",
       "  0.983846127986908,\n",
       "  0.9776923060417175,\n",
       "  0.9776923060417175,\n",
       "  0.9784615635871887,\n",
       "  0.9784615635871887,\n",
       "  0.9784615635871887,\n",
       "  0.9807692170143127,\n",
       "  0.9830769300460815,\n",
       "  0.9815384745597839,\n",
       "  0.9769230484962463,\n",
       "  0.9769230484962463,\n",
       "  0.9815384745597839,\n",
       "  0.9792307615280151,\n",
       "  0.9807692170143127,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.9830769300460815,\n",
       "  0.9807692170143127,\n",
       "  0.9823076725006104,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.9792307615280151,\n",
       "  0.9823076725006104,\n",
       "  0.9807692170143127,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.983846127986908,\n",
       "  0.9792307615280151,\n",
       "  0.9792307615280151,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9815384745597839,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9823076725006104,\n",
       "  0.9853846430778503,\n",
       "  0.9815384745597839,\n",
       "  0.983846127986908,\n",
       "  0.9800000190734863,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9815384745597839,\n",
       "  0.9823076725006104,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9800000190734863,\n",
       "  0.9807692170143127,\n",
       "  0.9846153855323792,\n",
       "  0.9823076725006104,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9807692170143127,\n",
       "  0.9815384745597839,\n",
       "  0.9846153855323792,\n",
       "  0.9823076725006104,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815],\n",
       " 'val_loss': [0.32847678661346436,\n",
       "  0.2580905258655548,\n",
       "  0.22175279259681702,\n",
       "  0.2115815430879593,\n",
       "  0.20876087248325348,\n",
       "  0.20354196429252625,\n",
       "  0.20162473618984222,\n",
       "  0.20025357604026794,\n",
       "  0.19737903773784637,\n",
       "  0.19643454253673553,\n",
       "  0.19470463693141937,\n",
       "  0.19239398837089539,\n",
       "  0.19139905273914337,\n",
       "  0.18897700309753418,\n",
       "  0.18793639540672302,\n",
       "  0.18647333979606628,\n",
       "  0.18477222323417664,\n",
       "  0.18422922492027283,\n",
       "  0.18560321629047394,\n",
       "  0.18909098207950592,\n",
       "  0.18015165627002716,\n",
       "  0.17749491333961487,\n",
       "  0.17599055171012878,\n",
       "  0.17448575794696808,\n",
       "  0.17252708971500397,\n",
       "  0.1717223972082138,\n",
       "  0.16965916752815247,\n",
       "  0.16777312755584717,\n",
       "  0.16783446073532104,\n",
       "  0.16431809961795807,\n",
       "  0.16220013797283173,\n",
       "  0.16143856942653656,\n",
       "  0.15930743515491486,\n",
       "  0.15697596967220306,\n",
       "  0.15630891919136047,\n",
       "  0.15404212474822998,\n",
       "  0.15277151763439178,\n",
       "  0.14951100945472717,\n",
       "  0.15511131286621094,\n",
       "  0.15363569557666779,\n",
       "  0.14451856911182404,\n",
       "  0.148173987865448,\n",
       "  0.14111299812793732,\n",
       "  0.13975633680820465,\n",
       "  0.13769440352916718,\n",
       "  0.13696296513080597,\n",
       "  0.13466809689998627,\n",
       "  0.13375836610794067,\n",
       "  0.13104881346225739,\n",
       "  0.1310100257396698,\n",
       "  0.12882180511951447,\n",
       "  0.1257750242948532,\n",
       "  0.12410826236009598,\n",
       "  0.12195560336112976,\n",
       "  0.1199272945523262,\n",
       "  0.12162497639656067,\n",
       "  0.12070652842521667,\n",
       "  0.1267596036195755,\n",
       "  0.12281111627817154,\n",
       "  0.11246737092733383,\n",
       "  0.11090601980686188,\n",
       "  0.1079101711511612,\n",
       "  0.10623291879892349,\n",
       "  0.10690978169441223,\n",
       "  0.10375303030014038,\n",
       "  0.1063857227563858,\n",
       "  0.10087095946073532,\n",
       "  0.10404037684202194,\n",
       "  0.09517710655927658,\n",
       "  0.09948936849832535,\n",
       "  0.09263312816619873,\n",
       "  0.09214466065168381,\n",
       "  0.09133143723011017,\n",
       "  0.09471728652715683,\n",
       "  0.08942145854234695,\n",
       "  0.08938484638929367,\n",
       "  0.09041000157594681,\n",
       "  0.08799050003290176,\n",
       "  0.09096088260412216,\n",
       "  0.0860031247138977,\n",
       "  0.0852065235376358,\n",
       "  0.08644390851259232,\n",
       "  0.08466575294733047,\n",
       "  0.08997567743062973,\n",
       "  0.09290146082639694,\n",
       "  0.08601294457912445,\n",
       "  0.08257802575826645,\n",
       "  0.08235342055559158,\n",
       "  0.08542829006910324,\n",
       "  0.08536508679389954,\n",
       "  0.08283177763223648,\n",
       "  0.0826273113489151,\n",
       "  0.08201242983341217,\n",
       "  0.08068323880434036,\n",
       "  0.08078750967979431,\n",
       "  0.08137324452400208,\n",
       "  0.08182305097579956,\n",
       "  0.08189211040735245,\n",
       "  0.08008281886577606,\n",
       "  0.07953553646802902,\n",
       "  0.09088791906833649,\n",
       "  0.08556049317121506,\n",
       "  0.08362586051225662,\n",
       "  0.08110398799180984,\n",
       "  0.0797458291053772,\n",
       "  0.07898567616939545,\n",
       "  0.08097827434539795,\n",
       "  0.08235843479633331,\n",
       "  0.07793042063713074,\n",
       "  0.07856954634189606,\n",
       "  0.080030657351017,\n",
       "  0.07814900577068329,\n",
       "  0.08324143290519714,\n",
       "  0.07850723713636398,\n",
       "  0.07900309562683105,\n",
       "  0.07807021588087082,\n",
       "  0.07787048071622849,\n",
       "  0.07747632265090942,\n",
       "  0.07706889510154724,\n",
       "  0.07734761387109756,\n",
       "  0.07978258281946182,\n",
       "  0.08099570870399475,\n",
       "  0.08560572564601898,\n",
       "  0.07716215401887894,\n",
       "  0.07729744911193848,\n",
       "  0.0764259546995163,\n",
       "  0.07820230722427368,\n",
       "  0.08255700021982193,\n",
       "  0.07628154754638672,\n",
       "  0.07623441517353058,\n",
       "  0.07943807542324066,\n",
       "  0.07633044570684433,\n",
       "  0.07667212933301926,\n",
       "  0.07702013850212097,\n",
       "  0.07667208462953568,\n",
       "  0.07750328630208969,\n",
       "  0.07744483649730682,\n",
       "  0.07643086463212967,\n",
       "  0.07909908890724182,\n",
       "  0.07849404215812683,\n",
       "  0.0877099558711052,\n",
       "  0.08105672150850296,\n",
       "  0.07555890828371048,\n",
       "  0.07667316496372223,\n",
       "  0.08594787865877151,\n",
       "  0.08000532537698746,\n",
       "  0.07897016406059265,\n",
       "  0.07502135634422302,\n",
       "  0.07493693381547928,\n",
       "  0.07995694875717163,\n",
       "  0.07881685346364975,\n",
       "  0.07880471646785736,\n",
       "  0.0755501464009285,\n",
       "  0.07583295553922653,\n",
       "  0.07439512759447098,\n",
       "  0.07493802905082703,\n",
       "  0.07457777112722397,\n",
       "  0.08194095641374588,\n",
       "  0.07798554748296738,\n",
       "  0.07365886867046356,\n",
       "  0.07459892332553864,\n",
       "  0.07502325624227524,\n",
       "  0.0744413435459137,\n",
       "  0.07512659579515457,\n",
       "  0.07483114302158356,\n",
       "  0.07436396926641464,\n",
       "  0.07307236641645432,\n",
       "  0.07287922501564026,\n",
       "  0.07388167083263397,\n",
       "  0.07519107311964035,\n",
       "  0.07453176379203796,\n",
       "  0.072309710085392,\n",
       "  0.07227948307991028,\n",
       "  0.07295718044042587,\n",
       "  0.07405073195695877,\n",
       "  0.07251208275556564,\n",
       "  0.0723840594291687,\n",
       "  0.07198897004127502,\n",
       "  0.07235690951347351,\n",
       "  0.07247097045183182,\n",
       "  0.07445317506790161,\n",
       "  0.07241269201040268,\n",
       "  0.07436984777450562,\n",
       "  0.07387298345565796,\n",
       "  0.07198932021856308,\n",
       "  0.07239647209644318,\n",
       "  0.07213632762432098,\n",
       "  0.071798175573349,\n",
       "  0.07287104427814484,\n",
       "  0.07474448531866074,\n",
       "  0.07982313632965088,\n",
       "  0.07281935214996338,\n",
       "  0.07239915430545807,\n",
       "  0.07441624999046326,\n",
       "  0.07341276854276657,\n",
       "  0.07490811496973038,\n",
       "  0.07109005004167557,\n",
       "  0.07102060317993164,\n",
       "  0.07334188371896744,\n",
       "  0.07171524316072464,\n",
       "  0.07212073355913162,\n",
       "  0.07093211263418198,\n",
       "  0.07236310094594955,\n",
       "  0.07132274657487869,\n",
       "  0.07083489000797272,\n",
       "  0.07070072740316391,\n",
       "  0.07106257230043411,\n",
       "  0.07138165086507797,\n",
       "  0.07246264070272446,\n",
       "  0.07004976272583008,\n",
       "  0.07106590270996094,\n",
       "  0.07151839882135391,\n",
       "  0.07083883136510849,\n",
       "  0.07122556120157242,\n",
       "  0.07336798310279846,\n",
       "  0.07182072103023529,\n",
       "  0.07488633692264557,\n",
       "  0.08078156411647797,\n",
       "  0.07802087813615799,\n",
       "  0.07457621395587921,\n",
       "  0.07642956078052521,\n",
       "  0.07456992566585541,\n",
       "  0.07162158191204071,\n",
       "  0.07174435257911682,\n",
       "  0.07164761424064636,\n",
       "  0.07041750103235245,\n",
       "  0.0706121027469635,\n",
       "  0.07101332396268845,\n",
       "  0.07115050405263901,\n",
       "  0.0702291801571846]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history3.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8753846287727356,\n",
       " 0.9107692241668701,\n",
       " 0.9223076701164246,\n",
       " 0.9284615516662598,\n",
       " 0.9284615516662598,\n",
       " 0.9300000071525574,\n",
       " 0.9330769181251526,\n",
       " 0.931538462638855,\n",
       " 0.9338461756706238,\n",
       " 0.9330769181251526,\n",
       " 0.9330769181251526,\n",
       " 0.9346153736114502,\n",
       " 0.9346153736114502,\n",
       " 0.9361538290977478,\n",
       " 0.9361538290977478,\n",
       " 0.936923086643219,\n",
       " 0.936923086643219,\n",
       " 0.9384615421295166,\n",
       " 0.9346153736114502,\n",
       " 0.9353846311569214,\n",
       " 0.9361538290977478,\n",
       " 0.9384615421295166,\n",
       " 0.9384615421295166,\n",
       " 0.9384615421295166,\n",
       " 0.939230740070343,\n",
       " 0.9407692551612854,\n",
       " 0.9407692551612854,\n",
       " 0.9407692551612854,\n",
       " 0.9384615421295166,\n",
       " 0.942307710647583,\n",
       " 0.9438461661338806,\n",
       " 0.9438461661338806,\n",
       " 0.9438461661338806,\n",
       " 0.944615364074707,\n",
       " 0.9461538195610046,\n",
       " 0.9438461661338806,\n",
       " 0.9461538195610046,\n",
       " 0.9469230771064758,\n",
       " 0.9461538195610046,\n",
       " 0.942307710647583,\n",
       " 0.947692334651947,\n",
       " 0.9461538195610046,\n",
       " 0.947692334651947,\n",
       " 0.947692334651947,\n",
       " 0.9484615325927734,\n",
       " 0.949999988079071,\n",
       " 0.949999988079071,\n",
       " 0.949999988079071,\n",
       " 0.9523077011108398,\n",
       " 0.9515384435653687,\n",
       " 0.9515384435653687,\n",
       " 0.9523077011108398,\n",
       " 0.9599999785423279,\n",
       " 0.9607692360877991,\n",
       " 0.9576923251152039,\n",
       " 0.9561538696289062,\n",
       " 0.9569230675697327,\n",
       " 0.9484615325927734,\n",
       " 0.9507692456245422,\n",
       " 0.9592307806015015,\n",
       " 0.9700000286102295,\n",
       " 0.9684615135192871,\n",
       " 0.9715384840965271,\n",
       " 0.9730769395828247,\n",
       " 0.9738461375236511,\n",
       " 0.9707692265510559,\n",
       " 0.9738461375236511,\n",
       " 0.9700000286102295,\n",
       " 0.9738461375236511,\n",
       " 0.9707692265510559,\n",
       " 0.9738461375236511,\n",
       " 0.9761538505554199,\n",
       " 0.9761538505554199,\n",
       " 0.9723076820373535,\n",
       " 0.9761538505554199,\n",
       " 0.9769230484962463,\n",
       " 0.9753845930099487,\n",
       " 0.9776923060417175,\n",
       " 0.9723076820373535,\n",
       " 0.9784615635871887,\n",
       " 0.9800000190734863,\n",
       " 0.9761538505554199,\n",
       " 0.9776923060417175,\n",
       " 0.9776923060417175,\n",
       " 0.9730769395828247,\n",
       " 0.9784615635871887,\n",
       " 0.9784615635871887,\n",
       " 0.9792307615280151,\n",
       " 0.9761538505554199,\n",
       " 0.9738461375236511,\n",
       " 0.9807692170143127,\n",
       " 0.9800000190734863,\n",
       " 0.9792307615280151,\n",
       " 0.9807692170143127,\n",
       " 0.9800000190734863,\n",
       " 0.9800000190734863,\n",
       " 0.9792307615280151,\n",
       " 0.9784615635871887,\n",
       " 0.9792307615280151,\n",
       " 0.9792307615280151,\n",
       " 0.9723076820373535,\n",
       " 0.9753845930099487,\n",
       " 0.9776923060417175,\n",
       " 0.9784615635871887,\n",
       " 0.9800000190734863,\n",
       " 0.9792307615280151,\n",
       " 0.9784615635871887,\n",
       " 0.9776923060417175,\n",
       " 0.9815384745597839,\n",
       " 0.9807692170143127,\n",
       " 0.9800000190734863,\n",
       " 0.9815384745597839,\n",
       " 0.9784615635871887,\n",
       " 0.9792307615280151,\n",
       " 0.9800000190734863,\n",
       " 0.9815384745597839,\n",
       " 0.9815384745597839,\n",
       " 0.9807692170143127,\n",
       " 0.9830769300460815,\n",
       " 0.9792307615280151,\n",
       " 0.9784615635871887,\n",
       " 0.9792307615280151,\n",
       " 0.9761538505554199,\n",
       " 0.9792307615280151,\n",
       " 0.9776923060417175,\n",
       " 0.9800000190734863,\n",
       " 0.9800000190734863,\n",
       " 0.9784615635871887,\n",
       " 0.9815384745597839,\n",
       " 0.9815384745597839,\n",
       " 0.9776923060417175,\n",
       " 0.9807692170143127,\n",
       " 0.9815384745597839,\n",
       " 0.9807692170143127,\n",
       " 0.9815384745597839,\n",
       " 0.9800000190734863,\n",
       " 0.9807692170143127,\n",
       " 0.9807692170143127,\n",
       " 0.9784615635871887,\n",
       " 0.9784615635871887,\n",
       " 0.9753845930099487,\n",
       " 0.9776923060417175,\n",
       " 0.9815384745597839,\n",
       " 0.9784615635871887,\n",
       " 0.9761538505554199,\n",
       " 0.9784615635871887,\n",
       " 0.9776923060417175,\n",
       " 0.9807692170143127,\n",
       " 0.983846127986908,\n",
       " 0.9776923060417175,\n",
       " 0.9776923060417175,\n",
       " 0.9784615635871887,\n",
       " 0.9784615635871887,\n",
       " 0.9784615635871887,\n",
       " 0.9807692170143127,\n",
       " 0.9830769300460815,\n",
       " 0.9815384745597839,\n",
       " 0.9769230484962463,\n",
       " 0.9769230484962463,\n",
       " 0.9815384745597839,\n",
       " 0.9792307615280151,\n",
       " 0.9807692170143127,\n",
       " 0.9830769300460815,\n",
       " 0.9846153855323792,\n",
       " 0.9830769300460815,\n",
       " 0.9823076725006104,\n",
       " 0.9823076725006104,\n",
       " 0.9830769300460815,\n",
       " 0.9807692170143127,\n",
       " 0.9823076725006104,\n",
       " 0.983846127986908,\n",
       " 0.9830769300460815,\n",
       " 0.9846153855323792,\n",
       " 0.9830769300460815,\n",
       " 0.9792307615280151,\n",
       " 0.9823076725006104,\n",
       " 0.9807692170143127,\n",
       " 0.9846153855323792,\n",
       " 0.9846153855323792,\n",
       " 0.9846153855323792,\n",
       " 0.9846153855323792,\n",
       " 0.983846127986908,\n",
       " 0.9792307615280151,\n",
       " 0.9792307615280151,\n",
       " 0.9830769300460815,\n",
       " 0.9853846430778503,\n",
       " 0.9853846430778503,\n",
       " 0.9830769300460815,\n",
       " 0.9853846430778503,\n",
       " 0.983846127986908,\n",
       " 0.9815384745597839,\n",
       " 0.9853846430778503,\n",
       " 0.9846153855323792,\n",
       " 0.9861538410186768,\n",
       " 0.9861538410186768,\n",
       " 0.9846153855323792,\n",
       " 0.9823076725006104,\n",
       " 0.9853846430778503,\n",
       " 0.9815384745597839,\n",
       " 0.983846127986908,\n",
       " 0.9800000190734863,\n",
       " 0.9846153855323792,\n",
       " 0.9846153855323792,\n",
       " 0.9815384745597839,\n",
       " 0.9823076725006104,\n",
       " 0.983846127986908,\n",
       " 0.983846127986908,\n",
       " 0.9800000190734863,\n",
       " 0.9807692170143127,\n",
       " 0.9846153855323792,\n",
       " 0.9823076725006104,\n",
       " 0.9830769300460815,\n",
       " 0.9830769300460815,\n",
       " 0.9853846430778503,\n",
       " 0.9861538410186768,\n",
       " 0.9853846430778503,\n",
       " 0.9853846430778503,\n",
       " 0.9807692170143127,\n",
       " 0.9815384745597839,\n",
       " 0.9846153855323792,\n",
       " 0.9823076725006104,\n",
       " 0.983846127986908,\n",
       " 0.9861538410186768,\n",
       " 0.9853846430778503,\n",
       " 0.9853846430778503,\n",
       " 0.983846127986908,\n",
       " 0.9846153855323792,\n",
       " 0.9853846430778503,\n",
       " 0.9846153855323792,\n",
       " 0.9830769300460815]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history3.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history3.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3_df = pd.DataFrame(history3.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.873749</td>\n",
       "      <td>0.333835</td>\n",
       "      <td>0.875385</td>\n",
       "      <td>0.328477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.904285</td>\n",
       "      <td>0.260201</td>\n",
       "      <td>0.910769</td>\n",
       "      <td>0.258091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.924044</td>\n",
       "      <td>0.213518</td>\n",
       "      <td>0.922308</td>\n",
       "      <td>0.221753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.933025</td>\n",
       "      <td>0.196699</td>\n",
       "      <td>0.928462</td>\n",
       "      <td>0.211582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.935335</td>\n",
       "      <td>0.190007</td>\n",
       "      <td>0.928462</td>\n",
       "      <td>0.208761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.990762</td>\n",
       "      <td>0.036693</td>\n",
       "      <td>0.983846</td>\n",
       "      <td>0.070418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.989479</td>\n",
       "      <td>0.037619</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.070612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0.989222</td>\n",
       "      <td>0.036112</td>\n",
       "      <td>0.985385</td>\n",
       "      <td>0.071013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.989479</td>\n",
       "      <td>0.037461</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.071151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.988966</td>\n",
       "      <td>0.040526</td>\n",
       "      <td>0.983077</td>\n",
       "      <td>0.070229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     accuracy      loss  val_accuracy  val_loss\n",
       "0    0.873749  0.333835      0.875385  0.328477\n",
       "1    0.904285  0.260201      0.910769  0.258091\n",
       "2    0.924044  0.213518      0.922308  0.221753\n",
       "3    0.933025  0.196699      0.928462  0.211582\n",
       "4    0.935335  0.190007      0.928462  0.208761\n",
       "..        ...       ...           ...       ...\n",
       "225  0.990762  0.036693      0.983846  0.070418\n",
       "226  0.989479  0.037619      0.984615  0.070612\n",
       "227  0.989222  0.036112      0.985385  0.071013\n",
       "228  0.989479  0.037461      0.984615  0.071151\n",
       "229  0.988966  0.040526      0.983077  0.070229\n",
       "\n",
       "[230 rows x 4 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loss = history3_df['loss']\n",
    "y_vloss = history3_df['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = np.arange(len(y_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAAG2CAYAAACEbnlbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABnVklEQVR4nO3deVyTV9o//k8SDYsIqCiLRXCvdQGrQqkztT7S4jKgHWdc6ozUsTqdaq1Q2yptlW7i47SWr0v16YzV6UytdlNw/Y1Sta3ihqKtWisOVWsFq1ZQQUByfn/cc2e5SUISEpLA5/165RVy5+ReEiSX51znOiohhAARERER6andfQJEREREnoYBEhEREZECAyQiIiIiBQZIRERERAoMkIiIiIgUGCARERERKTBAIiIiIlJggERERESkwACJiIiISIEBEhEREZGCRwRIK1asQHR0NHx9fREfH49Dhw5ZbPu3v/0Nv/71r9GmTRu0adMGiYmJddoLITB//nyEh4fDz88PiYmJOHv2rEmb69evY9KkSQgMDERwcDCmTp2KW7duueT6iIiIyLu4PUDasGED0tPTsWDBAhw9ehQxMTFISkrClStXzLbfs2cPJk6ciN27dyM/Px+RkZF49NFHcenSJX2bxYsXY+nSpVi1ahUOHjyIVq1aISkpCXfu3NG3mTRpEk6ePImdO3diy5Yt+PLLLzF9+nSXXy8RERF5PpW7F6uNj4/HoEGDsHz5cgCATqdDZGQknnnmGcydO7fe19fW1qJNmzZYvnw5Jk+eDCEEIiIi8Nxzz2HOnDkAgLKyMoSGhmLt2rWYMGECTp8+jfvuuw+HDx/GwIEDAQA7duzAyJEj8eOPPyIiIsJ1F0xEREQer4U7D15dXY2CggLMmzdPv02tViMxMRH5+fk27aOiogI1NTVo27YtAKC4uBglJSVITEzUtwkKCkJ8fDzy8/MxYcIE5OfnIzg4WB8cAUBiYiLUajUOHjyIxx57rM5xqqqqUFVVpX+s0+lw/fp1tGvXDiqVyu5rJyIiosYnhMDNmzcREREBtdryQJpbA6SrV6+itrYWoaGhJttDQ0Px3Xff2bSPF198EREREfqAqKSkRL8P5T7l50pKStChQweT51u0aIG2bdvq2yhlZWXh1VdftemciIiIyLNdvHgR99xzj8Xn3RogNdSiRYuwfv167NmzB76+vi491rx585Cenq5/XFZWhk6dOuHixYsIDAx06bHrtW0bMHEioFYDOh3w0UfAyJHuPSciIiIPVF5ejsjISLRu3dpqO7cGSCEhIdBoNCgtLTXZXlpairCwMKuvfeutt7Bo0SLs2rUL/fr102+XX1daWorw8HCTfcbGxurbKJPA7969i+vXr1s8ro+PD3x8fOpsDwwMdH+ANGEC4O8P7NkDPPwwkJLi3vMhIiLycPWlx7h1FptWq8WAAQOQl5en36bT6ZCXl4eEhASLr1u8eDFef/117NixwySPCAA6d+6MsLAwk32Wl5fj4MGD+n0mJCTgxo0bKCgo0Lf54osvoNPpEB8f76zLa1wpKcCSJdLPaWlAbq57z4eIiMibCTdbv3698PHxEWvXrhWnTp0S06dPF8HBwaKkpEQIIcQf//hHMXfuXH37RYsWCa1WKz799FNx+fJl/e3mzZsmbYKDg0VOTo44ceKEGD16tOjcubOorKzUtxk+fLjo37+/OHjwoPj6669F9+7dxcSJE20+77KyMgFAlJWVOeFdqF9OjhCzZ0v3VhsBQmg00r3VxkRERM2Prd/fbg+QhBBi2bJlolOnTkKr1Yq4uDhx4MAB/XNDhgwRqamp+sdRUVECQJ3bggUL9G10Op145ZVXRGhoqPDx8RHDhg0TZ86cMTnmtWvXxMSJE0VAQIAIDAwUU6ZMMQmy6tOYAZLNcc/s2YZGarUQ/fszSCIiIjJi6/e32+sgeavy8nIEBQWhrKzM5TlIaWnAsmVAbS2g0QCzZhlG00zk5gKjRxuSteX7nBzmJRFRk6LT6VBdXe3u0yAP1LJlS2g0GovP2/r97dWz2JqLoUOB7GwpOKqtlfKwzUpJkYKhzEzg+HEpONJopORtBkhE1ERUV1ejuLgYOp3O3adCHio4OBhhYWENqlPIAMkLyHGPTZPU5CdHjzZEVEVFUu8SgyQi8nJCCFy+fBkajQaRkZFWC/1R8yOEQEVFhX6muvFsdntxiM1BjTnE5pDcXGD1auleDpQ41EZEXq6mpgZFRUWIiIhAUFCQu0+HPNS1a9dw5coV9OjRo85wm63f3wy9m6qUFKBLF0NwJA+1ERF5sdraWgBSmRgiS/z9/QFIAbWjGCA1ZUOHGoIjq8lLRETehWtgkjXO+P1gDlJTZpy85OcH7N5t2E5EREQWsQepqUtJkXqOFi6UagWMHs0q20RE5DRr165FcHCwu0/D6RggNQe7dxuG2dRqqQwAgyQiokahUqms3jIzMxu0702bNjntXAEgOjoa2dnZTt2nN2KA1BzIuUhy4cjjx9mTRETUSC5fvqy/ZWdnIzAw0GTbnDlz3H2KZAYDpOZAzkWKiTEESZzVRkTUKMLCwvS3oKAgqFQqk23r169Hr1694Ovri3vvvRfvvvuu/rXV1dWYOXMmwsPD4evri6ioKGRlZQGQenoA4LHHHoNKpdI/Pn78OIYOHYrWrVsjMDAQAwYMwJEjR/T7/Prrr/HrX/8afn5+iIyMxKxZs3D79m0AwMMPP4zz588jLS1N38PliJUrV6Jr167QarXo2bMn/vnPf+qfE0IgMzMTnTp1go+PDyIiIjBr1iz98++++y66d+8OX19fhIaG4ne/+51D59BQTNL2Mrm50ojZ0KF25lqbKyDJWW1E1Jw5/AfVeT788EPMnz8fy5cvR//+/XHs2DFMmzYNrVq1QmpqKpYuXYrc3Fx8/PHH6NSpEy5evIiLFy8CAA4fPowOHTpgzZo1GD58uL7ez6RJk9C/f3+sXLkSGo0GhYWFaNmyJQDg3LlzGD58ON544w28//77+PnnnzFz5kzMnDkTa9asweeff46YmBhMnz4d06ZNc+iaNm7ciGeffRbZ2dlITEzEli1bMGXKFNxzzz0YOnQoPvvsM7zzzjtYv349evfujZKSEhw/fhwAcOTIEcyaNQv//Oc/8eCDD+L69ev46quvnPBOO8DVi8I1VY25WK3M5kVr69tJWpoQGRnS4ra27iQnx772REQuUFlZKU6dOiUqKysbtiOn/EG135o1a0RQUJD+cdeuXcW6detM2rz++usiISFBCCHEM888I/7nf/5H6HQ6s/sDIDZu3GiyrXXr1mLt2rVm20+dOlVMnz7dZNtXX30l1Gq1/j2NiooS77zzjsPX9OCDD4pp06aZtPn9738vRo4cKYQQ4u233xY9evQQ1dXVdfb12WeficDAQFFeXm7z8c2x9nti6/c3h9i8iHGutcMjZI7MapMXweUsOCJqKpzyB7Vhbt++jXPnzmHq1KkICAjQ39544w2cO3cOAPDEE0+gsLAQPXv2xKxZs/Dvf/+73v2mp6fjySefRGJiIhYtWqTfFyANv61du9bkeElJSdDpdCguLnbKdZ0+fRqDBw822TZ48GCcPn0aAPD73/8elZWV6NKlC6ZNm4aNGzfi7t27AIBHHnkEUVFR6NKlC/74xz/iww8/REVFhVPOy14MkLyI0+o+2jurzQP+kBAROZUHFNK9desWAOBvf/sbCgsL9bdvv/0WBw4cAADcf//9KC4uxuuvv47KykqMGzeu3pyczMxMnDx5EqNGjcIXX3yB++67Dxs3btQf889//rPJ8Y4fP46zZ8+ia9eurr3g/4qMjMSZM2fw7rvvws/PD08//TQeeugh1NTUoHXr1jh69Cg++ugjhIeHY/78+YiJicGNGzca5dxMNKgPqxlzxxCbEIYRsgb1Bstdy2q16b2lnbqpK5qISMlpQ2xCOOkPqn2Uw1ERERHitddes/n1O3bsEADEtWvXhBBCtGzZUnz66adWXzNhwgSRnJwshBDi8ccfF8OGDbPavnv37uKtt96y+ZxsHWIbNWqU2dd/9913AoAoKCio89ytW7dEixYtxGeffWbz+QjhnCE2Jml7mZQUJ+QSyrPaMjOlKf/Gs9rM7dy4IvfDD7MSNxE1DU75g9owr776KmbNmoWgoCAMHz4cVVVVOHLkCH755Rekp6djyZIlCA8PR//+/aFWq/HJJ58gLCxMX5gxOjoaeXl5GDx4MHx8fODr64vnn38ev/vd79C5c2f8+OOPOHz4MMaOHQsAePHFF/HAAw9g5syZePLJJ9GqVSucOnUKO3fuxPLly/X7/PLLLzFhwgT4+PggJCTErmt6/vnnMW7cOPTv3x+JiYnYvHkzPv/8c+zatQuAVFiytrYW8fHx8Pf3x7/+9S/4+fkhKioKW7ZswX/+8x889NBDaNOmDbZt2wadToeePXs67023lV0hGem5qwfJqZQ9Q8nJ7B0iIo/m1B4kN1D2tgghxIcffihiY2OFVqsVbdq0EQ899JD4/PPPhRBCvPfeeyI2Nla0atVKBAYGimHDhomjR4/qX5ubmyu6desmWrRoIaKiokRVVZWYMGGCiIyMFFqtVkRERIiZM2eavF+HDh0SjzzyiAgICBCtWrUS/fr1E2+++ab++fz8fNGvXz/h4+MjbAkTzF3Tu+++K7p06SJatmwpevToIT744AP9cxs3bhTx8fEiMDBQtGrVSjzwwANi165dQggpYXzIkCGiTZs2ws/PT/Tr109s2LDB5vdX5oweJJUQQjR+WOb9ysvLERQUhLKyMgQGBrr7dByXmwusXi3dy2PxOTlu/18VEZE5d+7cQXFxMTp37gxfX193nw55KGu/J7Z+fzNJ20vl5gJpaU6YUJaSAnTpYjkJ22kHIiIi8h4MkLyQ02fdK2dzFBVJO+X0fiKiZm/EiBEmZQGMbwsXLnT36bkMk7S9kLlZ9w0aEZOTsOWhtm3bgM2bgc6dpTIATjsQERF5m7///e+orKw0+1zbtm0b+WwaDwMkLzR0KJCd7eTyHSkpppEXAMhFw+QgiUuTEBE1Ox07dnT3KbgFh9i8kNzhM2uWk/Op5aE248UJ1WogNpaJ20RE1KwwQPJSKSnAkiXSz07LoZYjr+Rk6bFGI9VIWrCAwRERETUrHGLzYnIOtUYjDbk5pZNHLpyWm8vCkERE1GwxQPJiTk/WNuYBFWaJiIjchUNsXszS7HwiIiJqGAZIXkxOGRo1Snq8bRvLFRERNRfR0dHIzs5292lY9MMPP0ClUqGwsNDdp+IQBkherr5C2E7HytpERHZRqVRWb5mZmQ7t9/Dhw5g+fbpzT9aKJ554AmPGjGm047kbc5CaAJfURTLHJVnhRERN2+XLl/U/b9iwAfPnz8eZM2f02wICAvQ/CyFQW1uLFi3q/3pu3769c0+UTLAHqQkwrouUkSElb7ukg8dcVjgREVkVFhamvwUFBUGlUukff/fdd2jdujW2b9+OAQMGwMfHB19//TXOnTuH0aNHIzQ0FAEBARg0aBB27dplsl/lEJtKpcLf//53PPbYY/D390f37t2Ra/Rl8Msvv2DSpElo3749/Pz80L17d6xZs0b//MWLFzFu3DgEBwejbdu2GD16NH744QcAQGZmJv7xj38gJydH3/O1x4HvgL179yIuLg4+Pj4IDw/H3LlzcffuXf3zn376Kfr27Qs/Pz+0a9cOiYmJuH37NgBgz549iIuLQ6tWrRAcHIzBgwfj/Pnzdp+DrRggNREpKVLP0cKFLlw6TZkVzsraROTFPCljYO7cuVi0aBFOnz6Nfv364datWxg5ciTy8vJw7NgxDB8+HMnJybhw4YLV/bz66qsYN24cTpw4gZEjR2LSpEm4fv06AOCVV17BqVOnsH37dpw+fRorV65ESEgIAKCmpgZJSUlo3bo1vvrqK+zbtw8BAQEYPnw4qqurMWfOHIwbNw7Dhw/H5cuXcfnyZTz44IN2XeOlS5cwcuRIDBo0CMePH8fKlSuxevVqvPHGGwCknraJEyfiT3/6E06fPo09e/bgt7/9LYQQuHv3LsaMGYMhQ4bgxIkTyM/Px/Tp06EyLmzsbIIcUlZWJgCIsrIyd5+K3uzZQmg0QgDSfVqaCw6SkyPtOCfHBTsnIrKusrJSnDp1SlRWVjZoPzk5hr+VQOP9SVuzZo0ICgrSP969e7cAIDZt2lTva3v37i2WLVumfxwVFSXeeecd/WMA4uWXX9Y/vnXrlgAgtm/fLoQQIjk5WUyZMsXsvv/5z3+Knj17Cp1Op99WVVUl/Pz8xP/3//1/QgghUlNTxejRo225TCGEEMXFxQKAOHbsmBBCiIyMjDrHWLFihQgICBC1tbWioKBAABA//PBDnX1du3ZNABB79uyx6djWfk9s/f5mD1IT0igdPC4p4U1E1Lg8LWNg4MCBJo9v3bqFOXPmoFevXggODkZAQABOnz5dbw9Sv3799D+3atUKgYGBuHLlCgDgL3/5C9avX4/Y2Fi88MIL2L9/v77t8ePHUVRUhNatWyMgIAABAQFo27Yt7ty5g3PnzjnlGk+fPo2EhASTXp/Bgwfj1q1b+PHHHxETE4Nhw4ahb9+++P3vf4+//e1v+OWXXwBIi+I+8cQTSEpKQnJyMv7f//t/JrldrsAAqQlptFwkOVnbZWN5RESu5WkZA61atTJ5PGfOHGzcuBELFy7EV199hcLCQvTt2xfV1dVW99OyZUuTxyqVCjqdDgAwYsQInD9/Hmlpafjpp58wbNgwzJkzB4AUkA0YMACFhYUmt++//x6PP/64E6/UMo1Gg507d2L79u247777sGzZMvTs2RPF/104fc2aNcjPz8eDDz6IDRs2oEePHjhw4IDLzsftAdKKFSsQHR0NX19fxMfH49ChQxbbnjx5EmPHjkV0dDRUKpXZ+g/yc8rbjBkz9G0efvjhOs8/9dRTrri8RtcouUjG//VSq4HMTAZJRORVXLbot5Ps27cPTzzxBB577DH07dsXYWFh+oTphmjfvj1SU1Pxr3/9C9nZ2XjvvfcAAPfffz/Onj2LDh06oFu3bia3oKAgAIBWq0Vtba3Dx+7Vqxfy8/MhhNBv27dvH1q3bo177rkHgBTQDR48GK+++iqOHTsGrVaLjRs36tv3798f8+bNw/79+9GnTx+sW7fO4fOpj1sDpA0bNiA9PR0LFizA0aNHERMTg6SkJH13oFJFRQW6dOmCRYsWISwszGybw4cP6xPILl++jJ07dwIAfv/735u0mzZtmkm7xYsXO/fi3Mjl8Yv8Xy+1WlrM9vhx9iQRkdeRMwY8LTgCgO7du+Pzzz9HYWEhjh8/jscff1zfE+So+fPnIycnB0VFRTh58iS2bNmCXr16AQAmTZqEkJAQjB49Gl999RWKi4uxZ88ezJo1Cz/++CMAqQPixIkTOHPmDK5evYqamhq7jv/000/j4sWLeOaZZ/Ddd98hJycHCxYsQHp6OtRqNQ4ePIiFCxfiyJEjuHDhAj7//HP8/PPP6NWrF4qLizFv3jzk5+fj/Pnz+Pe//42zZ8/qz98V3BogLVmyBNOmTcOUKVNw3333YdWqVfD398f7779vtv2gQYPw17/+FRMmTICPj4/ZNu3btzeZUrllyxZ07doVQ4YMMWnn7+9v0i4wMNDp1+cuLo9f5P96xcQYDuIJg/hERE3EkiVL0KZNGzz44INITk5GUlIS7r///gbtU6vVYt68eejXrx8eeughaDQarF+/HoD0nfjll1+iU6dO+O1vf4tevXph6tSpuHPnjv77cdq0aejZsycGDhyI9u3bY9++fXYdv2PHjti2bRsOHTqEmJgYPPXUU5g6dSpefvllAEBgYCC+/PJLjBw5Ej169MDLL7+Mt99+GyNGjIC/vz++++47jB07Fj169MD06dMxY8YM/PnPf27Qe2KVTengLlBVVSU0Go3YuHGjyfbJkyeLlJSUel+vzN63dIx27dqJN99802T7kCFDREhIiGjXrp3o3bu3mDt3rrh9+7bVfd25c0eUlZXpbxcvXvS4WWzGcnKE6N9fCLVamqGhVkuPnTpTQzkNJDmZs9uIyKWcNYuNmjavnsV29epV1NbWIjQ01GR7aGgoSkpKnHKMTZs24caNG3jiiSdMtj/++OP417/+hd27d2PevHn45z//iT/84Q9W95WVlYWgoCD9LTIy0inn6CopKdLQmk7XCD1JXAyOiIiaGLcnabvS6tWrMWLECERERJhsnz59OpKSktC3b19MmjQJH3zwATZu3Gh1KuO8efNQVlamv128eNHVp99gjTIS1uiLwRERkSdYuHChviSA8jZixAh3n16DuW0ttpCQEGg0GpSWlppsLy0ttZiAbY/z589j165d+Pzzz+ttGx8fDwAoKipC165dzbbx8fGxmPfkyeTkQ3kJNZdMZ220xeCIiMhTPPXUUxg3bpzZ5/z8/Br5bJzPbQGSVqvFgAEDkJeXp18dWKfTIS8vDzNnzmzw/tesWYMOHTpglDz8Y0VhYSEAIDw8vMHH9URyT9KePYCfnzTLTd7uHQcgIiJP07ZtW7Rt29bdp+EybguQACA9PR2pqakYOHAg4uLikJ2djdu3b2PKlCkAgMmTJ6Njx47IysoCAFRXV+PUqVP6ny9duoTCwkIEBASgW7du+v3qdDqsWbMGqampdVZEPnfuHNatW4eRI0eiXbt2OHHiBNLS0vDQQw+ZVCBtapQ9SdnZTq79Ye4AycnAk08yUCIiIq/j1gBp/Pjx+PnnnzF//nyUlJQgNjYWO3bs0CduX7hwAWq1IU3qp59+Qv/+/fWP33rrLbz11lsYMmSIyarCu3btwoULF/CnP/2pzjG1Wi127dqlD8YiIyMxduxY/TTDpsxcaX2nxi7GBwCALVuAzZs9swobEXk1YVRskEipoTWjAEAl+FvmkPLycgQFBaGsrMxraijJK4TIMUxyMtC3L1BRIaURNTiGkQ+gUgHyr5VGI5WqlddvIyJqgNraWpw9exb+/v5o3769a1dzJ68jhEB1dTV+/vln1NbWonv37iYdLYDt398MkBzkjQESIMUwq1dL9/LMNvneKR09xgeQIzH2IBGRE8mLm/Lriyzx9/dHeHg4tFptneds/f526xAbNb6UlLojYcbT/xscx6SkSLfcXCZtE5FLBAQEoHv37nYvdUHNg0ajQYsWLRrcu8gAqRmSZ+Ub9yA5fXa+y7PCiag502g00Gg07j4NasIYIDVDyln5lZVScOT02MXlWeFERESuwQCpmZJHwlyKBSSJiMhLMUAiAFLK0O7dTprNJpO7qlavlma1HTzogoMQERE5H2exOchbZ7GZo5z+79RUIXnnLpkyR0REZB9bv7+b9GK1ZBtzqUKW5OYCaWnSvV07l4t2uWTFXCIiIudigEQYOtQQHNXWAkVFdQOg3Fypw2f0aGDZMunepiBJ3rlcqMslU+aIiIici0NsDmpKQ2yA9fqODS6QbVwTyWVT5oiIiOrHQpFkF2UBSeNZ+crCkiqVnZ1AjTJljoiIyHk4xEZ6lobajLcD0hpuDcqxtjuRiYiIqHFxiM1BTW2ITWZpqA2QepQaPDrm0ilzRERE1nEWGzkkJQXo0sUQv6jVQGam9NySJU6IZeyZMkdEROQmDJCoDuOJZzodcPx43VlrDo+SKcfxOJuNiIg8EAMkqkMugB0TYwiSjDt75FEyu6b7K3c+axaQkSH1KDEXiYiIPAxnsZFZ8lCacbqQ3NnT4DVolTvPzpYyv598kvlIRETkEdiDRBYZd/YY51I7ZZRMWTtgyxYHuqOIiIhcgwESWZWSUjc521LgZBc5ylKppMdCMGmbiIg8BofYyCENrv0oR1nKmgJM2iYiIg/AAIncR46yjJci2b3b8BwREZGbMECiBsnNlWKaoUMbENOYS9pmAUkiInIj5iCRwxo03V+JBSSJiMiDMEAihzk1pmEBSSIi8iAMkMhhlha3dYhTpsYRERE5BwMkcpgc04waJT3etq2BQ21yTQHAwXVMiIiInIMBEjWIcnHbBg+1KROb5FluREREjYgBEjWYU9OHWGGbiIg8AAMkajCnrj9rT4Xt3FwOxRERkUuohBDC3SfhjcrLyxEUFISysjIEBga6+3Q8gjw6JncAOZxrnZtbt8K2cmdOOxgRETUntn5/s1AkOY25af8OxSy2VNh22sGIiIjq4hAbOY1Tp/0DUsDz8MPAwoV1q1E6/WBEREQGDJDIaZw+7R8w7SlSq4HMTGmHLjkYERGRhAESOZXTp/3LPUVqNaDTAcePGwIhpx+MiIhIwgCJnM6p0/7lnqKYGEOQZBwIcYkSIiJyAbcHSCtWrEB0dDR8fX0RHx+PQ4cOWWx78uRJjB07FtHR0VCpVMjOzq7TJjMzEyqVyuR27733mrS5c+cOZsyYgXbt2iEgIABjx45FaWmpsy+t2XLqtH95h5mZhuDIOBDiEiVEROQCbg2QNmzYgPT0dCxYsABHjx5FTEwMkpKScOXKFbPtKyoq0KVLFyxatAhhYWEW99u7d29cvnxZf/v6669Nnk9LS8PmzZvxySefYO/evfjpp5/w29/+1qnX1twZ51cvXSqNir30UgN3aCkQkpcoYXBERERO4tY6SPHx8Rg0aBCWL18OANDpdIiMjMQzzzyDuXPnWn1tdHQ0Zs+ejdmzZ5tsz8zMxKZNm1BYWGj2dWVlZWjfvj3WrVuH3/3udwCA7777Dr169UJ+fj4eeOABm86ddZDql5YmBUc6nWEbO3mIiMidbP3+dlsPUnV1NQoKCpCYmGg4GbUaiYmJyM/Pb9C+z549i4iICHTp0gWTJk3ChQsX9M8VFBSgpqbG5Lj33nsvOnXq1ODjkqmhQ02DI7XaSTnUrKBNREQu5rYA6erVq6itrUVoaKjJ9tDQUJSUlDi83/j4eKxduxY7duzAypUrUVxcjF//+te4efMmAKCkpARarRbBwcF2Hbeqqgrl5eUmN7IuJUXKQQIM+dUNzqFWLmbLIImIiFygyVXSHjFihP7nfv36IT4+HlFRUfj4448xdepUh/eblZWFV1991Rmn2Ky8+SYQH2+5ILbdWEGbiIgagdt6kEJCQqDRaOrMHistLbWagG2v4OBg9OjRA0VFRQCAsLAwVFdX48aNG3Ydd968eSgrK9PfLl686LRzbOqsFcS2my3T+jkER0REDeS2AEmr1WLAgAHIy8vTb9PpdMjLy0NCQoLTjnPr1i2cO3cO4eHhAIABAwagZcuWJsc9c+YMLly4YPW4Pj4+CAwMNLmR7cx1/Dikvmn9HIIjIiIncOs0//T0dPztb3/DP/7xD5w+fRp/+ctfcPv2bUyZMgUAMHnyZMybN0/fvrq6GoWFhSgsLER1dTUuXbqEwsJCfe8QAMyZMwd79+7FDz/8gP379+Oxxx6DRqPBxIkTAQBBQUGYOnUq0tPTsXv3bhQUFGDKlClISEiweQYb2c+pS6dZm9bvtEiMiIiaNeFmy5YtE506dRJarVbExcWJAwcO6J8bMmSISE1N1T8uLi4WAOrchgwZom8zfvx4ER4eLrRarejYsaMYP368KCoqMjlmZWWlePrpp0WbNm2Ev7+/eOyxx8Tly5ftOu+ysjIBQJSVlTl03c1RTo4QKSlCAEJoNNJ9To4LDuLSAxARkTez9fvbrXWQvBnrIDkmLU0a/ZI7eGbNkjqDnCo3V+o5evhhJnATEZEJW7+/m9wsNvJsQ4cC2dkuXjotJYWBERERNQgDJGpUco6106b9ExERuQADJGp0cjA0erTUk5SdDSQnA08+2cBAKTdXiriGDmXERUREDeLWWWzUfBlPNgOALVsaOCuf0/uJiMiJGCCRW8jT/lUq6bEQDZyVz+n9RETkRAyQyC3kXKTkZOlxg5O2bamwTUREZCNO83cQp/k7jzwr388PqKhoQAoRp/cTEVE9bP3+ZoDkIAZIziWnEMkdQOZWESEiImooW7+/OcRGHsElKURctJaIiBzEAIk8gtNTiDirjYiIGoB1kMgjOL2ApHGXlFoNZGY2cIdERNScsAeJPEZKitRztHChEzp+5C4ptRrQ6YDjx9mTRERENmOARB7FablIcpdUTIwhSGJ9JCIishEDJPIoTs1FSkmRhtbk4Ij1kYiIyEbMQSKP4vRcJK6OS0REDmAdJAexDpJrOb0uEgstERERWAeJvJy5SWgNyq92+g6JiKgpY4BEHsnpk9A4q42IiOzAAIk8ktMnoXFWGxER2YFJ2uSx5BQh49ShoiKp08eh9CFzO+SsNiIiMoNJ2g5iknbjyc0FVq+W7p2SY52ba5jVVlEhDb8xYZuIqFmw9fubPUjk8VJSnLxyiLInKTubs9qIiMgEc5DIKzg9x5qz2oiIyAoGSOQVnJ5jzVltRERkBQMk8hpOXTmEs9qIiMgK5iCRVzFeOeThh6XHubnSiJm/v50515zVRkREFnAWm4M4i839cnOBv/8d2LzZ0Akk39uVcy3PapMjLiIiarI4i42aNHlpNZVKeqzTGe7lkTKbY52UFENXVFqaA11RRETU1DBAIq9kPAnNmFrt4EiZHHEZd0Vx+j8RUbPFAIm80tChUvwiB0kpKUCfPkBlpYMjZcqIy6GuKCIiaioYIJFXMpes3SByxGXcg8SkbSKiZotJ2g5iknYTZLwESWUllyIhImqCbP3+ZoDkIAZInkue9t+guEbOSXLK4m9EROQpbP3+ZqFIalLkuGbZMicuRcICkkREzQ4DJGpSnBbXyEuROFJAUi4XwGVLiIi8FgMkalKUcU1RkYNxipwFPmuWfcNrTuvCIiIid3J7gLRixQpER0fD19cX8fHxOHTokMW2J0+exNixYxEdHQ2VSoXs7Ow6bbKysjBo0CC0bt0aHTp0wJgxY3DmzBmTNg8//DBUKpXJ7amnnnL2pZEbyHHNqFHS423bGhCnpKQAS5ZIP9vaI8ShOSKiJsGtAdKGDRuQnp6OBQsW4OjRo4iJiUFSUhKuXLlitn1FRQW6dOmCRYsWISwszGybvXv3YsaMGThw4AB27tyJmpoaPProo7h9+7ZJu2nTpuHy5cv62+LFi51+feQeKSlAly5OilPs7RFqyNAcERF5DLfWQVqyZAmmTZuGKVOmAABWrVqFrVu34v3338fcuXPrtB80aBAGDRoEAGafB4AdO3aYPF67di06dOiAgoICPPTQQ/rt/v7+FoMs8n7KQpIOxynmeoSsDbc5vUATERG5g9t6kKqrq1FQUIDExETDyajVSExMRH5+vtOOU1ZWBgBo27atyfYPP/wQISEh6NOnD+bNm4eKigqr+6mqqkJ5ebnJjTyXoylEdTiS1CQPzTE4IiLyWm7rQbp69Spqa2sRGhpqsj00NBTfffedU46h0+kwe/ZsDB48GH369NFvf/zxxxEVFYWIiAicOHECL774Is6cOYPPP//c4r6ysrLw6quvOuW8qHHIa9A2eCc5OcDq1VJgtG0bsHkz6yIRETVxTXqpkRkzZuDbb7/F119/bbJ9+vTp+p/79u2L8PBwDBs2DOfOnUPXrl3N7mvevHlIT0/XPy4vL0dkZKRrTpycqsGFI1NS7B9qIyIir+a2IbaQkBBoNBqUlpaabC8tLXVKbtDMmTOxZcsW7N69G/fcc4/VtvHx8QCAoqIii218fHwQGBhociPP57RZ944kX7MeEhGR13JbgKTVajFgwADk5eXpt+l0OuTl5SEhIcHh/QohMHPmTGzcuBFffPEFOnfuXO9rCgsLAQDh4eEOH5c8k9Nm3RsnNWVkSDu2FviwHhIRkVdz6xBbeno6UlNTMXDgQMTFxSE7Oxu3b9/Wz2qbPHkyOnbsiKysLABSYvepU6f0P1+6dAmFhYUICAhAt27dAEjDauvWrUNOTg5at26NkpISAEBQUBD8/Pxw7tw5rFu3DiNHjkS7du1w4sQJpKWl4aGHHkK/fv3c8C6QKylns8k51g4PtQGGNdqys6WgCag7hschOSIi7ybcbNmyZaJTp05Cq9WKuLg4ceDAAf1zQ4YMEampqfrHxcXFAkCd25AhQ/RtzD0PQKxZs0YIIcSFCxfEQw89JNq2bSt8fHxEt27dxPPPPy/KysrsOu+ysjIBwO7XUePLyREiJUUIQAiNRrrPyXFwZ7NnG3ai0VjecU6Okw5IRETOZOv3t0oIIdwSmXk5W1cDJs+QliaNdskdOrNmGYpk20UeOpN7h5KTpZlttbWAWg3ExACZmVJvUW4u6yEREXkYW7+/m/QsNiKZ0wpHGheC9PMDvvnGEBzpdMDx41IAJZcBYGBEROSVGCBRs6CMa3bvNmx3aGeAoScJAKKjgR9+kIIk5hwREXk9BkjUbFjKsXYojlEmYffpA/znP1yDjYioiXDrYrVEjc04rlGrpXQhh2bgK+siTZ3qpLVNiIjIEzBJ20FM0vZOco61nDIk3zsU0zAJm4jI69j6/c0eJGpW5FykmBhDcORwAUkuSktE1GQxB4maHWUuElOGiIhIiQESNUtOndVGRERNDgMkaracOquNiIiaFOYgUbPmtMVsiYioSWGARM2acra+vJgtERE1bwyQqFmTc5FGjZIeb9smDbkxSCIiat4YIFGzl5ICdOnCoTYiIjJggEQEDrUREZEpBkhEcOFQW24ukJbGaIuIyMswQCL6L6cPtcnrmixbxsQmIiIvwwCJyIi5obaXXnKwE4g1BIiIvBYDJCIjyqG2rVuBhQuBpUsd6ARiYhMRkddigESkYDzUptNJ2xxa1JY1BIiIvBYDJCIz5M4f9X//hajVDi5qyxoCREReiWuxEZmhXMy2slIKjhxap23oUGmhNzlIsjvKIiKixqYSQgh3n4Q3Ki8vR1BQEMrKyhAYGOju0yFPl5srRVsOR1lEROQMtn5/c4iNyEYNKmmUkgIsWSL9zLpIREQejz1IDmIPUvMilzSSR8lychzoCHLKToiIqCFs/f5mDhKRDYxLGqnVQGamtN2u+MZSXaTdu6U8JQZLREQeg0NsRDYwntWm0wHHjzuhLtLXX9ettM2lSYiIPAIDJCIbyLPaYmIMQZLck2RzLKOsi3TkiHQvB02rV3NpEiIiD8EAichGKSlSQCQHR8Y9SSkpNsYzxnWR5PQ/lUoKkoRgvSQiIg/BAInIDuZ6kgBgyxY7On2Mh9oAIDlZ2umTT5oOwbFeEhGR2zBJm8hOci716NFS548Qhs6fPXsMz+fmWsi/Nq5CqayLpNxucSdERORKDk3z/8c//oGQkBCM+m8uxQsvvID33nsP9913Hz766CNERUU5/UQ9Daf5U26ulDaUm1t35r7dM/rNBUIsC0BE5HQuLRS5cOFC+Pn5AQDy8/OxYsUKLF68GCEhIUhLS3PsjIm8jNwRlJMDzJoFZGRIMY4c69icTiQHQsrkbLt2QkREzuRQgHTx4kV069YNALBp0yaMHTsW06dPR1ZWFr766iunniCRp0tJkUbEFi40xDj+/nakE1kKhJRlAZiTRETUaBwKkAICAnDt2jUAwL///W888sgjAABfX19UVlY67+yIvISykOT27VKP0qxZNoyMWQqE5C4qm3ZCRETO5FCS9iOPPIInn3wS/fv3x/fff4+RI0cCAE6ePIno6Ghnnh+RVxg6FMjONp3+f+yYjXGNtaTtlBQGRkREbuBQD9KKFSuQkJCAn3/+GZ999hnatWsHACgoKMDEiRPt3ld0dDR8fX0RHx+PQ4cOWWx78uRJjB07FtHR0VCpVMjOznZon3fu3MGMGTPQrl07BAQEYOzYsSgtLbXrvImMmZv+L9d+tKkwtryYrXGCNitqExG5j3Cj9evXC61WK95//31x8uRJMW3aNBEcHCxKS0vNtj906JCYM2eO+Oijj0RYWJh45513HNrnU089JSIjI0VeXp44cuSIeOCBB8SDDz5o17mXlZUJAKKsrMyu11HTlpMjTfrXaOTJ/4afc3Ic3InNLyQiovrY+v3tUIC0fft28dVXX+kfL1++XMTExIiJEyeK69ev27yfuLg4MWPGDP3j2tpaERERIbKysup9bVRUlNkAqb593rhxQ7Rs2VJ88skn+janT58WAER+fr7N584AiSzJyREiLU2I5GRDjKNWC9G/v42xzuzZhhdqNNLOiIjIKWz9/nZoiO35559HeXk5AOCbb77Bc889h5EjR6K4uBjp6ek27aO6uhoFBQVITEzUb1Or1UhMTER+fr4jp2XTPgsKClBTU2PS5t5770WnTp2sHreqqgrl5eUmNyJz5NEyuTC23QvcWpu9xqE3IqJG4VCAVFxcjPvuuw8A8Nlnn+E3v/kNFi5ciBUrVmD79u027ePq1auora1FaGioyfbQ0FCUlJQ4clo27bOkpARarRbBwcF2HTcrKwtBQUH6W2RkpEPnSM2HpbykessZWZq9ZqleEhEROZ1DAZJWq0VFRQUAYNeuXXj00UcBAG3btm2yPSvz5s1DWVmZ/nbx4kV3nxJ5AeMFbu0qZ6RM2gZYOJKIqBE5NM3/V7/6FdLT0zF48GAcOnQIGzZsAAB8//33uOeee2zaR0hICDQaTZ3ZY6WlpQgLC3PktGzaZ1hYGKqrq3Hjxg2TXqT6juvj4wMfHx+HzouaN2uz+O0i1xJg4UgiIpdzqAdp+fLlaNGiBT799FOsXLkSHTt2BABs374dw4cPt2kfWq0WAwYMQF5enn6bTqdDXl4eEhISHDktm/Y5YMAAtGzZ0qTNmTNncOHCBYePS1Qfcx1CxmxKLWLhSCKixtNISeNmrV+/Xvj4+Ii1a9eKU6dOienTp4vg4GBRUlIihBDij3/8o5g7d66+fVVVlTh27Jg4duyYCA8PF3PmzBHHjh0TZ8+etXmfQkjT/Dt16iS++OILceTIEZGQkCASEhLsOnfOYiNn4ax+IqLGY+v3t0NDbABQW1uLTZs24fTp0wCA3r17IyUlBRqNxuZ9jB8/Hj///DPmz5+PkpISxMbGYseOHfok6wsXLkCtNnRy/fTTT+jfv7/+8VtvvYW33noLQ4YMwZ7/5mPUt08AeOedd6BWqzF27FhUVVUhKSkJ7777rqNvBZHN5IVshw41dACZSy1i5xARkXuphBDC3hcVFRVh5MiRuHTpEnr27AlAGqaKjIzE1q1b0bVrV6efqKcpLy9HUFAQysrKEBgY6O7TIS8gT0KTgyF5lMzSdiIicj5bv78d6kGaNWsWunbtigMHDqBt27YAgGvXruEPf/gDZs2aha1btzp21kRNmKVJaLt3SwvbVlZaSeI21/VEREQu41CAtHfvXpPgCADatWuHRYsWYfDgwU47OaKmRDkJzc/Pxp4j4y6m7GzrXUwMpIiInMKhWWw+Pj64efNmne23bt2CVqtt8EkRNUXKSWgVFTaWNbK1/hELSRIROY1DAdJvfvMbTJ8+HQcPHoSQ1nPDgQMH8NRTTyGF/2slssh4ur+1FUVM2NqQhSSJiJzGoSTtGzduIDU1FZs3b0bLli0BADU1NRg9ejTWrFlTZxmPpohJ2tQQ8kiYv389uUfGL6iv0qQrs705dEdETYSt398OBUiyoqIi/TT/Xr16oVu3bo7uyuswQCJHOSWOMY6wKioMgYstgZRbTpiIyDM4fRZbenq61ed3796t/3nJkiW27pao2Wlw3SM5YJFXwFWrTZO3nR28sFATETVDNgdIx44ds6mdSqVy+GSImoMGL6lmHLAAhiApM1N67OzghWvAEVEz1KAhtuaMQ2zUEA0aCVP2IKlUgBCGx7YMgdmbU+SKoTtbMf+JiJyoUXKQmjMGSORWcsDi5wds3w4cPy4FRxqNVEdAOcxtHGQAtucUuTs4Yf4TETmZSytpE5GbGecaxcebBhEPP2w5IMrOBpKTbcspsqdApasw/4mI3IQBEpG3kytQykNggGlg07mzNPwmBxkqleN1lRo7OGH+ExG5CYfYHMQhNvJYaWlSNW05iVtmnKMEuLeukj3cmf9ERE0Oc5BcjAESuYJTUn7kwEZO3gak4Cg2Fhg+3LRuki37YnBCRE0IAyQXY4BEzubUDpvcXGD1aule3mFGBrBwoft7hKjxuDvJnsgD2fr97dBabETkfE5dSk3OS3JodVxqErh4MVGDMEAi8hDKNWmLipzwnWbv6ri5uVIOE79MvR8XLyZqEA6xOYhDbOQK5kbGkpOBJ5900giJMqfI3vpIHLLxHraM2fLzpGaIOUguxgCJXEU5CU3OtXZ6ypDyC7RzZ+D8ecsFJ6194VpaPNeZvOXL3JPO01qSvafMUiRqZMxBIvJS8kiYvKyhEC4aIVGu6VZcbFjXzdwYn6UhG/mLdulSKQl86VLn57x4Sz6Np52n8RCrEofgiKxigETkYeT86uRk6bHLaiQqIzFACo6io6Wft20z/ZK3lMMkf9HqdNJjuQfKmV+43vJl7i3nCdiWk0bUjDFAIvJA5iahOX30w1wkptMBvXub/5KX2ytPSP6iVf/3z4ncA+XML1yXZLC7gDcFHZY+TyICwBwkhzEHiZoU41wVwP7cFOPFcysrXVNY0lwGuzO+2J2dM8TimkQejUnaLsYAiZq0+r7k6wsqXJW0bZzBbi6R3F7uSlT2pERuomaGAZKLMUCixuRR36fKoEJZh0B+Xl77TZ6Gl5EBvPmm9f3Wd5H2BDS27M/ZAZctmtvsMY/65SWy4/tbkEPKysoEAFFWVubuU6EmLidHCEAIjUa6z8lx8wnNnm04GUAIlcr0xJTPG9+MTz4nR2qbkSFEcrLtF5mTI0RaWv1tbNmfO95c4/dHo5GupanyuF9eItu/v5mkTeThPG5iVH11CJRJ2zK1WmqTmyv1JBiXBti8WWpjy0XKU9cB81W/c3OBzExDsri1/TkzUdnWKuTuSOR2V4V0j/vlJadq6pX3Gylga3LYg0SNxSP/E56TI0RKiuUTk3t5fvc76Tm1WrofNMi010l5U/ZGWTu+uWPL2+XjyfeuftPs/ZBs6QVz17k1lWOTa3nxZ2vr93cLdwdoRGSd3MkhTxLbvduw3a0nJS9VYi6ZW34eMJ19duSItE2Z+ijnKyUnA3361H+Rxj0TarXUY2Rue2wssGCB6X5cMWvNXI+VtX0bvz+uZq4Xx94lRxx9z4x/eZvzrL6mmIdly++Vt2ukgK3JYQ8SNTYv/g+b5byllBQpB0nuTVFeZHKy+Qu11FOUkWH9TXL2m+jsHis5L8uZH25919zQ58m6pvr+efF1MQeJqInx6nQO47wbQOopkithvvmmYTkM5fInW7aYX7JD7pmIiTH0PqnVwPbt0mw5SzlFtryJ9uRVGFcRl3usHM1lcsYyJebOvb48K3O9cZaWmJGff+mlpp174kxe/Q/XiuZQaLSRArYmhz1I1Nhs7VzxWPbMPjPOUbI208ueHpycnLqz5YzfxPqet3Z8e3KPLPUQNXR2m6O/IPW9h8rn5c+msfK7vJ0X97Q0VbZ+fzNAchADJHKH+nKjmwR7LzInR4j+/Q1f2MrgwlzgExdn+lgemlMmj8uPrQUbtiZcO3Moy1ygVV/5hfrOrb730Ph5W4JXW865uWjMpHx38LLPlgGSizFAIndpNmV07PlSqW9Wm7JHqn9/0zfR+LG12XXJyVIwJddvsvalIAdmcnBlywdn6ZqNv4As9RTZ2/tm63uofN6RfCv2ojRdXvjZMkByMQZI5C5eP9TmKuaCC0u9KspkbuVjuYepvnIEloIE+UMyvsklDuz5IjHX+9W5s2lPjjJ4s6X3zdL/+OsLSuXnjRPrbdFsovpmyNxn6+E9Sl4VIC1fvlxERUUJHx8fERcXJw4ePGi1/ccffyx69uwpfHx8RJ8+fcTWrVtNngdg9rZ48WJ9m6ioqDrPZ2Vl2XzODJDInZrFUJszKKPJlBTT3iXjL3lzj+U32VKgZOkLf/Zsy8N1ynMw1xtlHBhZO7a5/cu9SZYCGHf8j98LexnIRsrPtr6ZpB7AawKk9evXC61WK95//31x8uRJMW3aNBEcHCxKS0vNtt+3b5/QaDRi8eLF4tSpU+Lll18WLVu2FN98842+zeXLl01u77//vlCpVOLcuXP6NlFRUeK1114zaXfr1i2bz5sBErkb/1Nuo4bmfxgHSrYmKpvrQVJ+UJaGrCzlQxnf1GohunSxf0itvlwjR94bW4Yb5bZNOQ+nOTP+bL3gD5Ot398qIYRozFlzSvHx8Rg0aBCWL18OANDpdIiMjMQzzzyDuXPn1mk/fvx43L59G1u2bNFve+CBBxAbG4tVq1aZPcaYMWNw8+ZN5OXl6bdFR0dj9uzZmD17tkPnzcVqyd3qWzOWnEwuiunnB1RWGu4tFUCUC2SWlACHDtVdnNZ4oVyZRgP06wecOGHYLi/2GxdXdz+AoQhnfYvfKhcRlu/NtZcLG/r7AxUV5gscWtsfYHthRGURRVuObe313sxbrsXaeXrBYsxesVhtVVWV0Gg0YuPGjSbbJ0+eLFJSUsy+JjIyUrzzzjsm2+bPny/69etntn1JSYlo0aKF+PDDD022R0VFidDQUNG2bVsRGxsrFi9eLGpqaiye6507d0RZWZn+dvHiRfYgkdtZGmrz8BSA5sdc70l9PUi2DAta27+S8f/s1Woh7r/f+hBcfcnY5hYlNu7ZMr4eS++JMr9KztNSHru+gqGuSMhT/iNy5B9Vfa+xlnzvqf94rZ2ncY9iSorHJkh6xRDbpUuXBACxf/9+k+3PP/+8iIuLM/uali1binXr1plsW7FihejQoYPZ9v/7v/8r2rRpIyorK022v/3222L37t3i+PHjYuXKlSI4OFikWekKXLBggdm8JgZI5G7KHm3mJnkRS0nPrhiOsvUL2FzgY26oRBlIWRsStDQE6Uh+lfE+lKUHrJU2sCfAqS+vxpYvfltnBRrv08OHpoQQlofQlNfjwX+EGCD9V8+ePcXMmTPrPZfVq1eLFi1aiDt37ph9nj1I5KmUf5eMJzl58t9ZcgNbAi97pvMbB3jm6iTJr7elplJ9N2v5W7YGc7Z+YSuDAGUZCFtqTNWXi6Ps0ZN73qz1zLgr0LClp8v4elQqw3vkgX+EvGKpkZCQEGg0GpSWlppsLy0tRVhYmNnXhIWF2dz+q6++wpkzZ/Dkk0/Wey7x8fG4e/cufvjhB7PP+/j4IDAw0ORG5Ankiv+jRkmPz583pITU1gJFRVwRgv4rJcWwrIu1Njk5wLPPSsu2PPus5TwSeX9vviktQaLTGZaTAQx5SQ8/LD2W81OOHzf8kgJSfhVgeG1KinRs+ZhyPktRkbTMibw4sLyPLl1M28nHk49pbjFha4yXxqmtBUaMkO5VKul5Ierfj7+/YXkW+ZyMl4KRjyFfh/zdM2qU6fvtjCVoGkJ5fMD8EiPG75kcSpr7PLxIC3ceXKvVYsCAAcjLy8OYMWMASEnaeXl5mDlzptnXJCQkIC8vzyS5eufOnUhISKjTdvXq1RgwYABiYmLqPZfCwkKo1Wp06NDBoWshciflMmZqNRAdDfznP8C2bcDmzUziJjukpNj/iyIHVnv2GL4Q5Z/lfSl/SWNjgQULDMnZyvaAIdk9NxfYulX6ZZYT1+Xg4p13DMfz85OOc/Ag8M03UnvjYMo40DJOAlcmh2dkGJLx5cfffmuaEC8HPcqkcgBYuNBw3IwMaZucvJydLf2DzMiQ1g+UA0aNBuja1fT6La3lZi6x3dbkbnvamzu+uSDbls/f2zRSj5ZF69evFz4+PmLt2rXi1KlTYvr06SI4OFiUlJQIIYT44x//KObOnatvv2/fPtGiRQvx1ltvidOnT4sFCxbUmeYvhNSF5u/vL1auXFnnmPv37xfvvPOOKCwsFOfOnRP/+te/RPv27cXkyZNtPm9O8ydPYy2lwdZRASKXcjQR2VJCuDLR3NrQm7kEcvneUnK4uZo+xsOKcpK58nXmxrltLVpaX66SsuCovXWH7P0MGpoIrywFYUtJCBfzihwk2bJly0SnTp2EVqsVcXFx4sCBA/rnhgwZIlJTU03af/zxx6JHjx5Cq9WK3r171ykUKYQQ//d//yf8/PzEjRs36jxXUFAg4uPjRVBQkPD19RW9evUSCxcutJh/ZA4DJPJExikm5vJg1WopnYJBErmNIwnoDZlZZ/yPQPm/hvpynpRL0ihzoOpLMjc+T3OvMa48rUzSVwYV5oqWKs/R0j9w4/2Zq4NlqZ6VclaavcGtpUR+c5+fPTW1GsirAiRvxACJvIG5Gof2LqHFcgHkESzN+FO2MffLLpdKsNbDZC6wsdQ7YykQs6WHq75Aw9I1WOoStlYawVwvlzJIka/RUq9aQ2bZWXufjIMzSz1xLvrD4zWFIr0VC0WSN5HzVI1THWbNktIDvLjeG1FdyoKe5nKa5OeVOUUpKUCfPqavM5cbpfzHIb9OuT9rRTst5edYKiA6apTpvpWFQ7t0kRK9dbr63yM5/2v4cNMcKEvMHd+WPwjKYqLK3LFBg4DDhw3blecYEyP94XLyHx5bv78ZIDmIARJ5G+XfdPlvk7lK3MqAyoV/q4jcy1qw4sjrHN2f8eutVSiX9717tyGQMg5gzAUbxowTx40TyS29Tt6uPL6t12auAr0cSNZ3TGvV3huAAZKLMUAib2Q8IUj5t0l+bOnvpov+VhGRUn29YHIbZRcvYPgHrgywlL1jxgGWcY+SuV61qVOd+49e2Usm/5GRz1E5s2/WLGnmnJPY+v3t1mn+RNS4lOUAjAkhbd++3XQmdkgIcPWq4W/Vnj0MkIhcypYyC8pp9XJ742HB+tYLzM42/GOXyy0ADe8Fq8/QoabHTk42DcLi402DPzfVUWIPkoPYg0TeSvkfT2Uqg9yDZOkx6ykRNRGuDoQacmwXnhuH2FyMARJ5M+XfHuP/cFZUSDXvlHmqti4aT0TkyTjERkQWKXvw5Z8tzVpTDs1xqI2Imjq3rsVGRJ7D0ooGMuXyVNbSAoyXnCIi8kYMkIgIQP0BkJwTqlyjUsnda2sSETkDAyQiAmBbACQv3g5Y7iGqryeKiMgbMEAiIj05ALKWW1RfD5GyJ6qoiL1IROR9GCARkV2s9RDl5krPZ2RIhX0BYNs2DrURkfdhgEREdrGUq2Tcs7RwoaHwpLOG2pj4TUSNidP8icguxgV8/fykHiOgbs+SSmX7rLf6GBe3zM5mDSYicj0GSERkN2XdpOxsafFb44Bo6lTp5oxiuKzBRESNjQESETlEuabbkSPS/ahRpssqOSOQUS7d5KalmYioGWGAREQOkYMWeSFuOeeoa1fToEhO3Pb3l5YxGTrUvqDJOPHb2rqbRETOxACJiBwi5yIp12gz7t2Rc4fUakCnk+7tySFSLqzL3CMiaiycxUZEDpODJEsFJuVhOJ1OeqzT1T+jzXi2GotOEpG7sAeJiBpMufitTB6GM+5BMtfLtHu31BYwTfzOyHDeTDgiInswQCIil1GWBDDOIcrNBf7+d2DzZkNAlJxs2mNUWWl4vfHr5IDKk4fbvOU8icg8lRBCuPskvFF5eTmCgoJQVlaGwMBAd58OkdcwDozkBG9A6l2Kjgb+8x9DkJScDDz5pCHAsJaT5EkBCXOniDyXrd/f7EEiokYjBw4qlfTY+L9nOh3www/SzwMGAIcOScuUbN5sCDCMc5LUaiAz0/B6TyokybpNRN6PSdpE1GjkwEEOjORAqUsXQ56SRgPU1JhPzpaXOZHbHj8uBUZ//7tnJXNbWo6FiLwHAyQiajTGgQMgDaHl5ADvvGMIjmprgREjzAcYck5TTIxp4vfJk54VkMjnaW5mHxF5B+YgOYg5SESOyc01v/yIcruldnJbZX0lnU5qZ1zFm4hIydbvbwZIDmKAROReublSDtLx44bep1mzgCVL3H1mROTJmKRNRE2acsFceWjN3bPZ3H18InIO9iA5iD1IRJ7BeCgOMA2YkpOBvn0dWwPO0XPh9H7zGDiSp+AQm4sxQCLyPGlpwLJlUnBiTM5RcnXAYnx8DvkZMHAkT2Lr9zdnsRFRkyHPkpPLB8jkRO7MTOnL2pjx2m/OOr6nzKbzFFxTj7wRAyQiajLk6fXJydJj9X//wqlUpnWT5GBI7tlYtsx0e0OPL0/vB5wXfHkzBo7kjZikTURNirxwrpyb5OcHbN9uOttN7sHIzDQsoNvQitfGOTZLlpgOK7m7ure783+M1+QzV7aByBMxB8lBzEEi8h7KHJhBg4DDh+vWUVKu/Wbrvo0X3ZVzbHbv9ox8JOb/EJliDhIR0X/JPRijRkmPjxyR7uXgKDpaerxtmxRMyD1Q1uTmSu1Gjwa2bJG2GfdEuWNYSZlPJdeKUvaS1fc6IvKQAGnFihWIjo6Gr68v4uPjcejQIavtP/nkE9x7773w9fVF3759sW3bNpPnn3jiCahUKpPb8OHDTdpcv34dkyZNQmBgIIKDgzF16lTcunXL6ddGRJ4hJUVa8025FpxOB/TubQhkACngkQOll16qG3QoAyPj/cnBUGMvN6LMp3rpJeleHlqUgyRloObsPCxXYzBHjUa42fr164VWqxXvv/++OHnypJg2bZoIDg4WpaWlZtvv27dPaDQasXjxYnHq1Cnx8ssvi5YtW4pvvvlG3yY1NVUMHz5cXL58WX+7fv26yX6GDx8uYmJixIEDB8RXX30lunXrJiZOnGjzeZeVlQkAoqyszLELJ6JGl5MjBCCERiPdp6RI2+TtKpV0r7yp1dL9oEHm28mP5f25w+zZhuvSaITo39/wWK0W4v77655bTo7UTr4+jUaItDTLx8jJkY7jrmtUfn7uOg/ybrZ+f7s9QIqLixMzZszQP66trRUREREiKyvLbPtx48aJUaNGmWyLj48Xf/7zn/WPU1NTxejRoy0e89SpUwKAOHz4sH7b9u3bhUqlEpcuXbLpvBkgEXmnnBwpCDAXLKSkWA+UlLeGBkZywJGR0fDAQxk8ZGRYDybk9nJwJN9bOgdPCE6UQaC1YI7IElu/v906i626uhoFBQWYN2+efptarUZiYiLy8/PNviY/Px/p6ekm25KSkrBp0yaTbXv27EGHDh3Qpk0b/M///A/eeOMNtGvXTr+P4OBgDBw4UN8+MTERarUaBw8exGOPPVbnuFVVVaiqqtI/Li8vt/t6icj95Flulrbn5gKrV0v3cvK2kkolhUjJybYtjqucRWac2G2cKC7PdgPsn3VmbqZYfLzlmWPGtYnUaiA2FliwwPLxLNUyqu88nTmDbuhQ6T1iuQBqFI0UsJl16dIlAUDs37/fZPvzzz8v4uLizL6mZcuWYt26dSbbVqxYITp06KB//NFHH4mcnBxx4sQJsXHjRtGrVy8xaNAgcffuXSGEEG+++abo0aNHnX23b99evPvuu2aPu2DBAgGgzo09SERNk9zTlJFh6FlSDs3Zuh9zPTvmeqk0mrrHakjPlLXX2tsjZG8PlSPHsIWlHkAiW3lFD5KrTJgwQf9z37590a9fP3Tt2hV79uzBsGHDHNrnvHnzTHquysvLERkZ2eBzJSLPpOxpMl7zzZ6eEGXPy/btpgnhMjmJ+ptvGlabydb6S/bWJlK2N9ejZK2Xyt5rsdTzZKkHkMjZ3DqLLSQkBBqNBqWlpSbbS0tLERYWZvY1YWFhdrUHgC5duiAkJARFRUX6fVy5csWkzd27d3H9+nWL+/Hx8UFgYKDJjYiaj5QUqY6RvV/Oyun+LVoYHsv7zcgAfvMb6fH589ZnnZljPLPLnmU9lNdU3wwxuT0AnDtXfxkDR0odGM8S9JaZddQ0uTVA0mq1GDBgAPLy8vTbdDod8vLykJCQYPY1CQkJJu0BYOfOnRbbA8CPP/6Ia9euITw8XL+PGzduoKCgQN/miy++gE6nQ3x8fEMuiYjIhLIG09Gj0v2oUdL2nBzgzTcNJQjk4Cg2tv7yAOaCCX9/x+ovKaf7W6oFJbeTq6vI12Gtl8rWUgfyvs3VlSJqbG4fYktPT0dqaioGDhyIuLg4ZGdn4/bt25gyZQoAYPLkyejYsSOysrIAAM8++yyGDBmCt99+G6NGjcL69etx5MgRvPfeewCAW7du4dVXX8XYsWMRFhaGc+fO4YUXXkC3bt2QlJQEAOjVqxeGDx+OadOmYdWqVaipqcHMmTMxYcIEREREuOeNIKImKyWlbs9O166mAYMyAXnBAml7WpoU9FRUmN5/842U5C0vzCvvt7LS8tCZ3MMk78d4+Mr4/AApSNm8uW5gU991mLt2W3vdlOdgXFeKqNE1Uk6UVcuWLROdOnUSWq1WxMXFiQMHDuifGzJkiEhNTTVp//HHH4sePXoIrVYrevfuLbZu3ap/rqKiQjz66KOiffv2omXLliIqKkpMmzZNlJSUmOzj2rVrYuLEiSIgIEAEBgaKKVOmiJs3b9p8zpzmT0T2sCVh2TgBWTkNX07qtlSCQN5e3zR9S9P6zdWCMjeV3tx1OFIfydxrLNWpInImW7+/uRabg7gWGxHZy55E77Q0w1pu1sglB1JSrJccMLc/5RpxxiUO5J4cc+vTGV8HYN9ab5bWrjPOg+KituRKtn5/M0ByEAMkInIlOR9HrpMkB0Lyvby9vsDI0v7ke3MBjblAyVLgYxx4GQdc5mahyecgXwNgeI08M87WeknOrK9EzYut399uz0EiIqK6jKfV+/lJuUXKe3t6Wcztz9LrlTlTarW06K38nDFl7lRRkbQO3MKFdUsNWMox8vOzXJrAXN4UYFspA6KGYA+Sg9iDRERNWUN7nOReImWvkvFwXEoK0KePVBtKXlTXXHvlOSQnS7PolL1Wrnof2FPVtLAHiYiIHCb3OGVmmgYv5oo9musdkv/rbTwLTVlsEqgbABm3V+5TPge558nVS47YWnSTmia31kEiIiLPlZIiBUhyYGItGJGLQsplBwDpZ+U0f+PilHIAZKn2k7xP9X+/qeQAaurUuvWV6ityKT//0kum7ay9zp6im9T0cIjNQRxiI6LmwtaZZfYkd8vt65sBJx/bWt5UffuxNFSXkWHIlbL2OuMZfX371q0h5Soc3nMNzmJzMQZIRETm2TNVv6HT+nNz6w4DKksXGD8v02iAfv2AEyfM5zIZJ4d/+6302JZ8LOW5ORrg2BI8kmOYg0RERG5hT/Xshiw+a65nyHgmnVxtXFkqQW43YgRw7JjpDDx5qE3Zc6TMhbI2s8/43IzzlwDbAyZbZxGS67AHyUHsQSIici/jGkxqNRAdDfznP4aAyJic4zR8uKFUgrxsi9xDJAcknTsbFg3WaKT15ox7kJS1qMwFP5bOzZ6CmrbOIiT7sAeJiIiaNGUNpt69pcBGWX1cpZKCiwULDAnd1nqIioule+Ok8KlTDblQxmUJ1GopGJKDn+xsaX8+PoakdZ1Oeh4wTfiubw07S7MIActr6tnD1iHA+tq5IlfKI/KvXLjcSZPGtdiIiNzP2vp18r1yTbfZsw3rvWk00vPKdejUaiHuv9/ymnnG+7d0M7d+nrk186ytZadcny4jw/KaevasiWfL2oCW2hkfR/l8cnLD18+z9dwcxbXYXIxDbEREnsfRWW+A/TPwzCV/K8lDY8rimPLwnpwnZetMPuPeK5nxMKC1NfQsnbul5V6U7cwNFRoX7QQMw48NGQq0tHyNs3CIjYiImh1bkr6VBSvl9nJQYMusOvk540ArLg44dMh8OQE5WJNfo8yTMk7GPnjQdOjM+FiWEs7lquVyoLJlixR42VLywNxyL3IJBOPjmRsqlIt2yucjn0d9Q4jWKIdOXVUItD4MkIiIqNmxFEjZOwNPGWjV14OVlmYayBjT6YDCQmlmnVpteR07cwnn33xTf6Ai9wgZB0exsVJulrIo5vbtpscLCQGuXjUEdXJgJIQUTCkT3f38pGt1JIfIUgDb2DjE5iAOsRERkb0szU7r0gX44Ye6Q2eW1rEzriBurhdLWdxSWfJAOQPv7383HeobNAg4fNjwWFlUU3kceT9ycGipAKcy+bq+x67AQpEuxgCJiIgcYa6XCah/Wr+54T9z+ToPP2zIp7JW8mDBAsNxLQU+KSnSDD7l8OPu3ZbzhFJSpCG++hYrVgZd9VU2dxbmIBEREXkgS8N48rCSpeE5c68zl69jbvFgmbLkgfGQn0YD1NSYPu7a1TRHy/j45vKEcnOlniiZceHO7dsNOU9qNfC3vxkeK4f1GprH5AwMkIiIiDyAI1XFLeXryIGTskcqOdnQI2TcTg5MlNXFLSVIWzquucBs61YpaJLzo+Qg7epV00TzFi0MwZE7k7NlHGJzEIfYiIjIk9lS8sC4nTLR3JEEaeUwmnKIT5nwba50gPGwniswB8nFGCARERHVZRyYGZcKMC57YJxrZFxLyRV1j5SYg0RERESNznioMD7efC+WvF0eRjOeQefuoTUZe5AcxB4kIiIi52jIsJ692INEREREXsGRBHVXU7v7BIiIiIg8DQMkIiIiIgUGSEREREQKDJCIiIiIFBggERERESkwQCIiIiJSYIBEREREpMAAiYiIiEiBARIRERGRAgMkIiIiIgUGSEREREQKDJCIiIiIFDwiQFqxYgWio6Ph6+uL+Ph4HDp0yGr7Tz75BPfeey98fX3Rt29fbNu2Tf9cTU0NXnzxRfTt2xetWrVCREQEJk+ejJ9++slkH9HR0VCpVCa3RYsWueT6iIiIyLu4PUDasGED0tPTsWDBAhw9ehQxMTFISkrClStXzLbfv38/Jk6ciKlTp+LYsWMYM2YMxowZg2+//RYAUFFRgaNHj+KVV17B0aNH8fnnn+PMmTNIMbNM8GuvvYbLly/rb88884xLr5WIiIi8g0oIIdx5AvHx8Rg0aBCWL18OANDpdIiMjMQzzzyDuXPn1mk/fvx43L59G1u2bNFve+CBBxAbG4tVq1aZPcbhw4cRFxeH8+fPo1OnTgCkHqTZs2dj9uzZDp13eXk5goKCUFZWhsDAQIf2QURERI3L1u9vt/YgVVdXo6CgAImJifptarUaiYmJyM/PN/ua/Px8k/YAkJSUZLE9AJSVlUGlUiE4ONhk+6JFi9CuXTv0798ff/3rX3H37l3HL4aIiIiajBbuPPjVq1dRW1uL0NBQk+2hoaH47rvvzL6mpKTEbPuSkhKz7e/cuYMXX3wREydONIkUZ82ahfvvvx9t27bF/v37MW/ePFy+fBlLliwxu5+qqipUVVXpH5eXl9t0jUREROR93BoguVpNTQ3GjRsHIQRWrlxp8lx6err+5379+kGr1eLPf/4zsrKy4OPjU2dfWVlZePXVV11+zkREROR+bh1iCwkJgUajQWlpqcn20tJShIWFmX1NWFiYTe3l4Oj8+fPYuXNnvXlC8fHxuHv3Ln744Qezz8+bNw9lZWX628WLF+u5OiIiIvJWbg2QtFotBgwYgLy8PP02nU6HvLw8JCQkmH1NQkKCSXsA2Llzp0l7OTg6e/Ysdu3ahXbt2tV7LoWFhVCr1ejQoYPZ5318fBAYGGhyIyIioqbJ7UNs6enpSE1NxcCBAxEXF4fs7Gzcvn0bU6ZMAQBMnjwZHTt2RFZWFgDg2WefxZAhQ/D2229j1KhRWL9+PY4cOYL33nsPgBQc/e53v8PRo0exZcsW1NbW6vOT2rZtC61Wi/z8fBw8eBBDhw5F69atkZ+fj7S0NPzhD39AmzZt3PNGEBERkcdwe4A0fvx4/Pzzz5g/fz5KSkoQGxuLHTt26BOxL1y4ALXa0NH14IMPYt26dXj55ZeRkZGB7t27Y9OmTejTpw8A4NKlS8jNzQUAxMbGmhxr9+7dePjhh+Hj44P169cjMzMTVVVV6Ny5M9LS0kzykoiIiKj5cnsdJG/FOkhERETexyvqIBERERF5IgZIRERERAoMkIiIiIgUGCARERERKTBAIiIiIlJggERERESkwACJiIiISIEBEhEREZECAyQiIiIiBQZIRERERAoMkIiIiIgUGCARERERKTBAIiIiIlJggERERESkwACJiIiISIEBEhEREZECAyQiIiIiBQZIRERERAoMkIiIiIgUGCARERERKTBAIiIiIlJggERERESkwACJiIiISIEBEhEREZECAyQiIiIiBQZIRERERAoMkIiIiIgUGCARERERKTBAIiIiIlJggERERESkwACJiIiISIEBEhEREZECAyQiIiIiBQZIRERERAoMkIiIiIgUGCARERERKXhEgLRixQpER0fD19cX8fHxOHTokNX2n3zyCe699174+vqib9++2LZtm8nzQgjMnz8f4eHh8PPzQ2JiIs6ePWvS5vr165g0aRICAwMRHByMqVOn4tatW06/NiIiIvI+bg+QNmzYgPT0dCxYsABHjx5FTEwMkpKScOXKFbPt9+/fj4kTJ2Lq1Kk4duwYxowZgzFjxuDbb7/Vt1m8eDGWLl2KVatW4eDBg2jVqhWSkpJw584dfZtJkybh5MmT2LlzJ7Zs2YIvv/wS06dPd/n1EhERkedTCSGEO08gPj4egwYNwvLlywEAOp0OkZGReOaZZzB37tw67cePH4/bt29jy5Yt+m0PPPAAYmNjsWrVKgghEBERgeeeew5z5swBAJSVlSE0NBRr167FhAkTcPr0adx33304fPgwBg4cCADYsWMHRo4ciR9//BERERH1nnd5eTmCgoJQVlaGwMBAZ7wVRERE5GK2fn+3aMRzqqO6uhoFBQWYN2+efptarUZiYiLy8/PNviY/Px/p6ekm25KSkrBp0yYAQHFxMUpKSpCYmKh/PigoCPHx8cjPz8eECROQn5+P4OBgfXAEAImJiVCr1Th48CAee+yxOsetqqpCVVWV/nFZWRkA6Y0mIiIi7yB/b9fXP+TWAOnq1auora1FaGioyfbQ0FB89913Zl9TUlJitn1JSYn+eXmbtTYdOnQweb5FixZo27atvo1SVlYWXn311TrbIyMjLV0eEREReaibN28iKCjI4vNuDZC8ybx580x6rnQ6Ha5fv4527dpBpVI57Tjl5eWIjIzExYsXOXTnZvwsPAM/B8/Bz8Iz8HNoGCEEbt68WW86jVsDpJCQEGg0GpSWlppsLy0tRVhYmNnXhIWFWW0v35eWliI8PNykTWxsrL6NMgn87t27uH79usXj+vj4wMfHx2RbcHCw9QtsgMDAQP7iewh+Fp6Bn4Pn4GfhGfg5OM5az5HMrbPYtFotBgwYgLy8PP02nU6HvLw8JCQkmH1NQkKCSXsA2Llzp759586dERYWZtKmvLwcBw8e1LdJSEjAjRs3UFBQoG/zxRdfQKfTIT4+3mnXR0RERN7J7UNs6enpSE1NxcCBAxEXF4fs7Gzcvn0bU6ZMAQBMnjwZHTt2RFZWFgDg2WefxZAhQ/D2229j1KhRWL9+PY4cOYL33nsPAKBSqTB79my88cYb6N69Ozp37oxXXnkFERERGDNmDACgV69eGD58OKZNm4ZVq1ahpqYGM2fOxIQJE2yawUZERERNm9sDpPHjx+Pnn3/G/PnzUVJSgtjYWOzYsUOfZH3hwgWo1YaOrgcffBDr1q3Dyy+/jIyMDHTv3h2bNm1Cnz599G1eeOEF3L59G9OnT8eNGzfwq1/9Cjt27ICvr6++zYcffoiZM2di2LBhUKvVGDt2LJYuXdp4F26Bj48PFixYUGc4jxofPwvPwM/Bc/Cz8Az8HBqH2+sgEREREXkat1fSJiIiIvI0DJCIiIiIFBggERERESkwQCIiIiJSYIDkYVasWIHo6Gj4+voiPj4ehw4dcvcpNWmZmZlQqVQmt3vvvVf//J07dzBjxgy0a9cOAQEBGDt2bJ1CpeSYL7/8EsnJyYiIiIBKpdKvpygTQmD+/PkIDw+Hn58fEhMTcfbsWZM2169fx6RJkxAYGIjg4GBMnToVt27dasSr8H71fQ5PPPFEnX8jw4cPN2nDz6HhsrKyMGjQILRu3RodOnTAmDFjcObMGZM2tvw9unDhAkaNGgV/f3906NABzz//PO7evduYl9JkMEDyIBs2bEB6ejoWLFiAo0ePIiYmBklJSXWqfpNz9e7dG5cvX9bfvv76a/1zaWlp2Lx5Mz755BPs3bsXP/30E37729+68Wybjtu3byMmJgYrVqww+/zixYuxdOlSrFq1CgcPHkSrVq2QlJSEO3fu6NtMmjQJJ0+exM6dO7FlyxZ8+eWXmD59emNdQpNQ3+cAAMOHDzf5N/LRRx+ZPM/PoeH27t2LGTNm4MCBA9i5cydqamrw6KOP4vbt2/o29f09qq2txahRo1BdXY39+/fjH//4B9auXYv58+e745K8nyCPERcXJ2bMmKF/XFtbKyIiIkRWVpYbz6ppW7BggYiJiTH73I0bN0TLli3FJ598ot92+vRpAUDk5+c30hk2DwDExo0b9Y91Op0ICwsTf/3rX/Xbbty4IXx8fMRHH30khBDi1KlTAoA4fPiwvs327duFSqUSly5darRzb0qUn4MQQqSmporRo0dbfA0/B9e4cuWKACD27t0rhLDt79G2bduEWq0WJSUl+jYrV64UgYGBoqqqqnEvoAlgD5KHqK6uRkFBARITE/Xb1Go1EhMTkZ+f78Yza/rOnj2LiIgIdOnSBZMmTcKFCxcAAAUFBaipqTH5TO6991506tSJn4mLFRcXo6SkxOS9DwoKQnx8vP69z8/PR3BwMAYOHKhvk5iYCLVajYMHDzb6OTdle/bsQYcOHdCzZ0/85S9/wbVr1/TP8XNwjbKyMgBA27ZtAdj29yg/Px99+/bVF1oGgKSkJJSXl+PkyZONePZNAwMkD3H16lXU1taa/GIDQGhoKEpKStx0Vk1ffHw81q5dix07dmDlypUoLi7Gr3/9a9y8eRMlJSXQarV1FiXmZ+J68vtr7d9DSUkJOnToYPJ8ixYt0LZtW34+TjR8+HB88MEHyMvLw//+7/9i7969GDFiBGprawHwc3AFnU6H2bNnY/DgwfpVImz5e1RSUmL234z8HNnH7UuNELnTiBEj9D/369cP8fHxiIqKwscffww/Pz83nhmRZ5gwYYL+5759+6Jfv37o2rUr9uzZg2HDhrnxzJquGTNm4NtvvzXJh6TGxx4kDxESEgKNRlNnRkJpaSnCwsLcdFbNT3BwMHr06IGioiKEhYWhuroaN27cMGnDz8T15PfX2r+HsLCwOhMY7t69i+vXr/PzcaEuXbogJCQERUVFAPg5ONvMmTOxZcsW7N69G/fcc49+uy1/j8LCwsz+m5GfI/swQPIQWq0WAwYMQF5enn6bTqdDXl4eEhIS3HhmzcutW7dw7tw5hIeHY8CAAWjZsqXJZ3LmzBlcuHCBn4mLde7cGWFhYSbvfXl5OQ4ePKh/7xMSEnDjxg0UFBTo23zxxRfQ6XSIj49v9HNuLn788Udcu3YN4eHhAPg5OIsQAjNnzsTGjRvxxRdfoHPnzibP2/L3KCEhAd98841JwLpz504EBgbivvvua5wLaUrcnSVOBuvXrxc+Pj5i7dq14tSpU2L69OkiODjYZEYCOddzzz0n9uzZI4qLi8W+fftEYmKiCAkJEVeuXBFCCPHUU0+JTp06iS+++EIcOXJEJCQkiISEBDefddNw8+ZNcezYMXHs2DEBQCxZskQcO3ZMnD9/XgghxKJFi0RwcLDIyckRJ06cEKNHjxadO3cWlZWV+n0MHz5c9O/fXxw8eFB8/fXXonv37mLixInuuiSvZO1zuHnzppgzZ47Iz88XxcXFYteuXeL+++8X3bt3F3fu3NHvg59Dw/3lL38RQUFBYs+ePeLy5cv6W0VFhb5NfX+P7t69K/r06SMeffRRUVhYKHbs2CHat28v5s2b545L8noMkDzMsmXLRKdOnYRWqxVxcXHiwIED7j6lJm38+PEiPDxcaLVa0bFjRzF+/HhRVFSkf76yslI8/fTTok2bNsLf31889thj4vLly24846Zj9+7dAkCdW2pqqhBCmur/yiuviNDQUOHj4yOGDRsmzpw5Y7KPa9euiYkTJ4qAgAARGBgopkyZIm7evOmGq/Fe1j6HiooK8eijj4r27duLli1biqioKDFt2rQ6/2nj59Bw5j4DAGLNmjX6Nrb8Pfrhhx/EiBEjhJ+fnwgJCRHPPfecqKmpaeSraRpUQgjR2L1WRERERJ6MOUhERERECgyQiIiIiBQYIBEREREpMEAiIiIiUmCARERERKTAAImIiIhIgQESERERkQIDJCIiJ9izZw9UKlWdtbKIyDsxQCIiIiJSYIBEREREpMAAiYiaBJ1Oh6ysLHTu3Bl+fn6IiYnBp59+CsAw/LV161b069cPvr6+eOCBB/Dtt9+a7OOzzz5D79694ePjg+joaLz99tsmz1dVVeHFF19EZGQkfHx80K1bN6xevdqkTUFBAQYOHAh/f388+OCDOHPmjGsvnIhcggESETUJWVlZ+OCDD7Bq1SqcPHkSaWlp+MMf/oC9e/fq2zz//PN4++23cfjwYbRv3x7JycmoqakBIAU248aNw4QJE/DNN98gMzMTr7zyCtauXat//eTJk/HRRx9h6dKlOH36NP7v//4PAQEBJufx0ksv4e2338aRI0fQokUL/OlPf2qU6yci5+JitUTk9aqqqtC2bVvs2rULCQkJ+u1PPvkkKioqMH36dAwdOhTr16/H+PHjAQDXr1/HPffcg7Vr12LcuHGYNGkSfv75Z/z73//Wv/6FF17A1q1bcfLkSXz//ffo2bMndu7cicTExDrnsGfPHgwdOhS7du3CsGHDAADbtm3DqFGjUFlZCV9fXxe/C0TkTOxBIiKvV1RUhIqKCjzyyCMICAjQ3z744AOcO3dO3844eGrbti169uyJ06dPAwBOnz6NwYMHm+x38ODBOHv2LGpra1FYWAiNRoMhQ4ZYPZd+/frpfw4PDwcAXLlypcHXSESNq4W7T4CIqKFu3boFANi6dSs6duxo8pyPj49JkOQoPz8/m9q1bNlS/7NKpQIg5UcRkXdhDxIReb377rsPPj4+uHDhArp162Zyi4yM1Lc7cOCA/udffvkF33//PXr16gUA6NWrF/bt22ey33379qFHjx7QaDTo27cvdDqdSU4TETVd7EEiIq/XunVrzJkzB2lpadDpdPjVr36FsrIy7Nu3D4GBgYiKigIAvPbaa2jXrh1CQ0Px0ksvISQkBGPGjAEAPPfccxg0aBBef/11jB8/Hvn5+Vi+fDneffddAEB0dDRSU1Pxpz/9CUuXLkVMTAzOnz+PK1euYNy4ce66dCJyEQZIRNQkvP7662jfvj2ysrLwn//8B8HBwbj//vuRkZGhH+JatGgRnn32WZw9exaxsbHYvHkztFotAOD+++/Hxx9/jPnz5+P1119HeHg4XnvtNTzxxBP6Y6xcuRIZGRl4+umnce3aNXTq1AkZGRnuuFwicjHOYiOiJk+eYfbLL78gODjY3adDRF6AOUhERERECgyQiIiIiBQ4xEZERESkwB4kIiIiIgUGSEREREQKDJCIiIiIFBggERERESkwQCIiIiJSYIBEREREpMAAiYiIiEiBARIRERGRAgMkIiIiIoX/H208YfzG5BRQAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_len, y_vloss, 'o', c='red', markersize=2, label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, 'o', c='blue', markersize=2, label='Trainset_loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim(0, 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - accuracy: 0.9865 - loss: 0.0410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.046542972326278687, 0.986923098564148]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
